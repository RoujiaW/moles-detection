{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import VOC tensorflow dataset as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Input,\n",
    "    Lambda,\n",
    "    LeakyReLU,\n",
    "    MaxPool2D,\n",
    "    UpSampling2D,\n",
    "    ZeroPadding2D,\n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import (binary_crossentropy, sparse_categorical_crossentropy)\n",
    "\n",
    "\n",
    "from absl import app, flags, logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "ds, ds_info = tfds.load('voc', with_info=True)\n",
    "ds_train = ds['train']\n",
    "ds_test = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(data, v_boxes, labels, class_labels):\n",
    "    pyplot.imshow(data)\n",
    "    ax = pyplot.gca()\n",
    "    h = tf.dtypes.cast(tf.shape(img)[1], tf.float32)\n",
    "    w = tf.dtypes.cast(tf.shape(img)[0], tf.float32)\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        y1, x1, y2, x2 = box[0]*w, box[1]*h, box[2]*w, box[3]*h\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height, fill=False, color='white')\n",
    "        ax.add_patch(rect)\n",
    "        index = tf.get_static_value(labels[i])\n",
    "        label = class_labels[index]\n",
    "        pyplot.text(x1, y1, label, color='white')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aeroplane',\n",
       " 1: 'bicycle',\n",
       " 2: 'bird',\n",
       " 3: 'boat',\n",
       " 4: 'bottle',\n",
       " 5: 'bus',\n",
       " 6: 'car',\n",
       " 7: 'cat',\n",
       " 8: 'chair',\n",
       " 9: 'cow',\n",
       " 10: 'diningtable',\n",
       " 11: 'dog',\n",
       " 12: 'horse',\n",
       " 13: 'motorbike',\n",
       " 14: 'person',\n",
       " 15: 'pottedplant',\n",
       " 16: 'sheep',\n",
       " 17: 'sofa',\n",
       " 18: 'train',\n",
       " 19: 'tvmonitor'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://keras.io/examples/vision/retinanet/\n",
    "# from VOC tfds\n",
    "def read_class_names(class_file_name):\n",
    "    names = {}\n",
    "    with open(class_file_name, 'r') as data:\n",
    "        for ID, name in enumerate(data):\n",
    "            names[ID] = name.strip('\\n')\n",
    "    return names\n",
    "class_labels = read_class_names(\"/home/jupyter/moles-detection/voc.names\")\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "voc_iter = iter(ds_train)\n",
    "val = next(voc_iter)\n",
    "img = val['image']\n",
    "img_info = val['objects']\n",
    "box = img_info.get('bbox')\n",
    "labels = img_info.get('label')\n",
    "draw_boxes(img, box, labels, class_labels)\n",
    "\n",
    "voc_iter = iter(ds_test)\n",
    "val = next(voc_iter)\n",
    "img = val['image']\n",
    "img_info = val['objects']\n",
    "box = img_info.get('bbox')\n",
    "labels = img_info.get('label')\n",
    "draw_boxes(img, box, labels, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    def call(self, x, training=False):\n",
    "        if not training:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "    \n",
    "# convolutional block\n",
    "def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True, activate_type='leaky'):\n",
    "    if downsample:\n",
    "        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n",
    "        padding = 'valid'\n",
    "        strides = 2\n",
    "    else:\n",
    "        strides = 1\n",
    "        padding = 'same'\n",
    "    conv = tf.keras.layers.Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], \n",
    "                                  strides=strides, padding=padding, use_bias=not bn, \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.0005),\n",
    "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                                  bias_initializer=tf.constant_initializer(value = 0.))(input_layer)\n",
    "\n",
    "    if bn: conv = BatchNormalization()(conv)\n",
    "    if activate == True:\n",
    "        if activate_type == \"leaky\":\n",
    "            conv = tf.nn.leaky_relu(conv, alpha=0.1)\n",
    "        elif activate_type == \"mish\":\n",
    "            conv = mish(conv)\n",
    "    return conv\n",
    "\n",
    "# miss loss function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# residual block\n",
    "def residual_block(input_layer, input_channel, filter_num1, filter_num2, activate_type='leaky'):\n",
    "    short_cut = input_layer\n",
    "    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1), activate_type=activate_type)\n",
    "    conv = convolutional(conv, filters_shape=(3, 3, filter_num1,   filter_num2), activate_type=activate_type)\n",
    "    residual_output = short_cut + conv\n",
    "    return residual_output\n",
    "\n",
    "def route_group(input_layer, groups, group_id):\n",
    "    convs = tf.split(input_layer, num_or_size_splits=groups, axis=-1)\n",
    "    return convs[group_id]\n",
    "\n",
    "# image upsampling\n",
    "def upsample(input_layer):\n",
    "    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "import cv2\n",
    "import random\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_freeze_layer(model='yolov4', tiny=False):\n",
    "    freeze_layouts = ['conv2d_93', 'conv2d_101', 'conv2d_109']\n",
    "    return freeze_layouts\n",
    "\n",
    "def load_weights(model, weights_file, model_name='yolov4', is_tiny=False):\n",
    "    layer_size = 110\n",
    "    output_pos = [93, 101, 109]\n",
    "    wf = open(weights_file, 'rb')\n",
    "    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
    "    \n",
    "    j = 0\n",
    "    for i in range(layer_size):\n",
    "        conv_layer_name = 'conv2d_%d' %i if i > 0 else 'conv2d'\n",
    "        bn_layer_name = 'batch_normalization_%d' %j if j > 0 else 'batch_normalization'\n",
    "\n",
    "        conv_layer = model.get_layer(conv_layer_name)\n",
    "        filters = conv_layer.filters\n",
    "        k_size = conv_layer.kernel_size[0]\n",
    "        in_dim = conv_layer.input_shape[-1]\n",
    "\n",
    "        if i not in output_pos:\n",
    "            # darknet weights: [beta, gamma, mean, variance]\n",
    "            bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n",
    "            # tf weights: [gamma, beta, mean, variance]\n",
    "            bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
    "            bn_layer = model.get_layer(bn_layer_name)\n",
    "            j += 1\n",
    "        else:\n",
    "            conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n",
    "\n",
    "        # darknet shape (out_dim, in_dim, height, width)\n",
    "        conv_shape = (filters, in_dim, k_size, k_size)\n",
    "        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
    "        # tf shape (height, width, in_dim, out_dim)\n",
    "        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
    "\n",
    "        if i not in output_pos:\n",
    "            conv_layer.set_weights([conv_weights])\n",
    "            bn_layer.set_weights(bn_weights)\n",
    "        else:\n",
    "            conv_layer.set_weights([conv_weights, conv_bias])\n",
    "    wf.close()\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    anchors = np.array(anchors_path)\n",
    "    return anchors.reshape(3, 3, 2)\n",
    "\n",
    "def image_preprocess(image, target_size, gt_boxes=None):\n",
    "\n",
    "    ih, iw    = target_size\n",
    "    h,  w, _  = image.shape\n",
    "\n",
    "    scale = min(iw/w, ih/h)\n",
    "    nw, nh  = int(scale * w), int(scale * h)\n",
    "    image_resized = cv2.resize(image, (nw, nh))\n",
    "\n",
    "    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n",
    "    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n",
    "    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n",
    "    image_paded = image_paded / 255.\n",
    "\n",
    "    if gt_boxes is None:\n",
    "        return image_paded\n",
    "\n",
    "    else:\n",
    "        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n",
    "        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n",
    "        return image_paded, gt_boxes\n",
    "\n",
    "def bbox_iou(bboxes1, bboxes2):\n",
    "    \"\"\"\n",
    "    @param bboxes1: (a, b, ..., 4)\n",
    "    @param bboxes2: (A, B, ..., 4)\n",
    "        x:X is 1:n or n:n or n:1\n",
    "    @return (max(a,A), max(b,B), ...)\n",
    "    ex) (4,):(3,4) -> (3,)\n",
    "        (2,1,4):(2,3,4) -> (2,3)\n",
    "    \"\"\"\n",
    "    bboxes1_area = bboxes1[..., 2] * bboxes1[..., 3]\n",
    "    bboxes2_area = bboxes2[..., 2] * bboxes2[..., 3]\n",
    "\n",
    "    bboxes1_coor = tf.concat(\n",
    "        [\n",
    "            bboxes1[..., :2] - bboxes1[..., 2:] * 0.5,\n",
    "            bboxes1[..., :2] + bboxes1[..., 2:] * 0.5,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bboxes2_coor = tf.concat(\n",
    "        [\n",
    "            bboxes2[..., :2] - bboxes2[..., 2:] * 0.5,\n",
    "            bboxes2[..., :2] + bboxes2[..., 2:] * 0.5,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    left_up = tf.maximum(bboxes1_coor[..., :2], bboxes2_coor[..., :2])\n",
    "    right_down = tf.minimum(bboxes1_coor[..., 2:], bboxes2_coor[..., 2:])\n",
    "\n",
    "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "\n",
    "    union_area = bboxes1_area + bboxes2_area - inter_area\n",
    "\n",
    "    iou = tf.math.divide_no_nan(inter_area, union_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def bbox_giou(bboxes1, bboxes2):\n",
    "    \"\"\"\n",
    "    Generalized IoU\n",
    "    @param bboxes1: (a, b, ..., 4)\n",
    "    @param bboxes2: (A, B, ..., 4)\n",
    "        x:X is 1:n or n:n or n:1\n",
    "    @return (max(a,A), max(b,B), ...)\n",
    "    ex) (4,):(3,4) -> (3,)\n",
    "        (2,1,4):(2,3,4) -> (2,3)\n",
    "    \"\"\"\n",
    "    bboxes1_area = bboxes1[..., 2] * bboxes1[..., 3]\n",
    "    bboxes2_area = bboxes2[..., 2] * bboxes2[..., 3]\n",
    "\n",
    "    bboxes1_coor = tf.concat(\n",
    "        [\n",
    "            bboxes1[..., :2] - bboxes1[..., 2:] * 0.5,\n",
    "            bboxes1[..., :2] + bboxes1[..., 2:] * 0.5,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bboxes2_coor = tf.concat(\n",
    "        [\n",
    "            bboxes2[..., :2] - bboxes2[..., 2:] * 0.5,\n",
    "            bboxes2[..., :2] + bboxes2[..., 2:] * 0.5,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    left_up = tf.maximum(bboxes1_coor[..., :2], bboxes2_coor[..., :2])\n",
    "    right_down = tf.minimum(bboxes1_coor[..., 2:], bboxes2_coor[..., 2:])\n",
    "\n",
    "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "\n",
    "    union_area = bboxes1_area + bboxes2_area - inter_area\n",
    "\n",
    "    iou = tf.math.divide_no_nan(inter_area, union_area)\n",
    "\n",
    "    enclose_left_up = tf.minimum(bboxes1_coor[..., :2], bboxes2_coor[..., :2])\n",
    "    enclose_right_down = tf.maximum(\n",
    "        bboxes1_coor[..., 2:], bboxes2_coor[..., 2:]\n",
    "    )\n",
    "\n",
    "    enclose_section = enclose_right_down - enclose_left_up\n",
    "    enclose_area = enclose_section[..., 0] * enclose_section[..., 1]\n",
    "\n",
    "    giou = iou - tf.math.divide_no_nan(enclose_area - union_area, enclose_area)\n",
    "\n",
    "    return giou\n",
    "\n",
    "def nms(bboxes, iou_threshold, sigma=0.3, method='nms'):\n",
    "    \"\"\"\n",
    "    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n",
    "    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n",
    "          https://github.com/bharatsingh430/soft-nms\n",
    "    \"\"\"\n",
    "    classes_in_img = list(set(bboxes[:, 5]))\n",
    "    best_bboxes = []\n",
    "\n",
    "    for cls in classes_in_img:\n",
    "        cls_mask = (bboxes[:, 5] == cls)\n",
    "        cls_bboxes = bboxes[cls_mask]\n",
    "\n",
    "        while len(cls_bboxes) > 0:\n",
    "            max_ind = np.argmax(cls_bboxes[:, 4])\n",
    "            best_bbox = cls_bboxes[max_ind]\n",
    "            best_bboxes.append(best_bbox)\n",
    "            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n",
    "            iou = bbox_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n",
    "            weight = np.ones((len(iou),), dtype=np.float32)\n",
    "\n",
    "            assert method in ['nms', 'soft-nms']\n",
    "\n",
    "            if method == 'nms':\n",
    "                iou_mask = iou > iou_threshold\n",
    "                weight[iou_mask] = 0.0\n",
    "\n",
    "            if method == 'soft-nms':\n",
    "                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n",
    "\n",
    "            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n",
    "            score_mask = cls_bboxes[:, 4] > 0.\n",
    "            cls_bboxes = cls_bboxes[score_mask]\n",
    "\n",
    "    return best_bboxes\n",
    "\n",
    "def freeze_all(model, frozen=True):\n",
    "    model.trainable = not frozen\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        for l in model.layers:\n",
    "            freeze_all(l, frozen)\n",
    "            \n",
    "def unfreeze_all(model, frozen=False):\n",
    "    model.trainable = not frozen\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        for l in model.layers:\n",
    "            unfreeze_all(l, frozen)\n",
    "\n",
    "def process_image(ele): \n",
    "    return ele['image']\n",
    "def process_bbox(ele): \n",
    "    return ele['objects'].get('bbox')    \n",
    "def process_label(ele): \n",
    "    return ele['objects'].get('label')\n",
    "def create_bbox(v_boxes, labels, image):\n",
    "    bboxes = []\n",
    "    h = tf.dtypes.cast(tf.shape(image)[1], tf.float32)\n",
    "    w = tf.dtypes.cast(tf.shape(image)[0], tf.float32)\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        y1, x1, y2, x2 = box[0]*w, box[1]*h, box[2]*w, box[3]*h\n",
    "        label = tf.dtypes.cast(labels[i], tf.float32)\n",
    "        new_box = [y1, x1, y2, x2]\n",
    "        new_box.append(label)\n",
    "        bboxes.append(new_box)\n",
    "#     bboxes = tf.reshape(bboxes, [-1])\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSPdarknet53 as backbone\n",
    "def cspdarknet53(x):\n",
    "    x = convolutional(x, (3, 3,  3,  32), activate_type=\"mish\")\n",
    "    x = convolutional(x, (3, 3, 32,  64), downsample=True, activate_type=\"mish\")\n",
    "\n",
    "    route = x\n",
    "    route = convolutional(route, (1, 1, 64, 64), activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 64, 64), activate_type=\"mish\")\n",
    "    x = residual_block(x,  64,  32, 64, activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 64, 64), activate_type=\"mish\")\n",
    "\n",
    "    x = tf.concat([x, route], axis=-1)\n",
    "    x = convolutional(x, (1, 1, 128, 64), activate_type=\"mish\")\n",
    "    x = convolutional(x, (3, 3, 64, 128), downsample=True, activate_type=\"mish\")\n",
    "    route = x\n",
    "    route = convolutional(route, (1, 1, 128, 64), activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 128, 64), activate_type=\"mish\")\n",
    "    for i in range(2):\n",
    "        x = residual_block(x, 64,  64, 64, activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 64, 64), activate_type=\"mish\")\n",
    "    x = tf.concat([x, route], axis=-1)\n",
    "\n",
    "    x = convolutional(x, (1, 1, 128, 128), activate_type=\"mish\")\n",
    "    x = convolutional(x, (3, 3, 128, 256), downsample=True, activate_type=\"mish\")\n",
    "    route = x\n",
    "    route = convolutional(route, (1, 1, 256, 128), activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 256, 128), activate_type=\"mish\")\n",
    "    for i in range(8):\n",
    "        x = residual_block(x, 128, 128, 128, activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 128, 128), activate_type=\"mish\")\n",
    "    x = tf.concat([x, route], axis=-1)\n",
    "\n",
    "    x = convolutional(x, (1, 1, 256, 256), activate_type=\"mish\")\n",
    "    route_1 = x\n",
    "    x = convolutional(x, (3, 3, 256, 512), downsample=True, activate_type=\"mish\")\n",
    "    route = x\n",
    "    route = convolutional(route, (1, 1, 512, 256), activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 512, 256), activate_type=\"mish\")\n",
    "    for i in range(8):\n",
    "        x = residual_block(x, 256, 256, 256, activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 256, 256), activate_type=\"mish\")\n",
    "    x = tf.concat([x, route], axis=-1)\n",
    "\n",
    "    x = convolutional(x, (1, 1, 512, 512), activate_type=\"mish\")\n",
    "    route_2 = x\n",
    "    x = convolutional(x, (3, 3, 512, 1024), downsample=True, activate_type=\"mish\")\n",
    "    route = x\n",
    "    route = convolutional(route, (1, 1, 1024, 512), activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 1024, 512), activate_type=\"mish\")\n",
    "    for i in range(4):\n",
    "        x = residual_block(x, 512, 512, 512, activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 512, 512), activate_type=\"mish\")\n",
    "    x = tf.concat([x, route], axis=-1)\n",
    "\n",
    "    x = convolutional(x, (1, 1, 1024, 1024), activate_type=\"mish\")\n",
    "    x = convolutional(x, (1, 1, 1024, 512))\n",
    "    x = convolutional(x, (3, 3, 512, 1024))\n",
    "    x = convolutional(x, (1, 1, 1024, 512))\n",
    "\n",
    "    x = tf.concat([tf.nn.max_pool(x, ksize=13, padding='SAME', strides=1), tf.nn.max_pool(x, ksize=9, padding='SAME', strides=1)\n",
    "                            , tf.nn.max_pool(x, ksize=5, padding='SAME', strides=1), x], axis=-1)\n",
    "    x = convolutional(x, (1, 1, 2048, 512))\n",
    "    x = convolutional(x, (3, 3, 512, 1024))\n",
    "    x = convolutional(x, (1, 1, 1024, 512))\n",
    "\n",
    "    return route_1, route_2, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLOv4(input_layer, NUM_CLASS):\n",
    "    route_1, route_2, conv = cspdarknet53(input_layer)\n",
    "    route = conv\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv = upsample(conv)\n",
    "    route_2 = convolutional(route_2, (1, 1, 512, 256))\n",
    "    conv = tf.concat([route_2, conv], axis=-1)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "\n",
    "    route_2 = conv\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "    conv = upsample(conv)\n",
    "    route_1 = convolutional(route_1, (1, 1, 256, 128))\n",
    "    conv = tf.concat([route_1, conv], axis=-1)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "    conv = convolutional(conv, (3, 3, 128, 256))\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "    conv = convolutional(conv, (3, 3, 128, 256))\n",
    "    conv = convolutional(conv, (1, 1, 256, 128))\n",
    "\n",
    "    route_1 = conv\n",
    "    conv = convolutional(conv, (3, 3, 128, 256))\n",
    "    conv_sbbox = convolutional(conv, (1, 1, 256, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n",
    "\n",
    "    conv = convolutional(route_1, (3, 3, 128, 256), downsample=True)\n",
    "    conv = tf.concat([conv, route_2], axis=-1)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv = convolutional(conv, (1, 1, 512, 256))\n",
    "\n",
    "    route_2 = conv\n",
    "    conv = convolutional(conv, (3, 3, 256, 512))\n",
    "    conv_mbbox = convolutional(conv, (1, 1, 512, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n",
    "\n",
    "    conv = convolutional(route_2, (3, 3, 256, 512), downsample=True)\n",
    "    conv = tf.concat([conv, route], axis=-1)\n",
    "\n",
    "    conv = convolutional(conv, (1, 1, 1024, 512))\n",
    "    conv = convolutional(conv, (3, 3, 512, 1024))\n",
    "    conv = convolutional(conv, (1, 1, 1024, 512))\n",
    "    conv = convolutional(conv, (3, 3, 512, 1024))\n",
    "    conv = convolutional(conv, (1, 1, 1024, 512))\n",
    "\n",
    "    conv = convolutional(conv, (3, 3, 512, 1024))\n",
    "    conv_lbbox = convolutional(conv, (1, 1, 1024, 3 * (NUM_CLASS + 5)), activate=False, bn=False)\n",
    "\n",
    "    return [conv_sbbox, conv_mbbox, conv_lbbox]\n",
    "\n",
    "def decode_train(conv_output, output_size, NUM_CLASS, STRIDES, ANCHORS, i=0, XYSCALE=[1, 1, 1]):\n",
    "    conv_output = tf.reshape(conv_output,\n",
    "                             (tf.shape(conv_output)[0], output_size, output_size, 3, 5 + NUM_CLASS))\n",
    "\n",
    "    conv_raw_dxdy, conv_raw_dwdh, conv_raw_conf, conv_raw_prob = tf.split(conv_output, (2, 2, 1, NUM_CLASS),\n",
    "                                                                          axis=-1)\n",
    "\n",
    "    xy_grid = tf.meshgrid(tf.range(output_size), tf.range(output_size))\n",
    "    xy_grid = tf.expand_dims(tf.stack(xy_grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n",
    "    xy_grid = tf.tile(tf.expand_dims(xy_grid, axis=0), [tf.shape(conv_output)[0], 1, 1, 3, 1])\n",
    "\n",
    "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
    "\n",
    "    pred_xy = ((tf.sigmoid(conv_raw_dxdy) * XYSCALE[i]) - 0.5 * (XYSCALE[i] - 1) + xy_grid) * \\\n",
    "              STRIDES[i]\n",
    "    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i])\n",
    "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
    "\n",
    "    pred_conf = tf.sigmoid(conv_raw_conf)\n",
    "    pred_prob = tf.sigmoid(conv_raw_prob)\n",
    "    \n",
    "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n",
    "\n",
    "def compute_loss(pred, conv, label, bboxes, STRIDES, NUM_CLASS, IOU_LOSS_THRESH, i=0):\n",
    "    conv_shape  = tf.shape(conv)\n",
    "    batch_size  = conv_shape[0]\n",
    "    output_size = conv_shape[1]\n",
    "    input_size  = STRIDES[i] * output_size\n",
    "    conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n",
    "\n",
    "    conv_raw_conf = conv[:, :, :, :, 4:5]\n",
    "    conv_raw_prob = conv[:, :, :, :, 5:]\n",
    "\n",
    "    pred_xywh     = pred[:, :, :, :, 0:4]\n",
    "    pred_conf     = pred[:, :, :, :, 4:5]\n",
    "\n",
    "    label_xywh    = label[:, :, :, :, 0:4]\n",
    "    respond_bbox  = label[:, :, :, :, 4:5]\n",
    "    label_prob    = label[:, :, :, :, 5:]\n",
    "\n",
    "    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\n",
    "    input_size = tf.cast(input_size, tf.float32)\n",
    "\n",
    "    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n",
    "    giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n",
    "\n",
    "    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n",
    "    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n",
    "\n",
    "    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32 )\n",
    "\n",
    "    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n",
    "\n",
    "    conf_loss = conf_focal * (\n",
    "            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n",
    "            +\n",
    "            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n",
    "    )\n",
    "\n",
    "    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n",
    "\n",
    "    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n",
    "    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n",
    "    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n",
    "\n",
    "    return giou_loss, conf_loss, prob_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "SIZE = 416\n",
    "NUM_CLASS = 20\n",
    "\n",
    "ANCHORS              = [12,16, 19,36, 40,28, 36,75, 76,55, 72,146, 142,110, 192,243, 459,401]\n",
    "STRIDES              = [8, 16, 32]\n",
    "XYSCALE              = [1.2, 1.1, 1.05]\n",
    "ANCHOR_PER_SCALE     = 3\n",
    "IOU_LOSS_THRESH      = 0.5\n",
    "DATA_AUG = False\n",
    "# Train options\n",
    "BATCH_SIZE          = 2\n",
    "INPUT_SIZE          = 416\n",
    "LR_INIT             = 1e-3\n",
    "LR_END              = 5e-5 #originally 1e-6\n",
    "WARMUP_EPOCHS       = 2\n",
    "FISRT_STAGE_EPOCHS    = 20\n",
    "SECOND_STAGE_EPOCHS   = 30\n",
    "\n",
    "CLASSES = {0: 'aeroplane', \n",
    "           1: 'bicycle',\n",
    "           2: 'bird',\n",
    "           3: 'boat',\n",
    "           4: 'bottle',\n",
    "           5: 'bus',\n",
    "           6: 'car',\n",
    "           7: 'cat',\n",
    "           8: 'chair',\n",
    "           9: 'cow',\n",
    "           10: 'diningtable',\n",
    "           11: 'dog',\n",
    "           12: 'horse',\n",
    "           13: 'motorbike',\n",
    "           14: 'person',\n",
    "           15: 'pottedplant',\n",
    "           16: 'sheep',\n",
    "           17: 'sofa',\n",
    "           18: 'train',\n",
    "           19: 'tvmonitor'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datset.py\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class Dataset(object):\n",
    "    \"\"\"implement Dataset here\"\"\"\n",
    "    NUM_CLASS = 20\n",
    "    ANCHORS              = [12,16, 19,36, 40,28, 36,75, 76,55, 72,146, 142,110, 192,243, 459,401]\n",
    "    STRIDES              = [8, 16, 32]\n",
    "    XYSCALE              = [1.2, 1.1, 1.05]\n",
    "    ANCHOR_PER_SCALE     = 3\n",
    "    IOU_LOSS_THRESH      = 0.5\n",
    "    BATCH_SIZE          = 2\n",
    "    INPUT_SIZE          = 416\n",
    "    DATA_AUG            = False #default is True\n",
    "    LR_INIT             = 1e-3\n",
    "    LR_END              = 5e-5 # 1e-6 originally\n",
    "    WARMUP_EPOCHS       = 2\n",
    "    FISRT_STAGE_EPOCHS    = 20\n",
    "    SECOND_STAGE_EPOCHS   = 30\n",
    "    CLASSES = {0: 'aeroplane', \n",
    "               1: 'bicycle',\n",
    "               2: 'bird',\n",
    "               3: 'boat',\n",
    "               4: 'bottle',\n",
    "               5: 'bus',\n",
    "               6: 'car',\n",
    "               7: 'cat',\n",
    "               8: 'chair',\n",
    "               9: 'cow',\n",
    "               10: 'diningtable',\n",
    "               11: 'dog',\n",
    "               12: 'horse',\n",
    "               13: 'motorbike',\n",
    "               14: 'person',\n",
    "               15: 'pottedplant',\n",
    "               16: 'sheep',\n",
    "               17: 'sofa',\n",
    "               18: 'train',\n",
    "               19: 'tvmonitor'}\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.strides = np.array(STRIDES)\n",
    "        self.anchors = get_anchors(ANCHORS)\n",
    "        self.input_sizes = INPUT_SIZE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.data_aug = DATA_AUG\n",
    "        self.train_input_sizes = INPUT_SIZE       \n",
    "        self.classes = CLASSES     \n",
    "        self.num_classes = NUM_CLASS      \n",
    "        self.anchor_per_scale = ANCHOR_PER_SCALE\n",
    "        self.max_bbox_per_scale = 150\n",
    "        \n",
    "        self.images = list(dataset.map(process_image).as_numpy_iterator())\n",
    "        self.bboxes = list(dataset.map(process_bbox).as_numpy_iterator())\n",
    "        self.labels = list(dataset.map(process_label).as_numpy_iterator())  \n",
    "        \n",
    "        self.num_samples = len(dataset)\n",
    "        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.train_input_size = INPUT_SIZE\n",
    "            self.train_output_sizes = self.train_input_size // self.strides\n",
    "\n",
    "            batch_image = np.zeros(\n",
    "                (\n",
    "                    self.batch_size,\n",
    "                    self.train_input_size,\n",
    "                    self.train_input_size,\n",
    "                    3,\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "\n",
    "            batch_label_sbbox = np.zeros(\n",
    "                (\n",
    "                    self.batch_size,\n",
    "                    self.train_output_sizes[0],\n",
    "                    self.train_output_sizes[0],\n",
    "                    self.anchor_per_scale,\n",
    "                    5 + self.num_classes,\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            \n",
    "            batch_label_mbbox = np.zeros(\n",
    "                (\n",
    "                    self.batch_size,\n",
    "                    self.train_output_sizes[1],\n",
    "                    self.train_output_sizes[1],\n",
    "                    self.anchor_per_scale,\n",
    "                    5 + self.num_classes,\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            batch_label_lbbox = np.zeros(\n",
    "                (\n",
    "                    self.batch_size,\n",
    "                    self.train_output_sizes[2],\n",
    "                    self.train_output_sizes[2],\n",
    "                    self.anchor_per_scale,\n",
    "                    5 + self.num_classes,\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "\n",
    "            batch_sbboxes = np.zeros(\n",
    "                (self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32\n",
    "            )\n",
    "            batch_mbboxes = np.zeros(\n",
    "                (self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32\n",
    "            )\n",
    "            batch_lbboxes = np.zeros(\n",
    "                (self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32\n",
    "            )\n",
    "\n",
    "            num = 0\n",
    "            images = self.images  \n",
    "            bboxes = self.bboxes\n",
    "            labels = self.labels\n",
    "            \n",
    "            if self.batch_count < self.num_batchs:\n",
    "                while num < self.batch_size:\n",
    "                    index = self.batch_count * self.batch_size + num\n",
    "                    if index >= self.num_samples:\n",
    "                        index -= self.num_samples \n",
    "                        \n",
    "                    image = images[i]\n",
    "                    bbox = create_bbox(bboxes[i], labels[i], images[i])\n",
    "                    image, bbox = image_preprocess(\n",
    "                        np.copy(image), [self.train_input_size, \n",
    "                                         self.train_input_size], np.copy(bbox))\n",
    "                    (label_sbbox, label_mbbox, label_lbbox, sbboxes,\n",
    "                     mbboxes, lbboxes) = self.preprocess_true_boxes(bbox)\n",
    "                    batch_image[num, :, :, :] = image\n",
    "                \n",
    "                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n",
    "                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n",
    "                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n",
    "                    batch_sbboxes[num, :, :] = sbboxes\n",
    "                    batch_mbboxes[num, :, :] = mbboxes\n",
    "                    batch_lbboxes[num, :, :] = lbboxes\n",
    "                    num += 1\n",
    "                self.batch_count += 1\n",
    "                batch_smaller_target = batch_label_sbbox, batch_sbboxes\n",
    "                batch_medium_target = batch_label_mbbox, batch_mbboxes\n",
    "                batch_larger_target = batch_label_lbbox, batch_lbboxes\n",
    "\n",
    "                return (\n",
    "                    batch_image,\n",
    "                    (\n",
    "                        batch_smaller_target,\n",
    "                        batch_medium_target,\n",
    "                        batch_larger_target,\n",
    "                    ),\n",
    "                )\n",
    "            else:\n",
    "                self.batch_count = 0\n",
    "#                 np.random.shuffle(self.annotations)\n",
    "                raise StopIteration\n",
    "\n",
    "    def preprocess_true_boxes(self, bboxes):\n",
    "        label = [\n",
    "            np.zeros(\n",
    "                (\n",
    "                    self.train_output_sizes[i],\n",
    "                    self.train_output_sizes[i],\n",
    "                    self.anchor_per_scale,\n",
    "                    5 + self.num_classes,\n",
    "                )\n",
    "            )\n",
    "            for i in range(3)\n",
    "        ]\n",
    "        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n",
    "        bbox_count = np.zeros((3,))\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            bbox_coor = bbox[:4]\n",
    "            bbox_class_ind = int(bbox[4])\n",
    "            onehot = np.zeros(self.num_classes, dtype=np.float)\n",
    "            onehot[bbox_class_ind] = 1.0\n",
    "            uniform_distribution = np.full(\n",
    "                self.num_classes, 1.0 / self.num_classes\n",
    "            )\n",
    "            deta = 0.01\n",
    "            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n",
    "\n",
    "            bbox_xywh = np.concatenate(\n",
    "                [\n",
    "                    (bbox_coor[2:] + bbox_coor[:2]) * 0.5,\n",
    "                    bbox_coor[2:] - bbox_coor[:2],\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            bbox_xywh_scaled = (\n",
    "                1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n",
    "            )\n",
    "\n",
    "            iou = []\n",
    "            exist_positive = False\n",
    "            for i in range(3):\n",
    "                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n",
    "                anchors_xywh[:, 0:2] = (\n",
    "                    np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
    "                )\n",
    "                anchors_xywh[:, 2:4] = self.anchors[i]\n",
    "\n",
    "                iou_scale = bbox_iou(\n",
    "                    bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh\n",
    "                )\n",
    "                iou.append(iou_scale)\n",
    "                iou_mask = iou_scale > 0.3\n",
    "\n",
    "                if np.any(iou_mask):\n",
    "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(\n",
    "                        np.int32\n",
    "                    )\n",
    "\n",
    "                    label[i][yind, xind, iou_mask, :] = 0\n",
    "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n",
    "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
    "                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n",
    "\n",
    "                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n",
    "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n",
    "                    bbox_count[i] += 1\n",
    "\n",
    "                    exist_positive = True\n",
    "\n",
    "            if not exist_positive:\n",
    "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
    "                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n",
    "                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n",
    "                xind, yind = np.floor(\n",
    "                    bbox_xywh_scaled[best_detect, 0:2]\n",
    "                ).astype(np.int32)\n",
    "\n",
    "                label[best_detect][yind, xind, best_anchor, :] = 0\n",
    "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n",
    "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
    "                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n",
    "\n",
    "                bbox_ind = int(\n",
    "                    bbox_count[best_detect] % self.max_bbox_per_scale\n",
    "                )\n",
    "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n",
    "                bbox_count[best_detect] += 1\n",
    "        label_sbbox, label_mbbox, label_lbbox = label\n",
    "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
    "        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset(ds_train)\n",
    "input_layer = Input([SIZE, SIZE, 3])\n",
    "feature_maps = YOLOv4(input_layer, NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_93/BiasAdd:0' shape=(None, 52, 52, 75) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_101/BiasAdd:0' shape=(None, 26, 26, 75) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_109/BiasAdd:0' shape=(None, 13, 13, 75) dtype=float32>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_93/BiasAdd:0' shape=(None, 52, 52, 75) dtype=float32>,\n",
       " <tf.Tensor 'concat_11:0' shape=(None, 52, 52, 3, 25) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_101/BiasAdd:0' shape=(None, 26, 26, 75) dtype=float32>,\n",
       " <tf.Tensor 'concat_13:0' shape=(None, 26, 26, 3, 25) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_109/BiasAdd:0' shape=(None, 13, 13, 75) dtype=float32>,\n",
       " <tf.Tensor 'concat_15:0' shape=(None, 13, 13, 3, 25) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_tensors = []\n",
    "for i, fm in enumerate(feature_maps):\n",
    "    if i == 0:\n",
    "        bbox_tensor = decode_train(fm, SIZE // 8, NUM_CLASS, STRIDES, ANCHORS, i, XYSCALE)\n",
    "    elif i == 1:\n",
    "        bbox_tensor = decode_train(fm, SIZE // 16, NUM_CLASS, STRIDES, ANCHORS, i, XYSCALE)\n",
    "    else:\n",
    "        bbox_tensor = decode_train(fm, SIZE // 32, NUM_CLASS, STRIDES, ANCHORS, i, XYSCALE)\n",
    "    bbox_tensors.append(fm) \n",
    "    bbox_tensors.append(bbox_tensor) #confidence map\n",
    "\n",
    "bbox_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 416, 416, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 416, 416, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus (TensorFlo [(None, 416, 416, 32 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh (TensorFlowOpL [(None, 416, 416, 32 0           tf_op_layer_Softplus[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 416, 416, 32 0           batch_normalization[0][0]        \n",
      "                                                                 tf_op_layer_Tanh[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 417, 417, 32) 0           tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 208, 208, 64) 18432       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 208, 208, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_1 (TensorF [(None, 208, 208, 64 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_1 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_1[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 208, 208, 64) 4096        tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 208, 208, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_3 (TensorF [(None, 208, 208, 64 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_3 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_3 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_3[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 208, 208, 32) 2048        tf_op_layer_Mul_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 208, 208, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_4 (TensorF [(None, 208, 208, 32 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_4 (TensorFlowO [(None, 208, 208, 32 0           tf_op_layer_Softplus_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_4 (TensorFlowOp [(None, 208, 208, 32 0           batch_normalization_4[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 208, 208, 64) 18432       tf_op_layer_Mul_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 208, 208, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_5 (TensorF [(None, 208, 208, 64 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_5 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_5 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_5[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 208, 208, 64 0           tf_op_layer_Mul_3[0][0]          \n",
      "                                                                 tf_op_layer_Mul_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 208, 208, 64) 4096        tf_op_layer_AddV2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 208, 208, 64) 4096        tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 208, 208, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_6 (TensorF [(None, 208, 208, 64 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_2 (TensorF [(None, 208, 208, 64 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_6 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_2 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_6 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_6[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_2 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_2[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 208, 208, 12 0           tf_op_layer_Mul_6[0][0]          \n",
      "                                                                 tf_op_layer_Mul_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 208, 208, 64) 8192        tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 208, 208, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_7 (TensorF [(None, 208, 208, 64 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_7 (TensorFlowO [(None, 208, 208, 64 0           tf_op_layer_Softplus_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_7 (TensorFlowOp [(None, 208, 208, 64 0           batch_normalization_7[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 209, 209, 64) 0           tf_op_layer_Mul_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 104, 104, 128 73728       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 104, 104, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_8 (TensorF [(None, 104, 104, 12 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_8 (TensorFlowO [(None, 104, 104, 12 0           tf_op_layer_Softplus_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_8 (TensorFlowOp [(None, 104, 104, 12 0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 104, 104, 64) 8192        tf_op_layer_Mul_8[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 104, 104, 64) 256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_10 (Tensor [(None, 104, 104, 64 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_10 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_10 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_10[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 104, 104, 64) 4096        tf_op_layer_Mul_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 104, 104, 64) 256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_11 (Tensor [(None, 104, 104, 64 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_11 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_11 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_11[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 104, 104, 64) 36864       tf_op_layer_Mul_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 104, 104, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_12 (Tensor [(None, 104, 104, 64 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_12 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_12 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_12[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Mul_10[0][0]         \n",
      "                                                                 tf_op_layer_Mul_12[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 104, 104, 64) 4096        tf_op_layer_AddV2_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 104, 104, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_13 (Tensor [(None, 104, 104, 64 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_13 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_13 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_13[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 104, 104, 64) 36864       tf_op_layer_Mul_13[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 104, 104, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_14 (Tensor [(None, 104, 104, 64 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_14 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_14 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_14[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_2 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_AddV2_1[0][0]        \n",
      "                                                                 tf_op_layer_Mul_14[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 104, 104, 64) 4096        tf_op_layer_AddV2_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 104, 104, 64) 8192        tf_op_layer_Mul_8[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 104, 104, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 104, 104, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_15 (Tensor [(None, 104, 104, 64 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_9 (TensorF [(None, 104, 104, 64 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_15 (TensorFlow [(None, 104, 104, 64 0           tf_op_layer_Softplus_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_9 (TensorFlowO [(None, 104, 104, 64 0           tf_op_layer_Softplus_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_15 (TensorFlowO [(None, 104, 104, 64 0           batch_normalization_15[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_9 (TensorFlowOp [(None, 104, 104, 64 0           batch_normalization_9[0][0]      \n",
      "                                                                 tf_op_layer_Tanh_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 104, 104, 12 0           tf_op_layer_Mul_15[0][0]         \n",
      "                                                                 tf_op_layer_Mul_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 104, 104, 128 16384       tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 104, 104, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_16 (Tensor [(None, 104, 104, 12 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_16 (TensorFlow [(None, 104, 104, 12 0           tf_op_layer_Softplus_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_16 (TensorFlowO [(None, 104, 104, 12 0           batch_normalization_16[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 105, 105, 128 0           tf_op_layer_Mul_16[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 52, 52, 256)  294912      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 52, 52, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_17 (Tensor [(None, 52, 52, 256) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_17 (TensorFlow [(None, 52, 52, 256) 0           tf_op_layer_Softplus_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_17 (TensorFlowO [(None, 52, 52, 256) 0           batch_normalization_17[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_Mul_17[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 52, 52, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_19 (Tensor [(None, 52, 52, 128) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_19 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_19 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_19[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_Mul_19[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 52, 52, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_20 (Tensor [(None, 52, 52, 128) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_20 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_20 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_20[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_20[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_21 (Tensor [(None, 52, 52, 128) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_21 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_21 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_21[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_3 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Mul_19[0][0]         \n",
      "                                                                 tf_op_layer_Mul_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 52, 52, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_22 (Tensor [(None, 52, 52, 128) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_22 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_22[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_22 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_22[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 52, 52, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_23 (Tensor [(None, 52, 52, 128) 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_23 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_23 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_23[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_4 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_3[0][0]        \n",
      "                                                                 tf_op_layer_Mul_23[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 52, 52, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_24 (Tensor [(None, 52, 52, 128) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_24 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_24 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_24[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 52, 52, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_25 (Tensor [(None, 52, 52, 128) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_25 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_25 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_25[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_5 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_4[0][0]        \n",
      "                                                                 tf_op_layer_Mul_25[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 52, 52, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_26 (Tensor [(None, 52, 52, 128) 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_26 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_26 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_26[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_26[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 52, 52, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_27 (Tensor [(None, 52, 52, 128) 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_27 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_27[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_27 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_27[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_6 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_5[0][0]        \n",
      "                                                                 tf_op_layer_Mul_27[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 52, 52, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_28 (Tensor [(None, 52, 52, 128) 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_28 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_28[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_28 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_28[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_28[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 52, 52, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_29 (Tensor [(None, 52, 52, 128) 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_29 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_29 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_29[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_7 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_6[0][0]        \n",
      "                                                                 tf_op_layer_Mul_29[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 52, 52, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_30 (Tensor [(None, 52, 52, 128) 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_30 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_30[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_30 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_30[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_30[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 52, 52, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_31 (Tensor [(None, 52, 52, 128) 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_31 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_31[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_31 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_31[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_8 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_7[0][0]        \n",
      "                                                                 tf_op_layer_Mul_31[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 52, 52, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_32 (Tensor [(None, 52, 52, 128) 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_32 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_32 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_32[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_32[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 52, 52, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_33 (Tensor [(None, 52, 52, 128) 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_33 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_33[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_33 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_33[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_9 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_AddV2_8[0][0]        \n",
      "                                                                 tf_op_layer_Mul_33[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 52, 52, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_34 (Tensor [(None, 52, 52, 128) 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_34 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_34[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_34 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_34[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 52, 52, 128)  147456      tf_op_layer_Mul_34[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 52, 52, 128)  512         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_35 (Tensor [(None, 52, 52, 128) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_35 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_35 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_35[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_10 (TensorFlo [(None, 52, 52, 128) 0           tf_op_layer_AddV2_9[0][0]        \n",
      "                                                                 tf_op_layer_Mul_35[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 52, 52, 128)  16384       tf_op_layer_AddV2_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_Mul_17[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 52, 52, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 52, 52, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_36 (Tensor [(None, 52, 52, 128) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_18 (Tensor [(None, 52, 52, 128) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_36 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_36[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_18 (TensorFlow [(None, 52, 52, 128) 0           tf_op_layer_Softplus_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_36 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_36[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_18 (TensorFlowO [(None, 52, 52, 128) 0           batch_normalization_18[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 52, 52, 256) 0           tf_op_layer_Mul_36[0][0]         \n",
      "                                                                 tf_op_layer_Mul_18[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 52, 52, 256)  65536       tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 52, 52, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_37 (Tensor [(None, 52, 52, 256) 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_37 (TensorFlow [(None, 52, 52, 256) 0           tf_op_layer_Softplus_37[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_37 (TensorFlowO [(None, 52, 52, 256) 0           batch_normalization_37[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 53, 53, 256)  0           tf_op_layer_Mul_37[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 26, 26, 512)  1179648     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 26, 26, 512)  2048        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_38 (Tensor [(None, 26, 26, 512) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_38 (TensorFlow [(None, 26, 26, 512) 0           tf_op_layer_Softplus_38[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_38 (TensorFlowO [(None, 26, 26, 512) 0           batch_normalization_38[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_Mul_38[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 26, 26, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_40 (Tensor [(None, 26, 26, 256) 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_40 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_40[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_40 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_40[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_Mul_40[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 26, 26, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_41 (Tensor [(None, 26, 26, 256) 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_41 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_41[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_41 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_41[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_41[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 26, 26, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_42 (Tensor [(None, 26, 26, 256) 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_42 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_42[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_42 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_42[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_11 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_Mul_40[0][0]         \n",
      "                                                                 tf_op_layer_Mul_42[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 26, 26, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_43 (Tensor [(None, 26, 26, 256) 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_43 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_43[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_43 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_43[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_43[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 26, 26, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_44 (Tensor [(None, 26, 26, 256) 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_44 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_44[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_44 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_44[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_12 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_11[0][0]       \n",
      "                                                                 tf_op_layer_Mul_44[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 26, 26, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_45 (Tensor [(None, 26, 26, 256) 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_45 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_45[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_45 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_45[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_45[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_45[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 26, 26, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_46 (Tensor [(None, 26, 26, 256) 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_46 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_46[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_46 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_46[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_13 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_12[0][0]       \n",
      "                                                                 tf_op_layer_Mul_46[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 26, 26, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_47 (Tensor [(None, 26, 26, 256) 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_47 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_47[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_47 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_47[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_47[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 26, 26, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_48 (Tensor [(None, 26, 26, 256) 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_48 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_48[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_48 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_48[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_14 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_13[0][0]       \n",
      "                                                                 tf_op_layer_Mul_48[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 26, 26, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_49 (Tensor [(None, 26, 26, 256) 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_49 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_49[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_49 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_49[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_49[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 26, 26, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_50 (Tensor [(None, 26, 26, 256) 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_50 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_50[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_50 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_50[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_15 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_14[0][0]       \n",
      "                                                                 tf_op_layer_Mul_50[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 26, 26, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_51 (Tensor [(None, 26, 26, 256) 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_51 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_51[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_51 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_51[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_51[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 26, 26, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_52 (Tensor [(None, 26, 26, 256) 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_52 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_52[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_52 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_52[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_16 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_15[0][0]       \n",
      "                                                                 tf_op_layer_Mul_52[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 26, 26, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_53 (Tensor [(None, 26, 26, 256) 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_53 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_53[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_53 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_53[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_53[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 26, 26, 256)  1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_54 (Tensor [(None, 26, 26, 256) 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_54 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_54[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_54 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_54[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_17 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_16[0][0]       \n",
      "                                                                 tf_op_layer_Mul_54[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 26, 26, 256)  1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_55 (Tensor [(None, 26, 26, 256) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_55 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_55[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_55 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_55[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 26, 26, 256)  589824      tf_op_layer_Mul_55[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 26, 26, 256)  1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_56 (Tensor [(None, 26, 26, 256) 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_56 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_56[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_56 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_56[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_18 (TensorFlo [(None, 26, 26, 256) 0           tf_op_layer_AddV2_17[0][0]       \n",
      "                                                                 tf_op_layer_Mul_56[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 26, 26, 256)  65536       tf_op_layer_AddV2_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_Mul_38[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 26, 26, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 26, 26, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_57 (Tensor [(None, 26, 26, 256) 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_39 (Tensor [(None, 26, 26, 256) 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_57 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_57[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_39 (TensorFlow [(None, 26, 26, 256) 0           tf_op_layer_Softplus_39[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_57 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_57[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_39 (TensorFlowO [(None, 26, 26, 256) 0           batch_normalization_39[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo [(None, 26, 26, 512) 0           tf_op_layer_Mul_57[0][0]         \n",
      "                                                                 tf_op_layer_Mul_39[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 26, 26, 512)  262144      tf_op_layer_concat_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 26, 26, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_58 (Tensor [(None, 26, 26, 512) 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_58 (TensorFlow [(None, 26, 26, 512) 0           tf_op_layer_Softplus_58[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_58 (TensorFlowO [(None, 26, 26, 512) 0           batch_normalization_58[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 27, 27, 512)  0           tf_op_layer_Mul_58[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 13, 13, 1024) 4718592     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 13, 13, 1024) 4096        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_59 (Tensor [(None, 13, 13, 1024 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_59 (TensorFlow [(None, 13, 13, 1024 0           tf_op_layer_Softplus_59[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_59 (TensorFlowO [(None, 13, 13, 1024 0           batch_normalization_59[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_Mul_59[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 13, 13, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_61 (Tensor [(None, 13, 13, 512) 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_61 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_61[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_61 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_61[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_Mul_61[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 13, 13, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_62 (Tensor [(None, 13, 13, 512) 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_62 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_62[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_62 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_62[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_62[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 13, 13, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_63 (Tensor [(None, 13, 13, 512) 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_63 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_63[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_63 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_63[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_19 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_Mul_61[0][0]         \n",
      "                                                                 tf_op_layer_Mul_63[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 13, 13, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_64 (Tensor [(None, 13, 13, 512) 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_64 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_64[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_64 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_64[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_64[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_64[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 13, 13, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_65 (Tensor [(None, 13, 13, 512) 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_65 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_65[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_65 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_65[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_20 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_AddV2_19[0][0]       \n",
      "                                                                 tf_op_layer_Mul_65[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 13, 13, 512)  2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_66 (Tensor [(None, 13, 13, 512) 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_66 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_66[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_66 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_66[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_66[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_66[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 13, 13, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_67 (Tensor [(None, 13, 13, 512) 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_67 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_67[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_67 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_67[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_21 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_AddV2_20[0][0]       \n",
      "                                                                 tf_op_layer_Mul_67[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 13, 13, 512)  2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_68 (Tensor [(None, 13, 13, 512) 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_68 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_68[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_68 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_68[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_68[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 13, 13, 512)  2359296     tf_op_layer_Mul_68[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 13, 13, 512)  2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_69 (Tensor [(None, 13, 13, 512) 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_69 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_69[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_69 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_69[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_69[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_22 (TensorFlo [(None, 13, 13, 512) 0           tf_op_layer_AddV2_21[0][0]       \n",
      "                                                                 tf_op_layer_Mul_69[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 13, 13, 512)  262144      tf_op_layer_AddV2_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_Mul_59[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 13, 13, 512)  2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 13, 13, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_70 (Tensor [(None, 13, 13, 512) 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_60 (Tensor [(None, 13, 13, 512) 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_70 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_70[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_60 (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_Softplus_60[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_70 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_70[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_70[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_60 (TensorFlowO [(None, 13, 13, 512) 0           batch_normalization_60[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_4 (TensorFlo [(None, 13, 13, 1024 0           tf_op_layer_Mul_70[0][0]         \n",
      "                                                                 tf_op_layer_Mul_60[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 13, 13, 1024) 1048576     tf_op_layer_concat_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 13, 13, 1024) 4096        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus_71 (Tensor [(None, 13, 13, 1024 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_71 (TensorFlow [(None, 13, 13, 1024 0           tf_op_layer_Softplus_71[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_71 (TensorFlowO [(None, 13, 13, 1024 0           batch_normalization_71[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_71[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_Mul_71[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 13, 13, 512)  2048        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu (TensorFl [(None, 13, 13, 512) 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 13, 13, 1024) 4096        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_1 (Tensor [(None, 13, 13, 1024 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 13, 13, 512)  2048        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_2 (Tensor [(None, 13, 13, 512) 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MaxPool (TensorFlow [(None, 13, 13, 512) 0           tf_op_layer_LeakyRelu_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MaxPool_1 (TensorFl [(None, 13, 13, 512) 0           tf_op_layer_LeakyRelu_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MaxPool_2 (TensorFl [(None, 13, 13, 512) 0           tf_op_layer_LeakyRelu_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_5 (TensorFlo [(None, 13, 13, 2048 0           tf_op_layer_MaxPool[0][0]        \n",
      "                                                                 tf_op_layer_MaxPool_1[0][0]      \n",
      "                                                                 tf_op_layer_MaxPool_2[0][0]      \n",
      "                                                                 tf_op_layer_LeakyRelu_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 13, 13, 512)  1048576     tf_op_layer_concat_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 13, 13, 512)  2048        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_3 (Tensor [(None, 13, 13, 512) 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 13, 13, 1024) 4096        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_4 (Tensor [(None, 13, 13, 1024 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 13, 13, 512)  2048        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_5 (Tensor [(None, 13, 13, 512) 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 13, 13, 256)  131072      tf_op_layer_LeakyRelu_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_Mul_58[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 13, 13, 256)  1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 26, 26, 256)  1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_6 (Tensor [(None, 13, 13, 256) 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_7 (Tensor [(None, 26, 26, 256) 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ResizeBilinear (Ten [(None, 26, 26, 256) 0           tf_op_layer_LeakyRelu_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_6 (TensorFlo [(None, 26, 26, 512) 0           tf_op_layer_LeakyRelu_7[0][0]    \n",
      "                                                                 tf_op_layer_ResizeBilinear[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_concat_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 26, 26, 256)  1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_8 (Tensor [(None, 26, 26, 256) 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 26, 26, 512)  2048        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_9 (Tensor [(None, 26, 26, 512) 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_9[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 26, 26, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_10 (Tenso [(None, 26, 26, 256) 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_10[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 26, 26, 512)  2048        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_11 (Tenso [(None, 26, 26, 512) 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_11[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 26, 26, 256)  1024        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_12 (Tenso [(None, 26, 26, 256) 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 26, 26, 128)  32768       tf_op_layer_LeakyRelu_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_Mul_37[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 26, 26, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 52, 52, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_13 (Tenso [(None, 26, 26, 128) 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_14 (Tenso [(None, 52, 52, 128) 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ResizeBilinear_1 (T [(None, 52, 52, 128) 0           tf_op_layer_LeakyRelu_13[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_7 (TensorFlo [(None, 52, 52, 256) 0           tf_op_layer_LeakyRelu_14[0][0]   \n",
      "                                                                 tf_op_layer_ResizeBilinear_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_concat_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 52, 52, 128)  512         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_15 (Tenso [(None, 52, 52, 128) 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 52, 52, 256)  294912      tf_op_layer_LeakyRelu_15[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 52, 52, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_16 (Tenso [(None, 52, 52, 256) 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_LeakyRelu_16[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 52, 52, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_17 (Tenso [(None, 52, 52, 128) 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 52, 52, 256)  294912      tf_op_layer_LeakyRelu_17[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 52, 52, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_18 (Tenso [(None, 52, 52, 256) 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 52, 52, 128)  32768       tf_op_layer_LeakyRelu_18[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 52, 52, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_19 (Tenso [(None, 52, 52, 128) 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 53, 53, 128)  0           tf_op_layer_LeakyRelu_19[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 26, 26, 256)  294912      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 26, 26, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_21 (Tenso [(None, 26, 26, 256) 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_8 (TensorFlo [(None, 26, 26, 512) 0           tf_op_layer_LeakyRelu_21[0][0]   \n",
      "                                                                 tf_op_layer_LeakyRelu_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_concat_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 26, 26, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_22 (Tenso [(None, 26, 26, 256) 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_22[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 26, 26, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_23 (Tenso [(None, 26, 26, 512) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_23[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 26, 26, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_24 (Tenso [(None, 26, 26, 256) 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_24[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 26, 26, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_25 (Tenso [(None, 26, 26, 512) 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 26, 26, 256)  131072      tf_op_layer_LeakyRelu_25[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 26, 26, 256)  1024        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_26 (Tenso [(None, 26, 26, 256) 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 27, 27, 256)  0           tf_op_layer_LeakyRelu_26[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 13, 13, 512)  1179648     zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 13, 13, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_28 (Tenso [(None, 13, 13, 512) 0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_9 (TensorFlo [(None, 13, 13, 1024 0           tf_op_layer_LeakyRelu_28[0][0]   \n",
      "                                                                 tf_op_layer_LeakyRelu_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 13, 13, 512)  524288      tf_op_layer_concat_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 13, 13, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_29 (Tenso [(None, 13, 13, 512) 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_29[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 13, 13, 1024) 4096        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_30 (Tenso [(None, 13, 13, 1024 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_30[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 13, 13, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_31 (Tenso [(None, 13, 13, 512) 0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_31[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 13, 13, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_32 (Tenso [(None, 13, 13, 1024 0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 13, 13, 512)  524288      tf_op_layer_LeakyRelu_32[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 13, 13, 512)  2048        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_33 (Tenso [(None, 13, 13, 512) 0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 52, 52, 256)  294912      tf_op_layer_LeakyRelu_19[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 26, 26, 512)  1179648     tf_op_layer_LeakyRelu_26[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 13, 13, 1024) 4718592     tf_op_layer_LeakyRelu_33[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 52, 52, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 26, 26, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 13, 13, 1024) 4096        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_20 (Tenso [(None, 52, 52, 256) 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_27 (Tenso [(None, 26, 26, 512) 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LeakyRelu_34 (Tenso [(None, 13, 13, 1024 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 52, 52, 75)   19275       tf_op_layer_LeakyRelu_20[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 26, 26, 75)   38475       tf_op_layer_LeakyRelu_27[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 13, 13, 75)   76875       tf_op_layer_LeakyRelu_34[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(4,)]               0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(4,)]               0           conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(4,)]               0           conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(5,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(5,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(5,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 52, 52, 3, 2 0           conv2d_93[0][0]                  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 26, 26, 3, 2 0           conv2d_101[0][0]                 \n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 13, 13, 3, 2 0           conv2d_109[0][0]                 \n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(5,)]               0           tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(5,)]               0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(5,)]               0           tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 52, 52, 3, 2 0           tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 26, 26, 3, 2 0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_2 (TensorFlow [(None, 13, 13, 3, 2 0           tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid (TensorFlow [(None, 52, 52, 3, 2 0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile/multiples (Ten [(5,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_3 (TensorFl [(None, 26, 26, 3, 2 0           tf_op_layer_split_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_1/multiples (T [(5,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_6 (TensorFl [(None, 13, 13, 3, 2 0           tf_op_layer_split_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_2/multiples (T [(5,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_72 (TensorFlowO [(None, 52, 52, 3, 2 0           tf_op_layer_Sigmoid[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile (TensorFlowOpL [(None, 52, 52, 3, 2 0           tf_op_layer_Tile/multiples[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_75 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Sigmoid_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_1 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Tile_1/multiples[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_78 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Sigmoid_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_2 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Tile_2/multiples[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 52, 52, 3, 2 0           tf_op_layer_Mul_72[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlowOpL [(None, 52, 52, 3, 2 0           tf_op_layer_Tile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 26, 26, 3, 2 0           tf_op_layer_Mul_75[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast_1 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Tile_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(None, 13, 13, 3, 2 0           tf_op_layer_Mul_78[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast_2 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Tile_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_23 (TensorFlo [(None, 52, 52, 3, 2 0           tf_op_layer_Sub[0][0]            \n",
      "                                                                 tf_op_layer_Cast[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp (TensorFlowOpLa [(None, 52, 52, 3, 2 0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_24 (TensorFlo [(None, 26, 26, 3, 2 0           tf_op_layer_Sub_1[0][0]          \n",
      "                                                                 tf_op_layer_Cast_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_1 (TensorFlowOp [(None, 26, 26, 3, 2 0           tf_op_layer_split_1[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_25 (TensorFlo [(None, 13, 13, 3, 2 0           tf_op_layer_Sub_2[0][0]          \n",
      "                                                                 tf_op_layer_Cast_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_2 (TensorFlowOp [(None, 13, 13, 3, 2 0           tf_op_layer_split_2[0][1]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_73 (TensorFlowO [(None, 52, 52, 3, 2 0           tf_op_layer_AddV2_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_74 (TensorFlowO [(None, 52, 52, 3, 2 0           tf_op_layer_Exp[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_76 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_AddV2_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_77 (TensorFlowO [(None, 26, 26, 3, 2 0           tf_op_layer_Exp_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_79 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_AddV2_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_80 (TensorFlowO [(None, 13, 13, 3, 2 0           tf_op_layer_Exp_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_10 (TensorFl [(None, 52, 52, 3, 4 0           tf_op_layer_Mul_73[0][0]         \n",
      "                                                                 tf_op_layer_Mul_74[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_1 (TensorFl [(None, 52, 52, 3, 1 0           tf_op_layer_split[0][2]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_2 (TensorFl [(None, 52, 52, 3, 2 0           tf_op_layer_split[0][3]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_12 (TensorFl [(None, 26, 26, 3, 4 0           tf_op_layer_Mul_76[0][0]         \n",
      "                                                                 tf_op_layer_Mul_77[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_4 (TensorFl [(None, 26, 26, 3, 1 0           tf_op_layer_split_1[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_5 (TensorFl [(None, 26, 26, 3, 2 0           tf_op_layer_split_1[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_14 (TensorFl [(None, 13, 13, 3, 4 0           tf_op_layer_Mul_79[0][0]         \n",
      "                                                                 tf_op_layer_Mul_80[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_7 (TensorFl [(None, 13, 13, 3, 1 0           tf_op_layer_split_2[0][2]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_8 (TensorFl [(None, 13, 13, 3, 2 0           tf_op_layer_split_2[0][3]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_11 (TensorFl [(None, 52, 52, 3, 2 0           tf_op_layer_concat_10[0][0]      \n",
      "                                                                 tf_op_layer_Sigmoid_1[0][0]      \n",
      "                                                                 tf_op_layer_Sigmoid_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_13 (TensorFl [(None, 26, 26, 3, 2 0           tf_op_layer_concat_12[0][0]      \n",
      "                                                                 tf_op_layer_Sigmoid_4[0][0]      \n",
      "                                                                 tf_op_layer_Sigmoid_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_15 (TensorFl [(None, 13, 13, 3, 2 0           tf_op_layer_concat_14[0][0]      \n",
      "                                                                 tf_op_layer_Sigmoid_7[0][0]      \n",
      "                                                                 tf_op_layer_Sigmoid_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 64,106,305\n",
      "Trainable params: 64,040,001\n",
      "Non-trainable params: 66,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_layer, bbox_tensors, name='yolov4')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = round(len(trainset)/10) #len trainset originally\n",
    "first_stage_epochs = FISRT_STAGE_EPOCHS\n",
    "second_stage_epochs = SECOND_STAGE_EPOCHS\n",
    "global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n",
    "warmup_steps = WARMUP_EPOCHS * steps_per_epoch\n",
    "total_steps = (first_stage_epochs + second_stage_epochs) * steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> STEP    1/6250   lr: 0.001000   giou_loss: 2.90   conf_loss: 1978.83   prob_loss: 27.62   total_loss: 2009.35\n",
      "=> STEP    2/6250   lr: 0.000008   giou_loss: 2.90   conf_loss: 2110.14   prob_loss: 28.05   total_loss: 2141.09\n",
      "=> STEP    3/6250   lr: 0.000012   giou_loss: 2.90   conf_loss: 2064.17   prob_loss: 27.90   total_loss: 2094.97\n",
      "=> STEP    4/6250   lr: 0.000016   giou_loss: 2.90   conf_loss: 2008.83   prob_loss: 27.61   total_loss: 2039.34\n",
      "=> STEP    5/6250   lr: 0.000020   giou_loss: 2.90   conf_loss: 1956.89   prob_loss: 27.36   total_loss: 1987.15\n",
      "=> STEP    6/6250   lr: 0.000024   giou_loss: 2.90   conf_loss: 1912.34   prob_loss: 27.24   total_loss: 1942.48\n",
      "=> STEP    7/6250   lr: 0.000028   giou_loss: 2.90   conf_loss: 1872.23   prob_loss: 27.23   total_loss: 1902.35\n",
      "=> STEP    8/6250   lr: 0.000032   giou_loss: 2.90   conf_loss: 1834.98   prob_loss: 27.19   total_loss: 1865.07\n",
      "=> STEP    9/6250   lr: 0.000036   giou_loss: 2.90   conf_loss: 1800.08   prob_loss: 27.15   total_loss: 1830.12\n",
      "=> STEP   10/6250   lr: 0.000040   giou_loss: 2.90   conf_loss: 1766.28   prob_loss: 27.02   total_loss: 1796.20\n",
      "=> STEP   11/6250   lr: 0.000044   giou_loss: 2.90   conf_loss: 1733.57   prob_loss: 26.85   total_loss: 1763.32\n",
      "=> STEP   12/6250   lr: 0.000048   giou_loss: 2.90   conf_loss: 1700.44   prob_loss: 26.76   total_loss: 1730.10\n",
      "=> STEP   13/6250   lr: 0.000052   giou_loss: 2.90   conf_loss: 1666.91   prob_loss: 26.72   total_loss: 1696.53\n",
      "=> STEP   14/6250   lr: 0.000056   giou_loss: 2.90   conf_loss: 1634.34   prob_loss: 26.66   total_loss: 1663.91\n",
      "=> STEP   15/6250   lr: 0.000060   giou_loss: 2.90   conf_loss: 1603.92   prob_loss: 26.65   total_loss: 1633.46\n",
      "=> STEP   16/6250   lr: 0.000064   giou_loss: 2.90   conf_loss: 1576.27   prob_loss: 26.63   total_loss: 1605.80\n",
      "=> STEP   17/6250   lr: 0.000068   giou_loss: 2.90   conf_loss: 1551.05   prob_loss: 26.60   total_loss: 1580.55\n",
      "=> STEP   18/6250   lr: 0.000072   giou_loss: 2.90   conf_loss: 1526.88   prob_loss: 26.56   total_loss: 1556.35\n",
      "=> STEP   19/6250   lr: 0.000076   giou_loss: 2.90   conf_loss: 1503.60   prob_loss: 26.53   total_loss: 1533.03\n",
      "=> STEP   20/6250   lr: 0.000080   giou_loss: 2.90   conf_loss: 1480.89   prob_loss: 26.51   total_loss: 1510.30\n",
      "=> STEP   21/6250   lr: 0.000084   giou_loss: 2.90   conf_loss: 1459.16   prob_loss: 26.47   total_loss: 1488.53\n",
      "=> STEP   22/6250   lr: 0.000088   giou_loss: 2.90   conf_loss: 1438.81   prob_loss: 26.44   total_loss: 1468.15\n",
      "=> STEP   23/6250   lr: 0.000092   giou_loss: 2.90   conf_loss: 1419.92   prob_loss: 26.43   total_loss: 1449.24\n",
      "=> STEP   24/6250   lr: 0.000096   giou_loss: 2.90   conf_loss: 1402.19   prob_loss: 26.43   total_loss: 1431.51\n",
      "=> STEP   25/6250   lr: 0.000100   giou_loss: 2.90   conf_loss: 1385.60   prob_loss: 26.41   total_loss: 1414.90\n",
      "=> STEP   26/6250   lr: 0.000104   giou_loss: 2.90   conf_loss: 1370.32   prob_loss: 26.39   total_loss: 1399.61\n",
      "=> STEP   27/6250   lr: 0.000108   giou_loss: 2.90   conf_loss: 1356.13   prob_loss: 26.39   total_loss: 1385.42\n",
      "=> STEP   28/6250   lr: 0.000112   giou_loss: 2.90   conf_loss: 1342.73   prob_loss: 26.38   total_loss: 1372.01\n",
      "=> STEP   29/6250   lr: 0.000116   giou_loss: 2.90   conf_loss: 1330.38   prob_loss: 26.39   total_loss: 1359.66\n",
      "=> STEP   30/6250   lr: 0.000120   giou_loss: 2.90   conf_loss: 1318.96   prob_loss: 26.40   total_loss: 1348.26\n",
      "=> STEP   31/6250   lr: 0.000124   giou_loss: 2.90   conf_loss: 1308.27   prob_loss: 26.41   total_loss: 1337.58\n",
      "=> STEP   32/6250   lr: 0.000128   giou_loss: 2.90   conf_loss: 1298.21   prob_loss: 26.43   total_loss: 1327.54\n",
      "=> STEP   33/6250   lr: 0.000132   giou_loss: 2.90   conf_loss: 1288.73   prob_loss: 26.44   total_loss: 1318.07\n",
      "=> STEP   34/6250   lr: 0.000136   giou_loss: 2.90   conf_loss: 1279.83   prob_loss: 26.45   total_loss: 1309.18\n",
      "=> STEP   35/6250   lr: 0.000140   giou_loss: 2.90   conf_loss: 1271.46   prob_loss: 26.45   total_loss: 1300.81\n",
      "=> STEP   36/6250   lr: 0.000144   giou_loss: 2.90   conf_loss: 1263.69   prob_loss: 26.46   total_loss: 1293.04\n",
      "=> STEP   37/6250   lr: 0.000148   giou_loss: 2.90   conf_loss: 1256.38   prob_loss: 26.45   total_loss: 1285.73\n",
      "=> STEP   38/6250   lr: 0.000152   giou_loss: 2.90   conf_loss: 1249.44   prob_loss: 26.44   total_loss: 1278.78\n",
      "=> STEP   39/6250   lr: 0.000156   giou_loss: 2.90   conf_loss: 1242.87   prob_loss: 26.43   total_loss: 1272.20\n",
      "=> STEP   40/6250   lr: 0.000160   giou_loss: 2.90   conf_loss: 1236.70   prob_loss: 26.42   total_loss: 1266.02\n",
      "=> STEP   41/6250   lr: 0.000164   giou_loss: 2.90   conf_loss: 1231.14   prob_loss: 26.39   total_loss: 1260.43\n",
      "=> STEP   42/6250   lr: 0.000168   giou_loss: 2.90   conf_loss: 1228.19   prob_loss: 26.39   total_loss: 1257.48\n",
      "=> STEP   43/6250   lr: 0.000172   giou_loss: 2.90   conf_loss: 1226.81   prob_loss: 26.34   total_loss: 1256.06\n",
      "=> STEP   44/6250   lr: 0.000176   giou_loss: 2.90   conf_loss: 1219.88   prob_loss: 26.44   total_loss: 1249.22\n",
      "=> STEP   45/6250   lr: 0.000180   giou_loss: 2.90   conf_loss: 1215.94   prob_loss: 26.44   total_loss: 1245.28\n",
      "=> STEP   46/6250   lr: 0.000184   giou_loss: 2.90   conf_loss: 1210.35   prob_loss: 26.35   total_loss: 1239.59\n",
      "=> STEP   47/6250   lr: 0.000188   giou_loss: 2.90   conf_loss: 1205.35   prob_loss: 26.30   total_loss: 1234.55\n",
      "=> STEP   48/6250   lr: 0.000192   giou_loss: 2.90   conf_loss: 1201.18   prob_loss: 26.23   total_loss: 1230.31\n",
      "=> STEP   49/6250   lr: 0.000196   giou_loss: 2.90   conf_loss: 1196.78   prob_loss: 26.18   total_loss: 1225.86\n",
      "=> STEP   50/6250   lr: 0.000200   giou_loss: 2.90   conf_loss: 1193.11   prob_loss: 26.17   total_loss: 1222.18\n",
      "=> STEP   51/6250   lr: 0.000204   giou_loss: 2.90   conf_loss: 1188.41   prob_loss: 26.13   total_loss: 1217.44\n",
      "=> STEP   52/6250   lr: 0.000208   giou_loss: 2.90   conf_loss: 1184.78   prob_loss: 26.05   total_loss: 1213.74\n",
      "=> STEP   53/6250   lr: 0.000212   giou_loss: 2.90   conf_loss: 1181.19   prob_loss: 25.97   total_loss: 1210.06\n",
      "=> STEP   54/6250   lr: 0.000216   giou_loss: 2.90   conf_loss: 1177.27   prob_loss: 25.94   total_loss: 1206.10\n",
      "=> STEP   55/6250   lr: 0.000220   giou_loss: 2.90   conf_loss: 1174.59   prob_loss: 25.87   total_loss: 1203.36\n",
      "=> STEP   56/6250   lr: 0.000224   giou_loss: 2.90   conf_loss: 1171.35   prob_loss: 25.80   total_loss: 1200.04\n",
      "=> STEP   57/6250   lr: 0.000228   giou_loss: 2.90   conf_loss: 1168.45   prob_loss: 25.78   total_loss: 1197.13\n",
      "=> STEP   58/6250   lr: 0.000232   giou_loss: 2.90   conf_loss: 1165.57   prob_loss: 25.69   total_loss: 1194.15\n",
      "=> STEP   59/6250   lr: 0.000236   giou_loss: 2.90   conf_loss: 1161.66   prob_loss: 25.62   total_loss: 1190.17\n",
      "=> STEP   60/6250   lr: 0.000240   giou_loss: 2.90   conf_loss: 1159.25   prob_loss: 25.52   total_loss: 1187.67\n",
      "=> STEP   61/6250   lr: 0.000244   giou_loss: 2.90   conf_loss: 1156.03   prob_loss: 25.40   total_loss: 1184.33\n",
      "=> STEP   62/6250   lr: 0.000248   giou_loss: 2.90   conf_loss: 1153.05   prob_loss: 25.33   total_loss: 1181.27\n",
      "=> STEP   63/6250   lr: 0.000252   giou_loss: 2.90   conf_loss: 1150.60   prob_loss: 25.18   total_loss: 1178.68\n",
      "=> STEP   64/6250   lr: 0.000256   giou_loss: 2.90   conf_loss: 1147.74   prob_loss: 25.00   total_loss: 1175.64\n",
      "=> STEP   65/6250   lr: 0.000260   giou_loss: 2.90   conf_loss: 1145.01   prob_loss: 24.87   total_loss: 1172.78\n",
      "=> STEP   66/6250   lr: 0.000264   giou_loss: 2.90   conf_loss: 1142.09   prob_loss: 24.72   total_loss: 1169.71\n",
      "=> STEP   67/6250   lr: 0.000268   giou_loss: 2.90   conf_loss: 1139.56   prob_loss: 24.56   total_loss: 1167.03\n",
      "=> STEP   68/6250   lr: 0.000272   giou_loss: 2.90   conf_loss: 1137.20   prob_loss: 24.38   total_loss: 1164.48\n",
      "=> STEP   69/6250   lr: 0.000276   giou_loss: 2.90   conf_loss: 1134.48   prob_loss: 24.17   total_loss: 1161.54\n",
      "=> STEP   70/6250   lr: 0.000280   giou_loss: 2.90   conf_loss: 1132.34   prob_loss: 23.93   total_loss: 1159.17\n",
      "=> STEP   71/6250   lr: 0.000284   giou_loss: 2.90   conf_loss: 1129.93   prob_loss: 23.63   total_loss: 1156.46\n",
      "=> STEP   72/6250   lr: 0.000288   giou_loss: 2.90   conf_loss: 1127.50   prob_loss: 23.35   total_loss: 1153.75\n",
      "=> STEP   73/6250   lr: 0.000292   giou_loss: 2.90   conf_loss: 1125.34   prob_loss: 23.07   total_loss: 1151.31\n",
      "=> STEP   74/6250   lr: 0.000296   giou_loss: 2.90   conf_loss: 1123.19   prob_loss: 22.73   total_loss: 1148.82\n",
      "=> STEP   75/6250   lr: 0.000300   giou_loss: 2.90   conf_loss: 1120.96   prob_loss: 22.36   total_loss: 1146.22\n",
      "=> STEP   76/6250   lr: 0.000304   giou_loss: 2.90   conf_loss: 1118.90   prob_loss: 22.03   total_loss: 1143.83\n",
      "=> STEP   77/6250   lr: 0.000308   giou_loss: 2.90   conf_loss: 1117.34   prob_loss: 21.64   total_loss: 1141.88\n",
      "=> STEP   78/6250   lr: 0.000312   giou_loss: 2.90   conf_loss: 1117.38   prob_loss: 21.34   total_loss: 1141.62\n",
      "=> STEP   79/6250   lr: 0.000316   giou_loss: 2.90   conf_loss: 1117.78   prob_loss: 20.82   total_loss: 1141.50\n",
      "=> STEP   80/6250   lr: 0.000320   giou_loss: 2.90   conf_loss: 1116.12   prob_loss: 20.49   total_loss: 1139.51\n",
      "=> STEP   81/6250   lr: 0.000324   giou_loss: 2.90   conf_loss: 1114.81   prob_loss: 20.19   total_loss: 1137.90\n",
      "=> STEP   82/6250   lr: 0.000328   giou_loss: 2.90   conf_loss: 1110.83   prob_loss: 20.00   total_loss: 1133.73\n",
      "=> STEP   83/6250   lr: 0.000332   giou_loss: 2.90   conf_loss: 1108.84   prob_loss: 19.62   total_loss: 1131.36\n",
      "=> STEP   84/6250   lr: 0.000336   giou_loss: 2.90   conf_loss: 1106.46   prob_loss: 18.99   total_loss: 1128.35\n",
      "=> STEP   85/6250   lr: 0.000340   giou_loss: 2.90   conf_loss: 1104.87   prob_loss: 18.44   total_loss: 1126.21\n",
      "=> STEP   86/6250   lr: 0.000344   giou_loss: 2.90   conf_loss: 1102.15   prob_loss: 18.16   total_loss: 1123.21\n",
      "=> STEP   87/6250   lr: 0.000348   giou_loss: 2.90   conf_loss: 1099.84   prob_loss: 17.91   total_loss: 1120.65\n",
      "=> STEP   88/6250   lr: 0.000352   giou_loss: 2.90   conf_loss: 1098.53   prob_loss: 17.45   total_loss: 1118.88\n",
      "=> STEP   89/6250   lr: 0.000356   giou_loss: 2.90   conf_loss: 1096.36   prob_loss: 16.93   total_loss: 1116.19\n",
      "=> STEP   90/6250   lr: 0.000360   giou_loss: 2.90   conf_loss: 1094.22   prob_loss: 16.54   total_loss: 1113.65\n",
      "=> STEP   91/6250   lr: 0.000364   giou_loss: 2.90   conf_loss: 1092.17   prob_loss: 16.19   total_loss: 1111.26\n",
      "=> STEP   92/6250   lr: 0.000368   giou_loss: 2.90   conf_loss: 1090.50   prob_loss: 15.71   total_loss: 1109.12\n",
      "=> STEP   93/6250   lr: 0.000372   giou_loss: 2.90   conf_loss: 1088.54   prob_loss: 15.30   total_loss: 1106.74\n",
      "=> STEP   94/6250   lr: 0.000376   giou_loss: 2.90   conf_loss: 1087.00   prob_loss: 14.76   total_loss: 1104.66\n",
      "=> STEP   95/6250   lr: 0.000380   giou_loss: 2.90   conf_loss: 1085.13   prob_loss: 14.39   total_loss: 1102.41\n",
      "=> STEP   96/6250   lr: 0.000384   giou_loss: 2.90   conf_loss: 1083.14   prob_loss: 14.17   total_loss: 1100.21\n",
      "=> STEP   97/6250   lr: 0.000388   giou_loss: 2.90   conf_loss: 1081.64   prob_loss: 13.49   total_loss: 1098.03\n",
      "=> STEP   98/6250   lr: 0.000392   giou_loss: 2.90   conf_loss: 1080.37   prob_loss: 13.18   total_loss: 1096.45\n",
      "=> STEP   99/6250   lr: 0.000396   giou_loss: 2.90   conf_loss: 1078.48   prob_loss: 13.16   total_loss: 1094.54\n",
      "=> STEP  100/6250   lr: 0.000400   giou_loss: 2.90   conf_loss: 1076.98   prob_loss: 12.33   total_loss: 1092.21\n",
      "=> STEP  101/6250   lr: 0.000404   giou_loss: 2.90   conf_loss: 1074.65   prob_loss: 12.14   total_loss: 1089.68\n",
      "=> STEP  102/6250   lr: 0.000408   giou_loss: 2.90   conf_loss: 1072.61   prob_loss: 12.08   total_loss: 1087.59\n",
      "=> STEP  103/6250   lr: 0.000412   giou_loss: 2.90   conf_loss: 1071.48   prob_loss: 11.49   total_loss: 1085.88\n",
      "=> STEP  104/6250   lr: 0.000416   giou_loss: 2.90   conf_loss: 1069.96   prob_loss: 11.32   total_loss: 1084.18\n",
      "=> STEP  105/6250   lr: 0.000420   giou_loss: 2.90   conf_loss: 1068.67   prob_loss: 10.92   total_loss: 1082.50\n",
      "=> STEP  106/6250   lr: 0.000424   giou_loss: 2.90   conf_loss: 1066.45   prob_loss: 10.76   total_loss: 1080.11\n",
      "=> STEP  107/6250   lr: 0.000428   giou_loss: 2.90   conf_loss: 1064.30   prob_loss: 10.41   total_loss: 1077.61\n",
      "=> STEP  108/6250   lr: 0.000432   giou_loss: 2.90   conf_loss: 1062.60   prob_loss: 10.19   total_loss: 1075.69\n",
      "=> STEP  109/6250   lr: 0.000436   giou_loss: 2.90   conf_loss: 1060.86   prob_loss: 10.18   total_loss: 1073.94\n",
      "=> STEP  110/6250   lr: 0.000440   giou_loss: 2.90   conf_loss: 1060.15   prob_loss: 9.54   total_loss: 1072.59\n",
      "=> STEP  111/6250   lr: 0.000444   giou_loss: 2.90   conf_loss: 1058.35   prob_loss: 9.76   total_loss: 1071.00\n",
      "=> STEP  112/6250   lr: 0.000448   giou_loss: 2.90   conf_loss: 1056.97   prob_loss: 9.01   total_loss: 1068.87\n",
      "=> STEP  113/6250   lr: 0.000452   giou_loss: 2.90   conf_loss: 1054.92   prob_loss: 9.12   total_loss: 1066.94\n",
      "=> STEP  114/6250   lr: 0.000456   giou_loss: 2.90   conf_loss: 1053.62   prob_loss: 9.16   total_loss: 1065.67\n",
      "=> STEP  115/6250   lr: 0.000460   giou_loss: 2.90   conf_loss: 1053.41   prob_loss: 8.44   total_loss: 1064.75\n",
      "=> STEP  116/6250   lr: 0.000464   giou_loss: 2.90   conf_loss: 1050.64   prob_loss: 8.40   total_loss: 1061.94\n",
      "=> STEP  117/6250   lr: 0.000468   giou_loss: 2.90   conf_loss: 1050.16   prob_loss: 8.46   total_loss: 1061.52\n",
      "=> STEP  118/6250   lr: 0.000472   giou_loss: 2.90   conf_loss: 1048.20   prob_loss: 7.82   total_loss: 1058.92\n",
      "=> STEP  119/6250   lr: 0.000476   giou_loss: 2.90   conf_loss: 1046.41   prob_loss: 7.93   total_loss: 1057.24\n",
      "=> STEP  120/6250   lr: 0.000480   giou_loss: 2.90   conf_loss: 1045.60   prob_loss: 7.89   total_loss: 1056.39\n",
      "=> STEP  121/6250   lr: 0.000484   giou_loss: 2.90   conf_loss: 1042.48   prob_loss: 7.63   total_loss: 1053.00\n",
      "=> STEP  122/6250   lr: 0.000488   giou_loss: 2.90   conf_loss: 1041.87   prob_loss: 7.32   total_loss: 1052.09\n",
      "=> STEP  123/6250   lr: 0.000492   giou_loss: 2.90   conf_loss: 1039.40   prob_loss: 7.41   total_loss: 1049.71\n",
      "=> STEP  124/6250   lr: 0.000496   giou_loss: 2.90   conf_loss: 1037.90   prob_loss: 7.33   total_loss: 1048.13\n",
      "=> STEP  125/6250   lr: 0.000500   giou_loss: 2.90   conf_loss: 1037.03   prob_loss: 7.07   total_loss: 1046.99\n",
      "=> STEP  126/6250   lr: 0.000504   giou_loss: 2.90   conf_loss: 1034.85   prob_loss: 6.96   total_loss: 1044.71\n",
      "=> STEP  127/6250   lr: 0.000508   giou_loss: 2.90   conf_loss: 1033.32   prob_loss: 6.93   total_loss: 1043.15\n",
      "=> STEP  128/6250   lr: 0.000512   giou_loss: 2.90   conf_loss: 1031.46   prob_loss: 6.76   total_loss: 1041.12\n",
      "=> STEP  129/6250   lr: 0.000516   giou_loss: 2.90   conf_loss: 1030.01   prob_loss: 6.75   total_loss: 1039.65\n",
      "=> STEP  130/6250   lr: 0.000520   giou_loss: 2.90   conf_loss: 1029.05   prob_loss: 6.68   total_loss: 1038.63\n",
      "=> STEP  131/6250   lr: 0.000524   giou_loss: 2.90   conf_loss: 1027.48   prob_loss: 6.49   total_loss: 1036.87\n",
      "=> STEP  132/6250   lr: 0.000528   giou_loss: 2.90   conf_loss: 1025.96   prob_loss: 6.43   total_loss: 1035.29\n",
      "=> STEP  133/6250   lr: 0.000532   giou_loss: 2.90   conf_loss: 1024.13   prob_loss: 6.32   total_loss: 1033.35\n",
      "=> STEP  134/6250   lr: 0.000536   giou_loss: 2.90   conf_loss: 1022.43   prob_loss: 6.18   total_loss: 1031.50\n",
      "=> STEP  135/6250   lr: 0.000540   giou_loss: 2.90   conf_loss: 1020.99   prob_loss: 6.15   total_loss: 1030.04\n",
      "=> STEP  136/6250   lr: 0.000544   giou_loss: 2.90   conf_loss: 1019.60   prob_loss: 6.07   total_loss: 1028.57\n",
      "=> STEP  137/6250   lr: 0.000548   giou_loss: 2.90   conf_loss: 1017.89   prob_loss: 6.01   total_loss: 1026.80\n",
      "=> STEP  138/6250   lr: 0.000552   giou_loss: 2.90   conf_loss: 1016.36   prob_loss: 5.88   total_loss: 1025.14\n",
      "=> STEP  139/6250   lr: 0.000556   giou_loss: 2.90   conf_loss: 1014.90   prob_loss: 5.79   total_loss: 1023.59\n",
      "=> STEP  140/6250   lr: 0.000560   giou_loss: 2.90   conf_loss: 1013.34   prob_loss: 5.79   total_loss: 1022.03\n",
      "=> STEP  141/6250   lr: 0.000564   giou_loss: 2.90   conf_loss: 1012.20   prob_loss: 5.71   total_loss: 1020.80\n",
      "=> STEP  142/6250   lr: 0.000568   giou_loss: 2.90   conf_loss: 1010.87   prob_loss: 5.62   total_loss: 1019.39\n",
      "=> STEP  143/6250   lr: 0.000572   giou_loss: 2.90   conf_loss: 1010.33   prob_loss: 5.49   total_loss: 1018.71\n",
      "=> STEP  144/6250   lr: 0.000576   giou_loss: 2.90   conf_loss: 1009.52   prob_loss: 5.43   total_loss: 1017.84\n",
      "=> STEP  145/6250   lr: 0.000580   giou_loss: 2.90   conf_loss: 1008.11   prob_loss: 5.25   total_loss: 1016.26\n",
      "=> STEP  146/6250   lr: 0.000584   giou_loss: 2.90   conf_loss: 1006.56   prob_loss: 5.11   total_loss: 1014.57\n",
      "=> STEP  147/6250   lr: 0.000588   giou_loss: 2.90   conf_loss: 1004.79   prob_loss: 5.16   total_loss: 1012.85\n",
      "=> STEP  148/6250   lr: 0.000592   giou_loss: 2.90   conf_loss: 1003.81   prob_loss: 5.01   total_loss: 1011.72\n",
      "=> STEP  149/6250   lr: 0.000596   giou_loss: 2.90   conf_loss: 1003.55   prob_loss: 4.96   total_loss: 1011.40\n",
      "=> STEP  150/6250   lr: 0.000600   giou_loss: 2.90   conf_loss: 1001.33   prob_loss: 4.99   total_loss: 1009.22\n",
      "=> STEP  151/6250   lr: 0.000604   giou_loss: 2.90   conf_loss: 999.76   prob_loss: 5.07   total_loss: 1007.73\n",
      "=> STEP  152/6250   lr: 0.000608   giou_loss: 2.90   conf_loss: 998.82   prob_loss: 4.93   total_loss: 1006.65\n",
      "=> STEP  153/6250   lr: 0.000612   giou_loss: 2.90   conf_loss: 997.42   prob_loss: 4.84   total_loss: 1005.16\n",
      "=> STEP  154/6250   lr: 0.000616   giou_loss: 2.90   conf_loss: 996.12   prob_loss: 4.87   total_loss: 1003.89\n",
      "=> STEP  155/6250   lr: 0.000620   giou_loss: 2.89   conf_loss: 994.61   prob_loss: 4.83   total_loss: 1002.34\n",
      "=> STEP  156/6250   lr: 0.000624   giou_loss: 2.89   conf_loss: 993.33   prob_loss: 4.81   total_loss: 1001.03\n",
      "=> STEP  157/6250   lr: 0.000628   giou_loss: 2.89   conf_loss: 992.15   prob_loss: 4.80   total_loss: 999.84\n",
      "=> STEP  158/6250   lr: 0.000632   giou_loss: 2.89   conf_loss: 990.80   prob_loss: 4.75   total_loss: 998.45\n",
      "=> STEP  159/6250   lr: 0.000636   giou_loss: 2.89   conf_loss: 989.65   prob_loss: 4.72   total_loss: 997.26\n",
      "=> STEP  160/6250   lr: 0.000640   giou_loss: 2.89   conf_loss: 988.46   prob_loss: 4.73   total_loss: 996.08\n",
      "=> STEP  161/6250   lr: 0.000644   giou_loss: 2.89   conf_loss: 987.35   prob_loss: 4.66   total_loss: 994.91\n",
      "=> STEP  162/6250   lr: 0.000648   giou_loss: 2.89   conf_loss: 986.00   prob_loss: 4.59   total_loss: 993.48\n",
      "=> STEP  163/6250   lr: 0.000652   giou_loss: 2.89   conf_loss: 984.81   prob_loss: 4.55   total_loss: 992.26\n",
      "=> STEP  164/6250   lr: 0.000656   giou_loss: 2.89   conf_loss: 983.54   prob_loss: 4.53   total_loss: 990.95\n",
      "=> STEP  165/6250   lr: 0.000660   giou_loss: 2.89   conf_loss: 982.23   prob_loss: 4.55   total_loss: 989.67\n",
      "=> STEP  166/6250   lr: 0.000664   giou_loss: 2.89   conf_loss: 981.41   prob_loss: 4.53   total_loss: 988.83\n",
      "=> STEP  167/6250   lr: 0.000668   giou_loss: 2.89   conf_loss: 980.20   prob_loss: 4.44   total_loss: 987.52\n",
      "=> STEP  168/6250   lr: 0.000672   giou_loss: 2.89   conf_loss: 979.31   prob_loss: 4.44   total_loss: 986.64\n",
      "=> STEP  169/6250   lr: 0.000676   giou_loss: 2.89   conf_loss: 978.25   prob_loss: 4.38   total_loss: 985.52\n",
      "=> STEP  170/6250   lr: 0.000680   giou_loss: 2.89   conf_loss: 977.03   prob_loss: 4.36   total_loss: 984.27\n",
      "=> STEP  171/6250   lr: 0.000684   giou_loss: 2.89   conf_loss: 976.00   prob_loss: 4.30   total_loss: 983.19\n",
      "=> STEP  172/6250   lr: 0.000688   giou_loss: 2.88   conf_loss: 974.77   prob_loss: 4.28   total_loss: 981.94\n",
      "=> STEP  173/6250   lr: 0.000692   giou_loss: 2.88   conf_loss: 973.98   prob_loss: 4.28   total_loss: 981.14\n",
      "=> STEP  174/6250   lr: 0.000696   giou_loss: 2.88   conf_loss: 973.07   prob_loss: 4.27   total_loss: 980.22\n",
      "=> STEP  175/6250   lr: 0.000700   giou_loss: 2.88   conf_loss: 972.34   prob_loss: 4.21   total_loss: 979.43\n",
      "=> STEP  176/6250   lr: 0.000704   giou_loss: 2.88   conf_loss: 971.19   prob_loss: 4.17   total_loss: 978.24\n",
      "=> STEP  177/6250   lr: 0.000708   giou_loss: 2.88   conf_loss: 970.03   prob_loss: 4.14   total_loss: 977.05\n",
      "=> STEP  178/6250   lr: 0.000712   giou_loss: 2.88   conf_loss: 968.66   prob_loss: 4.12   total_loss: 975.65\n",
      "=> STEP  179/6250   lr: 0.000716   giou_loss: 2.88   conf_loss: 967.39   prob_loss: 4.08   total_loss: 974.35\n",
      "=> STEP  180/6250   lr: 0.000720   giou_loss: 2.87   conf_loss: 966.45   prob_loss: 4.04   total_loss: 973.36\n",
      "=> STEP  181/6250   lr: 0.000724   giou_loss: 2.87   conf_loss: 965.36   prob_loss: 4.00   total_loss: 972.23\n",
      "=> STEP  182/6250   lr: 0.000728   giou_loss: 2.87   conf_loss: 964.16   prob_loss: 3.96   total_loss: 971.00\n",
      "=> STEP  183/6250   lr: 0.000732   giou_loss: 2.87   conf_loss: 963.14   prob_loss: 3.92   total_loss: 969.93\n",
      "=> STEP  184/6250   lr: 0.000736   giou_loss: 2.87   conf_loss: 962.05   prob_loss: 3.89   total_loss: 968.81\n",
      "=> STEP  185/6250   lr: 0.000740   giou_loss: 2.87   conf_loss: 961.00   prob_loss: 3.85   total_loss: 967.72\n",
      "=> STEP  186/6250   lr: 0.000744   giou_loss: 2.87   conf_loss: 959.85   prob_loss: 3.82   total_loss: 966.53\n",
      "=> STEP  187/6250   lr: 0.000748   giou_loss: 2.86   conf_loss: 958.75   prob_loss: 3.79   total_loss: 965.41\n",
      "=> STEP  188/6250   lr: 0.000752   giou_loss: 2.86   conf_loss: 957.79   prob_loss: 3.76   total_loss: 964.42\n",
      "=> STEP  189/6250   lr: 0.000756   giou_loss: 2.86   conf_loss: 956.82   prob_loss: 3.72   total_loss: 963.39\n",
      "=> STEP  190/6250   lr: 0.000760   giou_loss: 2.86   conf_loss: 955.95   prob_loss: 3.73   total_loss: 962.54\n",
      "=> STEP  191/6250   lr: 0.000764   giou_loss: 2.85   conf_loss: 955.41   prob_loss: 3.65   total_loss: 961.92\n",
      "=> STEP  192/6250   lr: 0.000768   giou_loss: 2.85   conf_loss: 955.33   prob_loss: 3.70   total_loss: 961.89\n",
      "=> STEP  193/6250   lr: 0.000772   giou_loss: 2.85   conf_loss: 954.42   prob_loss: 3.59   total_loss: 960.86\n",
      "=> STEP  194/6250   lr: 0.000776   giou_loss: 2.85   conf_loss: 952.39   prob_loss: 3.64   total_loss: 958.87\n",
      "=> STEP  195/6250   lr: 0.000780   giou_loss: 2.84   conf_loss: 951.45   prob_loss: 3.58   total_loss: 957.87\n",
      "=> STEP  196/6250   lr: 0.000784   giou_loss: 2.83   conf_loss: 950.42   prob_loss: 3.47   total_loss: 956.72\n",
      "=> STEP  197/6250   lr: 0.000788   giou_loss: 2.83   conf_loss: 949.22   prob_loss: 3.43   total_loss: 955.48\n",
      "=> STEP  198/6250   lr: 0.000792   giou_loss: 2.82   conf_loss: 948.16   prob_loss: 3.41   total_loss: 954.39\n",
      "=> STEP  199/6250   lr: 0.000796   giou_loss: 2.81   conf_loss: 946.99   prob_loss: 3.40   total_loss: 953.19\n",
      "=> STEP  200/6250   lr: 0.000800   giou_loss: 2.79   conf_loss: 946.02   prob_loss: 3.38   total_loss: 952.18\n",
      "=> STEP  201/6250   lr: 0.000804   giou_loss: 2.76   conf_loss: 945.01   prob_loss: 3.30   total_loss: 951.06\n",
      "=> STEP  202/6250   lr: 0.000808   giou_loss: 2.72   conf_loss: 943.83   prob_loss: 3.29   total_loss: 949.84\n",
      "=> STEP  203/6250   lr: 0.000812   giou_loss: 2.67   conf_loss: 942.81   prob_loss: 3.29   total_loss: 948.77\n",
      "=> STEP  204/6250   lr: 0.000816   giou_loss: 2.55   conf_loss: 941.80   prob_loss: 3.26   total_loss: 947.61\n",
      "=> STEP  205/6250   lr: 0.000820   giou_loss: 2.32   conf_loss: 940.78   prob_loss: 3.26   total_loss: 946.35\n",
      "=> STEP  206/6250   lr: 0.000824   giou_loss: 1.95   conf_loss: 939.78   prob_loss: 3.28   total_loss: 945.00\n",
      "=> STEP  207/6250   lr: 0.000828   giou_loss: 1.81   conf_loss: 938.91   prob_loss: 3.23   total_loss: 943.95\n",
      "=> STEP  208/6250   lr: 0.000832   giou_loss: 1.72   conf_loss: 937.73   prob_loss: 3.26   total_loss: 942.71\n",
      "=> STEP  209/6250   lr: 0.000836   giou_loss: 2.03   conf_loss: 936.72   prob_loss: 3.35   total_loss: 942.10\n",
      "=> STEP  210/6250   lr: 0.000840   giou_loss: 2.14   conf_loss: 935.59   prob_loss: 3.28   total_loss: 941.01\n",
      "=> STEP  211/6250   lr: 0.000844   giou_loss: 1.97   conf_loss: 934.70   prob_loss: 3.19   total_loss: 939.86\n",
      "=> STEP  212/6250   lr: 0.000848   giou_loss: 1.62   conf_loss: 933.74   prob_loss: 3.09   total_loss: 938.45\n",
      "=> STEP  213/6250   lr: 0.000852   giou_loss: 2.07   conf_loss: 932.84   prob_loss: 3.05   total_loss: 937.97\n",
      "=> STEP  214/6250   lr: 0.000856   giou_loss: 2.11   conf_loss: 931.68   prob_loss: 3.09   total_loss: 936.88\n",
      "=> STEP  215/6250   lr: 0.000860   giou_loss: 1.75   conf_loss: 930.64   prob_loss: 3.17   total_loss: 935.56\n",
      "=> STEP  216/6250   lr: 0.000864   giou_loss: 2.03   conf_loss: 929.54   prob_loss: 3.26   total_loss: 934.82\n",
      "=> STEP  217/6250   lr: 0.000868   giou_loss: 2.16   conf_loss: 928.49   prob_loss: 3.20   total_loss: 933.86\n",
      "=> STEP  218/6250   lr: 0.000872   giou_loss: 2.15   conf_loss: 927.53   prob_loss: 3.11   total_loss: 932.79\n",
      "=> STEP  219/6250   lr: 0.000876   giou_loss: 1.86   conf_loss: 926.55   prob_loss: 3.04   total_loss: 931.45\n",
      "=> STEP  220/6250   lr: 0.000880   giou_loss: 1.75   conf_loss: 925.57   prob_loss: 3.00   total_loss: 930.31\n",
      "=> STEP  221/6250   lr: 0.000884   giou_loss: 1.77   conf_loss: 924.45   prob_loss: 3.02   total_loss: 929.24\n",
      "=> STEP  222/6250   lr: 0.000888   giou_loss: 1.88   conf_loss: 923.36   prob_loss: 3.07   total_loss: 928.31\n",
      "=> STEP  223/6250   lr: 0.000892   giou_loss: 1.82   conf_loss: 922.32   prob_loss: 3.06   total_loss: 927.20\n",
      "=> STEP  224/6250   lr: 0.000896   giou_loss: 1.72   conf_loss: 921.31   prob_loss: 3.03   total_loss: 926.06\n",
      "=> STEP  225/6250   lr: 0.000900   giou_loss: 1.75   conf_loss: 920.34   prob_loss: 3.01   total_loss: 925.10\n",
      "=> STEP  226/6250   lr: 0.000904   giou_loss: 1.56   conf_loss: 919.30   prob_loss: 3.02   total_loss: 923.88\n",
      "=> STEP  227/6250   lr: 0.000908   giou_loss: 1.77   conf_loss: 918.17   prob_loss: 3.00   total_loss: 922.94\n",
      "=> STEP  228/6250   lr: 0.000912   giou_loss: 1.76   conf_loss: 917.20   prob_loss: 2.97   total_loss: 921.93\n",
      "=> STEP  229/6250   lr: 0.000916   giou_loss: 1.80   conf_loss: 916.26   prob_loss: 2.89   total_loss: 920.96\n",
      "=> STEP  230/6250   lr: 0.000920   giou_loss: 1.85   conf_loss: 915.18   prob_loss: 2.87   total_loss: 919.90\n",
      "=> STEP  231/6250   lr: 0.000924   giou_loss: 1.69   conf_loss: 914.13   prob_loss: 2.91   total_loss: 918.73\n",
      "=> STEP  232/6250   lr: 0.000928   giou_loss: 1.66   conf_loss: 913.05   prob_loss: 2.91   total_loss: 917.62\n",
      "=> STEP  233/6250   lr: 0.000932   giou_loss: 1.78   conf_loss: 912.11   prob_loss: 2.90   total_loss: 916.79\n",
      "=> STEP  234/6250   lr: 0.000936   giou_loss: 1.82   conf_loss: 911.01   prob_loss: 2.86   total_loss: 915.69\n",
      "=> STEP  235/6250   lr: 0.000940   giou_loss: 1.71   conf_loss: 909.96   prob_loss: 2.87   total_loss: 914.53\n",
      "=> STEP  236/6250   lr: 0.000944   giou_loss: 1.78   conf_loss: 908.94   prob_loss: 2.88   total_loss: 913.60\n",
      "=> STEP  237/6250   lr: 0.000948   giou_loss: 1.69   conf_loss: 907.91   prob_loss: 2.84   total_loss: 912.44\n",
      "=> STEP  238/6250   lr: 0.000952   giou_loss: 1.59   conf_loss: 906.86   prob_loss: 2.84   total_loss: 911.29\n",
      "=> STEP  239/6250   lr: 0.000956   giou_loss: 1.65   conf_loss: 905.84   prob_loss: 2.81   total_loss: 910.31\n",
      "=> STEP  240/6250   lr: 0.000960   giou_loss: 1.60   conf_loss: 904.81   prob_loss: 2.82   total_loss: 909.23\n",
      "=> STEP  241/6250   lr: 0.000964   giou_loss: 1.85   conf_loss: 903.77   prob_loss: 2.79   total_loss: 908.41\n",
      "=> STEP  242/6250   lr: 0.000968   giou_loss: 1.73   conf_loss: 902.75   prob_loss: 2.79   total_loss: 907.26\n",
      "=> STEP  243/6250   lr: 0.000972   giou_loss: 1.66   conf_loss: 901.66   prob_loss: 2.79   total_loss: 906.10\n",
      "=> STEP  244/6250   lr: 0.000976   giou_loss: 1.68   conf_loss: 900.61   prob_loss: 2.79   total_loss: 905.08\n",
      "=> STEP  245/6250   lr: 0.000980   giou_loss: 1.62   conf_loss: 899.59   prob_loss: 2.76   total_loss: 903.97\n",
      "=> STEP  246/6250   lr: 0.000984   giou_loss: 1.88   conf_loss: 898.57   prob_loss: 2.74   total_loss: 903.19\n",
      "=> STEP  247/6250   lr: 0.000988   giou_loss: 1.81   conf_loss: 897.54   prob_loss: 2.75   total_loss: 902.09\n",
      "=> STEP  248/6250   lr: 0.000992   giou_loss: 1.57   conf_loss: 896.40   prob_loss: 2.76   total_loss: 900.73\n",
      "=> STEP  249/6250   lr: 0.000996   giou_loss: 1.97   conf_loss: 895.33   prob_loss: 2.77   total_loss: 900.07\n",
      "=> STEP  250/6250   lr: 0.001000   giou_loss: 2.17   conf_loss: 894.27   prob_loss: 2.76   total_loss: 899.19\n",
      "=> STEP  251/6250   lr: 0.001000   giou_loss: 2.13   conf_loss: 893.22   prob_loss: 2.74   total_loss: 898.09\n",
      "=> STEP  252/6250   lr: 0.001000   giou_loss: 2.03   conf_loss: 892.16   prob_loss: 2.72   total_loss: 896.91\n",
      "=> STEP  253/6250   lr: 0.001000   giou_loss: 1.77   conf_loss: 891.18   prob_loss: 2.69   total_loss: 895.63\n",
      "=> STEP  254/6250   lr: 0.001000   giou_loss: 1.79   conf_loss: 890.17   prob_loss: 2.66   total_loss: 894.61\n",
      "=> STEP  255/6250   lr: 0.001000   giou_loss: 1.88   conf_loss: 889.13   prob_loss: 2.67   total_loss: 893.69\n",
      "=> STEP  256/6250   lr: 0.001000   giou_loss: 1.74   conf_loss: 888.03   prob_loss: 2.70   total_loss: 892.47\n",
      "=> STEP  257/6250   lr: 0.001000   giou_loss: 1.71   conf_loss: 886.95   prob_loss: 2.72   total_loss: 891.38\n",
      "=> STEP  258/6250   lr: 0.001000   giou_loss: 1.79   conf_loss: 885.91   prob_loss: 2.70   total_loss: 890.40\n",
      "=> STEP  259/6250   lr: 0.001000   giou_loss: 1.65   conf_loss: 884.91   prob_loss: 2.67   total_loss: 889.23\n",
      "=> STEP  260/6250   lr: 0.001000   giou_loss: 1.78   conf_loss: 883.95   prob_loss: 2.63   total_loss: 888.35\n",
      "=> STEP  261/6250   lr: 0.001000   giou_loss: 1.80   conf_loss: 882.91   prob_loss: 2.62   total_loss: 887.33\n",
      "=> STEP  262/6250   lr: 0.001000   giou_loss: 1.56   conf_loss: 881.84   prob_loss: 2.65   total_loss: 886.04\n",
      "=> STEP  263/6250   lr: 0.001000   giou_loss: 1.70   conf_loss: 880.77   prob_loss: 2.65   total_loss: 885.12\n",
      "=> STEP  264/6250   lr: 0.001000   giou_loss: 1.62   conf_loss: 879.78   prob_loss: 2.63   total_loss: 884.03\n",
      "=> STEP  265/6250   lr: 0.001000   giou_loss: 1.75   conf_loss: 878.78   prob_loss: 2.60   total_loss: 883.13\n",
      "=> STEP  266/6250   lr: 0.001000   giou_loss: 1.73   conf_loss: 877.77   prob_loss: 2.59   total_loss: 882.09\n",
      "=> STEP  267/6250   lr: 0.001000   giou_loss: 1.63   conf_loss: 876.75   prob_loss: 2.59   total_loss: 880.97\n",
      "=> STEP  268/6250   lr: 0.001000   giou_loss: 1.76   conf_loss: 875.71   prob_loss: 2.60   total_loss: 880.07\n",
      "=> STEP  269/6250   lr: 0.001000   giou_loss: 1.79   conf_loss: 874.77   prob_loss: 2.58   total_loss: 879.15\n",
      "=> STEP  270/6250   lr: 0.001000   giou_loss: 1.74   conf_loss: 873.73   prob_loss: 2.57   total_loss: 878.04\n",
      "=> STEP  271/6250   lr: 0.001000   giou_loss: 1.81   conf_loss: 872.74   prob_loss: 2.56   total_loss: 877.11\n",
      "=> STEP  272/6250   lr: 0.001000   giou_loss: 1.80   conf_loss: 871.69   prob_loss: 2.57   total_loss: 876.06\n",
      "=> STEP  273/6250   lr: 0.001000   giou_loss: 1.58   conf_loss: 870.74   prob_loss: 2.56   total_loss: 874.88\n",
      "=> STEP  274/6250   lr: 0.001000   giou_loss: 1.87   conf_loss: 869.82   prob_loss: 2.54   total_loss: 874.23\n",
      "=> STEP  275/6250   lr: 0.001000   giou_loss: 1.93   conf_loss: 868.79   prob_loss: 2.53   total_loss: 873.25\n",
      "=> STEP  276/6250   lr: 0.001000   giou_loss: 1.81   conf_loss: 867.77   prob_loss: 2.53   total_loss: 872.11\n",
      "=> STEP  277/6250   lr: 0.001000   giou_loss: 1.73   conf_loss: 866.76   prob_loss: 2.54   total_loss: 871.03\n",
      "=> STEP  278/6250   lr: 0.001000   giou_loss: 1.71   conf_loss: 865.73   prob_loss: 2.54   total_loss: 869.98\n",
      "=> STEP  279/6250   lr: 0.001000   giou_loss: 1.78   conf_loss: 864.80   prob_loss: 2.52   total_loss: 869.10\n",
      "=> STEP  280/6250   lr: 0.001000   giou_loss: 1.83   conf_loss: 863.81   prob_loss: 2.50   total_loss: 868.13\n",
      "=> STEP  281/6250   lr: 0.001000   giou_loss: 1.67   conf_loss: 862.81   prob_loss: 2.49   total_loss: 866.98\n",
      "=> STEP  282/6250   lr: 0.001000   giou_loss: 1.83   conf_loss: 861.88   prob_loss: 2.50   total_loss: 866.22\n",
      "=> STEP  283/6250   lr: 0.001000   giou_loss: 1.96   conf_loss: 860.88   prob_loss: 2.50   total_loss: 865.33\n",
      "=> STEP  284/6250   lr: 0.001000   giou_loss: 1.77   conf_loss: 859.88   prob_loss: 2.48   total_loss: 864.13\n",
      "=> STEP  285/6250   lr: 0.001000   giou_loss: 1.65   conf_loss: 858.92   prob_loss: 2.48   total_loss: 863.04\n",
      "=> STEP  286/6250   lr: 0.001000   giou_loss: 1.69   conf_loss: 857.91   prob_loss: 2.46   total_loss: 862.06\n",
      "=> STEP  287/6250   lr: 0.001000   giou_loss: 1.59   conf_loss: 856.94   prob_loss: 2.45   total_loss: 860.98\n",
      "=> STEP  288/6250   lr: 0.001000   giou_loss: 1.75   conf_loss: 855.99   prob_loss: 2.45   total_loss: 860.18\n",
      "=> STEP  289/6250   lr: 0.001000   giou_loss: 1.83   conf_loss: 854.98   prob_loss: 2.44   total_loss: 859.25\n",
      "=> STEP  290/6250   lr: 0.001000   giou_loss: 1.56   conf_loss: 854.03   prob_loss: 2.44   total_loss: 858.04\n",
      "=> STEP  291/6250   lr: 0.001000   giou_loss: 1.72   conf_loss: 853.10   prob_loss: 2.43   total_loss: 857.25\n",
      "=> STEP  292/6250   lr: 0.001000   giou_loss: 1.56   conf_loss: 852.11   prob_loss: 2.41   total_loss: 856.08\n",
      "=> STEP  293/6250   lr: 0.001000   giou_loss: 1.79   conf_loss: 851.16   prob_loss: 2.40   total_loss: 855.35\n",
      "=> STEP  294/6250   lr: 0.001000   giou_loss: 1.58   conf_loss: 850.15   prob_loss: 2.41   total_loss: 854.14\n",
      "=> STEP  295/6250   lr: 0.001000   giou_loss: 1.82   conf_loss: 849.25   prob_loss: 2.41   total_loss: 853.48\n",
      "=> STEP  296/6250   lr: 0.001000   giou_loss: 1.82   conf_loss: 848.27   prob_loss: 2.41   total_loss: 852.50\n",
      "=> STEP  297/6250   lr: 0.001000   giou_loss: 1.72   conf_loss: 847.30   prob_loss: 2.40   total_loss: 851.42\n",
      "=> STEP  298/6250   lr: 0.001000   giou_loss: 1.85   conf_loss: 846.34   prob_loss: 2.40   total_loss: 850.59\n",
      "=> STEP  299/6250   lr: 0.001000   giou_loss: 1.96   conf_loss: 845.37   prob_loss: 2.40   total_loss: 849.72\n",
      "=> STEP  300/6250   lr: 0.001000   giou_loss: 1.97   conf_loss: 844.43   prob_loss: 2.39   total_loss: 848.79\n",
      "=> STEP  301/6250   lr: 0.001000   giou_loss: 1.75   conf_loss: 843.49   prob_loss: 2.37   total_loss: 847.61\n",
      "=> STEP  302/6250   lr: 0.001000   giou_loss: 1.73   conf_loss: 842.57   prob_loss: 2.37   total_loss: 846.66\n",
      "=> STEP  303/6250   lr: 0.001000   giou_loss: 1.73   conf_loss: 841.62   prob_loss: 2.37   total_loss: 845.72\n",
      "=> STEP  304/6250   lr: 0.001000   giou_loss: 1.63   conf_loss: 840.67   prob_loss: 2.38   total_loss: 844.67\n",
      "=> STEP  305/6250   lr: 0.001000   giou_loss: 1.70   conf_loss: 839.72   prob_loss: 2.36   total_loss: 843.78\n",
      "=> STEP  306/6250   lr: 0.001000   giou_loss: 1.59   conf_loss: 838.78   prob_loss: 2.35   total_loss: 842.73\n",
      "=> STEP  307/6250   lr: 0.001000   giou_loss: 1.83   conf_loss: 837.89   prob_loss: 2.35   total_loss: 842.06\n",
      "=> STEP  308/6250   lr: 0.001000   giou_loss: 1.75   conf_loss: 836.97   prob_loss: 2.33   total_loss: 841.05\n",
      "=> STEP  309/6250   lr: 0.001000   giou_loss: 1.67   conf_loss: 836.01   prob_loss: 2.32   total_loss: 839.99\n",
      "=> STEP  310/6250   lr: 0.001000   giou_loss: 1.75   conf_loss: 835.05   prob_loss: 2.33   total_loss: 839.14\n",
      "=> STEP  311/6250   lr: 0.001000   giou_loss: 1.74   conf_loss: 834.10   prob_loss: 2.34   total_loss: 838.19\n",
      "=> STEP  312/6250   lr: 0.001000   giou_loss: 1.56   conf_loss: 833.21   prob_loss: 2.32   total_loss: 837.09\n",
      "=> STEP  313/6250   lr: 0.001000   giou_loss: 1.75   conf_loss: 832.30   prob_loss: 2.30   total_loss: 836.35\n",
      "=> STEP  314/6250   lr: 0.001000   giou_loss: 1.62   conf_loss: 831.35   prob_loss: 2.31   total_loss: 835.27\n",
      "=> STEP  315/6250   lr: 0.001000   giou_loss: 1.78   conf_loss: 830.42   prob_loss: 2.31   total_loss: 834.51\n",
      "=> STEP  316/6250   lr: 0.001000   giou_loss: 1.85   conf_loss: 829.48   prob_loss: 2.29   total_loss: 833.62\n",
      "=> STEP  317/6250   lr: 0.001000   giou_loss: 1.74   conf_loss: 828.59   prob_loss: 2.29   total_loss: 832.62\n",
      "=> STEP  318/6250   lr: 0.001000   giou_loss: 1.71   conf_loss: 827.67   prob_loss: 2.29   total_loss: 831.67\n",
      "=> STEP  319/6250   lr: 0.001000   giou_loss: 1.59   conf_loss: 826.78   prob_loss: 2.27   total_loss: 830.64\n",
      "=> STEP  320/6250   lr: 0.001000   giou_loss: 1.89   conf_loss: 825.84   prob_loss: 2.25   total_loss: 829.99\n",
      "=> STEP  321/6250   lr: 0.001000   giou_loss: 1.85   conf_loss: 824.91   prob_loss: 2.27   total_loss: 829.02\n",
      "=> STEP  322/6250   lr: 0.001000   giou_loss: 1.70   conf_loss: 823.99   prob_loss: 2.30   total_loss: 827.98\n",
      "=> STEP  323/6250   lr: 0.001000   giou_loss: 1.85   conf_loss: 823.15   prob_loss: 2.28   total_loss: 827.28\n",
      "=> STEP  324/6250   lr: 0.001000   giou_loss: 1.77   conf_loss: 822.21   prob_loss: 2.25   total_loss: 826.23\n",
      "=> STEP  325/6250   lr: 0.001000   giou_loss: 1.78   conf_loss: 821.33   prob_loss: 2.24   total_loss: 825.34\n",
      "=> STEP  326/6250   lr: 0.001000   giou_loss: 1.83   conf_loss: 820.36   prob_loss: 2.25   total_loss: 824.44\n",
      "=> STEP  327/6250   lr: 0.001000   giou_loss: 1.64   conf_loss: 819.42   prob_loss: 2.28   total_loss: 823.34\n",
      "=> STEP  328/6250   lr: 0.001000   giou_loss: 1.66   conf_loss: 818.58   prob_loss: 2.26   total_loss: 822.50\n",
      "=> STEP  329/6250   lr: 0.001000   giou_loss: 1.70   conf_loss: 817.66   prob_loss: 2.22   total_loss: 821.59\n",
      "=> STEP  330/6250   lr: 0.001000   giou_loss: 1.71   conf_loss: 816.78   prob_loss: 2.22   total_loss: 820.70\n",
      "=> STEP  331/6250   lr: 0.001000   giou_loss: 1.65   conf_loss: 815.85   prob_loss: 2.24   total_loss: 819.73\n",
      "=> STEP  332/6250   lr: 0.001000   giou_loss: 1.74   conf_loss: 814.96   prob_loss: 2.24   total_loss: 818.95\n",
      "=> STEP  333/6250   lr: 0.001000   giou_loss: 1.71   conf_loss: 814.07   prob_loss: 2.22   total_loss: 818.01\n",
      "=> STEP  334/6250   lr: 0.001000   giou_loss: 1.63   conf_loss: 813.21   prob_loss: 2.20   total_loss: 817.03\n",
      "=> STEP  335/6250   lr: 0.001000   giou_loss: 1.63   conf_loss: 812.30   prob_loss: 2.19   total_loss: 816.12\n",
      "=> STEP  336/6250   lr: 0.001000   giou_loss: 1.69   conf_loss: 811.39   prob_loss: 2.20   total_loss: 815.28\n",
      "=> STEP  337/6250   lr: 0.001000   giou_loss: 1.61   conf_loss: 810.52   prob_loss: 2.20   total_loss: 814.33\n",
      "=> STEP  338/6250   lr: 0.000999   giou_loss: 1.70   conf_loss: 809.63   prob_loss: 2.19   total_loss: 813.52\n",
      "=> STEP  339/6250   lr: 0.000999   giou_loss: 1.74   conf_loss: 808.76   prob_loss: 2.17   total_loss: 812.68\n",
      "=> STEP  340/6250   lr: 0.000999   giou_loss: 1.85   conf_loss: 807.83   prob_loss: 2.18   total_loss: 811.86\n",
      "=> STEP  341/6250   lr: 0.000999   giou_loss: 1.72   conf_loss: 806.95   prob_loss: 2.20   total_loss: 810.87\n",
      "=> STEP  342/6250   lr: 0.000999   giou_loss: 1.69   conf_loss: 806.07   prob_loss: 2.19   total_loss: 809.95\n",
      "=> STEP  343/6250   lr: 0.000999   giou_loss: 1.65   conf_loss: 805.21   prob_loss: 2.17   total_loss: 809.03\n",
      "=> STEP  344/6250   lr: 0.000999   giou_loss: 1.85   conf_loss: 804.35   prob_loss: 2.15   total_loss: 808.35\n",
      "=> STEP  345/6250   lr: 0.000999   giou_loss: 1.84   conf_loss: 803.49   prob_loss: 2.16   total_loss: 807.49\n",
      "=> STEP  346/6250   lr: 0.000999   giou_loss: 1.80   conf_loss: 802.58   prob_loss: 2.17   total_loss: 806.54\n",
      "=> STEP  347/6250   lr: 0.000999   giou_loss: 1.82   conf_loss: 801.73   prob_loss: 2.18   total_loss: 805.74\n",
      "=> STEP  348/6250   lr: 0.000999   giou_loss: 1.69   conf_loss: 800.85   prob_loss: 2.18   total_loss: 804.72\n",
      "=> STEP  349/6250   lr: 0.000999   giou_loss: 1.72   conf_loss: 799.96   prob_loss: 2.18   total_loss: 803.86\n",
      "=> STEP  350/6250   lr: 0.000999   giou_loss: 1.64   conf_loss: 799.16   prob_loss: 2.15   total_loss: 802.94\n",
      "=> STEP  351/6250   lr: 0.000999   giou_loss: 1.80   conf_loss: 798.28   prob_loss: 2.13   total_loss: 802.21\n",
      "=> STEP  352/6250   lr: 0.000999   giou_loss: 1.92   conf_loss: 797.41   prob_loss: 2.13   total_loss: 801.46\n",
      "=> STEP  353/6250   lr: 0.000999   giou_loss: 1.82   conf_loss: 796.50   prob_loss: 2.14   total_loss: 800.46\n",
      "=> STEP  354/6250   lr: 0.000999   giou_loss: 1.75   conf_loss: 795.68   prob_loss: 2.13   total_loss: 799.55\n",
      "=> STEP  355/6250   lr: 0.000999   giou_loss: 1.65   conf_loss: 794.80   prob_loss: 2.12   total_loss: 798.58\n",
      "=> STEP  356/6250   lr: 0.000999   giou_loss: 1.69   conf_loss: 793.98   prob_loss: 2.11   total_loss: 797.78\n",
      "=> STEP  357/6250   lr: 0.000999   giou_loss: 1.65   conf_loss: 793.06   prob_loss: 2.11   total_loss: 796.82\n",
      "=> STEP  358/6250   lr: 0.000999   giou_loss: 1.63   conf_loss: 792.19   prob_loss: 2.12   total_loss: 795.94\n",
      "=> STEP  359/6250   lr: 0.000999   giou_loss: 1.68   conf_loss: 791.39   prob_loss: 2.11   total_loss: 795.18\n",
      "=> STEP  360/6250   lr: 0.000999   giou_loss: 1.59   conf_loss: 790.48   prob_loss: 2.11   total_loss: 794.18\n",
      "=> STEP  361/6250   lr: 0.000999   giou_loss: 1.75   conf_loss: 789.63   prob_loss: 2.11   total_loss: 793.49\n",
      "=> STEP  362/6250   lr: 0.000999   giou_loss: 1.79   conf_loss: 788.75   prob_loss: 2.12   total_loss: 792.66\n",
      "=> STEP  363/6250   lr: 0.000999   giou_loss: 1.65   conf_loss: 787.91   prob_loss: 2.10   total_loss: 791.67\n",
      "=> STEP  364/6250   lr: 0.000999   giou_loss: 1.73   conf_loss: 787.17   prob_loss: 2.07   total_loss: 790.98\n",
      "=> STEP  365/6250   lr: 0.000999   giou_loss: 1.74   conf_loss: 786.29   prob_loss: 2.08   total_loss: 790.10\n",
      "=> STEP  366/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 785.39   prob_loss: 2.09   total_loss: 789.05\n",
      "=> STEP  367/6250   lr: 0.000999   giou_loss: 1.69   conf_loss: 784.55   prob_loss: 2.10   total_loss: 788.34\n",
      "=> STEP  368/6250   lr: 0.000999   giou_loss: 1.63   conf_loss: 783.64   prob_loss: 2.08   total_loss: 787.35\n",
      "=> STEP  369/6250   lr: 0.000999   giou_loss: 1.71   conf_loss: 782.88   prob_loss: 2.06   total_loss: 786.65\n",
      "=> STEP  370/6250   lr: 0.000999   giou_loss: 1.64   conf_loss: 782.01   prob_loss: 2.06   total_loss: 785.71\n",
      "=> STEP  371/6250   lr: 0.000999   giou_loss: 1.76   conf_loss: 781.16   prob_loss: 2.09   total_loss: 785.00\n",
      "=> STEP  372/6250   lr: 0.000999   giou_loss: 1.77   conf_loss: 780.29   prob_loss: 2.08   total_loss: 784.14\n",
      "=> STEP  373/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 779.47   prob_loss: 2.06   total_loss: 783.09\n",
      "=> STEP  374/6250   lr: 0.000999   giou_loss: 1.75   conf_loss: 778.69   prob_loss: 2.03   total_loss: 782.47\n",
      "=> STEP  375/6250   lr: 0.000999   giou_loss: 1.63   conf_loss: 777.80   prob_loss: 2.05   total_loss: 781.48\n",
      "=> STEP  376/6250   lr: 0.000999   giou_loss: 1.69   conf_loss: 776.94   prob_loss: 2.08   total_loss: 780.71\n",
      "=> STEP  377/6250   lr: 0.000999   giou_loss: 1.67   conf_loss: 776.10   prob_loss: 2.06   total_loss: 779.83\n",
      "=> STEP  378/6250   lr: 0.000999   giou_loss: 1.58   conf_loss: 775.29   prob_loss: 2.03   total_loss: 778.91\n",
      "=> STEP  379/6250   lr: 0.000999   giou_loss: 1.86   conf_loss: 774.45   prob_loss: 2.02   total_loss: 778.34\n",
      "=> STEP  380/6250   lr: 0.000999   giou_loss: 1.81   conf_loss: 773.59   prob_loss: 2.04   total_loss: 777.44\n",
      "=> STEP  381/6250   lr: 0.000999   giou_loss: 1.66   conf_loss: 772.76   prob_loss: 2.06   total_loss: 776.48\n",
      "=> STEP  382/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 771.96   prob_loss: 2.04   total_loss: 775.56\n",
      "=> STEP  383/6250   lr: 0.000999   giou_loss: 1.57   conf_loss: 771.13   prob_loss: 2.03   total_loss: 774.73\n",
      "=> STEP  384/6250   lr: 0.000999   giou_loss: 1.66   conf_loss: 770.36   prob_loss: 2.02   total_loss: 774.04\n",
      "=> STEP  385/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 769.51   prob_loss: 2.02   total_loss: 773.08\n",
      "=> STEP  386/6250   lr: 0.000999   giou_loss: 1.62   conf_loss: 768.66   prob_loss: 2.01   total_loss: 772.29\n",
      "=> STEP  387/6250   lr: 0.000999   giou_loss: 1.59   conf_loss: 767.85   prob_loss: 2.00   total_loss: 771.44\n",
      "=> STEP  388/6250   lr: 0.000999   giou_loss: 1.63   conf_loss: 766.99   prob_loss: 2.00   total_loss: 770.62\n",
      "=> STEP  389/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 766.17   prob_loss: 2.00   total_loss: 769.73\n",
      "=> STEP  390/6250   lr: 0.000999   giou_loss: 1.62   conf_loss: 765.36   prob_loss: 2.00   total_loss: 768.98\n",
      "=> STEP  391/6250   lr: 0.000999   giou_loss: 1.62   conf_loss: 764.50   prob_loss: 2.00   total_loss: 768.13\n",
      "=> STEP  392/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 763.68   prob_loss: 2.00   total_loss: 767.24\n",
      "=> STEP  393/6250   lr: 0.000999   giou_loss: 1.56   conf_loss: 762.87   prob_loss: 1.99   total_loss: 766.43\n",
      "=> STEP  394/6250   lr: 0.000999   giou_loss: 1.58   conf_loss: 762.05   prob_loss: 1.99   total_loss: 765.62\n",
      "=> STEP  395/6250   lr: 0.000999   giou_loss: 1.72   conf_loss: 761.23   prob_loss: 1.99   total_loss: 764.94\n",
      "=> STEP  396/6250   lr: 0.000999   giou_loss: 1.70   conf_loss: 760.42   prob_loss: 1.98   total_loss: 764.11\n",
      "=> STEP  397/6250   lr: 0.000999   giou_loss: 1.59   conf_loss: 759.62   prob_loss: 1.97   total_loss: 763.19\n",
      "=> STEP  398/6250   lr: 0.000999   giou_loss: 1.74   conf_loss: 758.83   prob_loss: 1.96   total_loss: 762.53\n",
      "=> STEP  399/6250   lr: 0.000999   giou_loss: 1.67   conf_loss: 758.02   prob_loss: 1.96   total_loss: 761.66\n",
      "=> STEP  400/6250   lr: 0.000999   giou_loss: 1.60   conf_loss: 757.20   prob_loss: 1.99   total_loss: 760.80\n",
      "=> STEP  401/6250   lr: 0.000999   giou_loss: 1.68   conf_loss: 756.40   prob_loss: 1.98   total_loss: 760.06\n",
      "=> STEP  402/6250   lr: 0.000998   giou_loss: 1.57   conf_loss: 755.57   prob_loss: 1.96   total_loss: 759.10\n",
      "=> STEP  403/6250   lr: 0.000998   giou_loss: 1.82   conf_loss: 754.84   prob_loss: 1.95   total_loss: 758.61\n",
      "=> STEP  404/6250   lr: 0.000998   giou_loss: 1.91   conf_loss: 754.04   prob_loss: 1.95   total_loss: 757.89\n",
      "=> STEP  405/6250   lr: 0.000998   giou_loss: 1.72   conf_loss: 753.16   prob_loss: 1.97   total_loss: 756.86\n",
      "=> STEP  406/6250   lr: 0.000998   giou_loss: 1.77   conf_loss: 752.38   prob_loss: 2.01   total_loss: 756.16\n",
      "=> STEP  407/6250   lr: 0.000998   giou_loss: 1.95   conf_loss: 751.58   prob_loss: 1.99   total_loss: 755.51\n",
      "=> STEP  408/6250   lr: 0.000998   giou_loss: 1.85   conf_loss: 750.79   prob_loss: 1.96   total_loss: 754.60\n",
      "=> STEP  409/6250   lr: 0.000998   giou_loss: 1.57   conf_loss: 750.06   prob_loss: 1.92   total_loss: 753.54\n",
      "=> STEP  410/6250   lr: 0.000998   giou_loss: 1.81   conf_loss: 749.27   prob_loss: 1.92   total_loss: 753.00\n",
      "=> STEP  411/6250   lr: 0.000998   giou_loss: 1.70   conf_loss: 748.41   prob_loss: 1.95   total_loss: 752.06\n",
      "=> STEP  412/6250   lr: 0.000998   giou_loss: 1.71   conf_loss: 747.60   prob_loss: 1.99   total_loss: 751.30\n",
      "=> STEP  413/6250   lr: 0.000998   giou_loss: 1.89   conf_loss: 746.80   prob_loss: 1.98   total_loss: 750.67\n",
      "=> STEP  414/6250   lr: 0.000998   giou_loss: 1.78   conf_loss: 746.04   prob_loss: 1.95   total_loss: 749.77\n",
      "=> STEP  415/6250   lr: 0.000998   giou_loss: 1.61   conf_loss: 745.26   prob_loss: 1.92   total_loss: 748.79\n",
      "=> STEP  416/6250   lr: 0.000998   giou_loss: 2.01   conf_loss: 744.50   prob_loss: 1.92   total_loss: 748.43\n",
      "=> STEP  417/6250   lr: 0.000998   giou_loss: 2.15   conf_loss: 743.70   prob_loss: 1.92   total_loss: 747.78\n",
      "=> STEP  418/6250   lr: 0.000998   giou_loss: 2.04   conf_loss: 742.89   prob_loss: 1.93   total_loss: 746.87\n",
      "=> STEP  419/6250   lr: 0.000998   giou_loss: 2.11   conf_loss: 742.13   prob_loss: 1.93   total_loss: 746.17\n",
      "=> STEP  420/6250   lr: 0.000998   giou_loss: 1.84   conf_loss: 741.35   prob_loss: 1.95   total_loss: 745.14\n",
      "=> STEP  421/6250   lr: 0.000998   giou_loss: 1.78   conf_loss: 740.55   prob_loss: 1.97   total_loss: 744.30\n",
      "=> STEP  422/6250   lr: 0.000998   giou_loss: 1.77   conf_loss: 739.76   prob_loss: 1.97   total_loss: 743.51\n",
      "=> STEP  423/6250   lr: 0.000998   giou_loss: 1.94   conf_loss: 738.99   prob_loss: 1.94   total_loss: 742.87\n",
      "=> STEP  424/6250   lr: 0.000998   giou_loss: 1.89   conf_loss: 738.24   prob_loss: 1.91   total_loss: 742.04\n",
      "=> STEP  425/6250   lr: 0.000998   giou_loss: 1.58   conf_loss: 737.47   prob_loss: 1.90   total_loss: 740.96\n",
      "=> STEP  426/6250   lr: 0.000998   giou_loss: 1.86   conf_loss: 736.77   prob_loss: 1.88   total_loss: 740.52\n",
      "=> STEP  427/6250   lr: 0.000998   giou_loss: 1.94   conf_loss: 735.95   prob_loss: 1.90   total_loss: 739.79\n",
      "=> STEP  428/6250   lr: 0.000998   giou_loss: 1.84   conf_loss: 735.16   prob_loss: 1.93   total_loss: 738.93\n",
      "=> STEP  429/6250   lr: 0.000998   giou_loss: 1.60   conf_loss: 734.34   prob_loss: 1.95   total_loss: 737.89\n",
      "=> STEP  430/6250   lr: 0.000998   giou_loss: 1.84   conf_loss: 733.55   prob_loss: 1.96   total_loss: 737.36\n",
      "=> STEP  431/6250   lr: 0.000998   giou_loss: 1.99   conf_loss: 732.82   prob_loss: 1.92   total_loss: 736.73\n",
      "=> STEP  432/6250   lr: 0.000998   giou_loss: 1.97   conf_loss: 732.05   prob_loss: 1.90   total_loss: 735.93\n",
      "=> STEP  433/6250   lr: 0.000998   giou_loss: 1.86   conf_loss: 731.30   prob_loss: 1.88   total_loss: 735.04\n",
      "=> STEP  434/6250   lr: 0.000998   giou_loss: 1.60   conf_loss: 730.56   prob_loss: 1.86   total_loss: 734.02\n",
      "=> STEP  435/6250   lr: 0.000998   giou_loss: 1.89   conf_loss: 729.83   prob_loss: 1.85   total_loss: 733.58\n",
      "=> STEP  436/6250   lr: 0.000998   giou_loss: 2.03   conf_loss: 729.05   prob_loss: 1.87   total_loss: 732.95\n",
      "=> STEP  437/6250   lr: 0.000998   giou_loss: 1.96   conf_loss: 728.25   prob_loss: 1.90   total_loss: 732.11\n",
      "=> STEP  438/6250   lr: 0.000998   giou_loss: 1.78   conf_loss: 727.47   prob_loss: 1.92   total_loss: 731.18\n",
      "=> STEP  439/6250   lr: 0.000998   giou_loss: 1.62   conf_loss: 726.66   prob_loss: 1.93   total_loss: 730.22\n",
      "=> STEP  440/6250   lr: 0.000998   giou_loss: 1.73   conf_loss: 725.91   prob_loss: 1.91   total_loss: 729.55\n",
      "=> STEP  441/6250   lr: 0.000998   giou_loss: 1.64   conf_loss: 725.18   prob_loss: 1.88   total_loss: 728.70\n",
      "=> STEP  442/6250   lr: 0.000998   giou_loss: 1.60   conf_loss: 724.43   prob_loss: 1.86   total_loss: 727.89\n",
      "=> STEP  443/6250   lr: 0.000998   giou_loss: 1.68   conf_loss: 723.66   prob_loss: 1.89   total_loss: 727.23\n",
      "=> STEP  444/6250   lr: 0.000998   giou_loss: 1.66   conf_loss: 722.86   prob_loss: 1.87   total_loss: 726.40\n",
      "=> STEP  445/6250   lr: 0.000998   giou_loss: 1.65   conf_loss: 722.16   prob_loss: 1.85   total_loss: 725.66\n",
      "=> STEP  446/6250   lr: 0.000998   giou_loss: 1.57   conf_loss: 721.38   prob_loss: 1.84   total_loss: 724.79\n",
      "=> STEP  447/6250   lr: 0.000997   giou_loss: 1.56   conf_loss: 720.64   prob_loss: 1.84   total_loss: 724.05\n",
      "=> STEP  448/6250   lr: 0.000997   giou_loss: 1.56   conf_loss: 719.84   prob_loss: 1.86   total_loss: 723.26\n",
      "=> STEP  449/6250   lr: 0.000997   giou_loss: 1.56   conf_loss: 719.08   prob_loss: 1.85   total_loss: 722.50\n",
      "=> STEP  450/6250   lr: 0.000997   giou_loss: 1.58   conf_loss: 718.32   prob_loss: 1.84   total_loss: 721.74\n",
      "=> STEP  451/6250   lr: 0.000997   giou_loss: 1.87   conf_loss: 717.55   prob_loss: 1.84   total_loss: 721.26\n",
      "=> STEP  452/6250   lr: 0.000997   giou_loss: 1.91   conf_loss: 716.80   prob_loss: 1.84   total_loss: 720.54\n",
      "=> STEP  453/6250   lr: 0.000997   giou_loss: 1.64   conf_loss: 716.05   prob_loss: 1.83   total_loss: 719.52\n",
      "=> STEP  454/6250   lr: 0.000997   giou_loss: 1.91   conf_loss: 715.35   prob_loss: 1.82   total_loss: 719.09\n",
      "=> STEP  455/6250   lr: 0.000997   giou_loss: 1.94   conf_loss: 714.60   prob_loss: 1.83   total_loss: 718.37\n",
      "=> STEP  456/6250   lr: 0.000997   giou_loss: 1.69   conf_loss: 713.83   prob_loss: 1.84   total_loss: 717.35\n",
      "=> STEP  457/6250   lr: 0.000997   giou_loss: 1.90   conf_loss: 713.07   prob_loss: 1.86   total_loss: 716.82\n",
      "=> STEP  458/6250   lr: 0.000997   giou_loss: 1.97   conf_loss: 712.31   prob_loss: 1.85   total_loss: 716.13\n",
      "=> STEP  459/6250   lr: 0.000997   giou_loss: 1.74   conf_loss: 711.58   prob_loss: 1.83   total_loss: 715.15\n",
      "=> STEP  460/6250   lr: 0.000997   giou_loss: 1.86   conf_loss: 710.89   prob_loss: 1.82   total_loss: 714.57\n",
      "=> STEP  461/6250   lr: 0.000997   giou_loss: 1.98   conf_loss: 710.14   prob_loss: 1.82   total_loss: 713.93\n",
      "=> STEP  462/6250   lr: 0.000997   giou_loss: 1.79   conf_loss: 709.38   prob_loss: 1.83   total_loss: 713.00\n",
      "=> STEP  463/6250   lr: 0.000997   giou_loss: 1.72   conf_loss: 708.63   prob_loss: 1.82   total_loss: 712.17\n",
      "=> STEP  464/6250   lr: 0.000997   giou_loss: 1.83   conf_loss: 707.94   prob_loss: 1.80   total_loss: 711.57\n",
      "=> STEP  465/6250   lr: 0.000997   giou_loss: 1.72   conf_loss: 707.21   prob_loss: 1.79   total_loss: 710.72\n",
      "=> STEP  466/6250   lr: 0.000997   giou_loss: 1.62   conf_loss: 706.47   prob_loss: 1.81   total_loss: 709.90\n",
      "=> STEP  467/6250   lr: 0.000997   giou_loss: 1.74   conf_loss: 705.74   prob_loss: 1.83   total_loss: 709.31\n",
      "=> STEP  468/6250   lr: 0.000997   giou_loss: 1.68   conf_loss: 705.00   prob_loss: 1.81   total_loss: 708.49\n",
      "=> STEP  469/6250   lr: 0.000997   giou_loss: 1.66   conf_loss: 704.30   prob_loss: 1.77   total_loss: 707.73\n",
      "=> STEP  470/6250   lr: 0.000997   giou_loss: 1.66   conf_loss: 703.58   prob_loss: 1.78   total_loss: 707.02\n",
      "=> STEP  471/6250   lr: 0.000997   giou_loss: 1.58   conf_loss: 702.83   prob_loss: 1.80   total_loss: 706.21\n",
      "=> STEP  472/6250   lr: 0.000997   giou_loss: 1.84   conf_loss: 702.08   prob_loss: 1.80   total_loss: 705.73\n",
      "=> STEP  473/6250   lr: 0.000997   giou_loss: 1.76   conf_loss: 701.36   prob_loss: 1.80   total_loss: 704.92\n",
      "=> STEP  474/6250   lr: 0.000997   giou_loss: 1.72   conf_loss: 700.64   prob_loss: 1.79   total_loss: 704.15\n",
      "=> STEP  475/6250   lr: 0.000997   giou_loss: 1.93   conf_loss: 699.96   prob_loss: 1.78   total_loss: 703.67\n",
      "=> STEP  476/6250   lr: 0.000997   giou_loss: 1.79   conf_loss: 699.32   prob_loss: 1.77   total_loss: 702.88\n",
      "=> STEP  477/6250   lr: 0.000997   giou_loss: 1.76   conf_loss: 698.57   prob_loss: 1.76   total_loss: 702.09\n",
      "=> STEP  478/6250   lr: 0.000997   giou_loss: 1.93   conf_loss: 697.83   prob_loss: 1.77   total_loss: 701.54\n",
      "=> STEP  479/6250   lr: 0.000997   giou_loss: 1.78   conf_loss: 697.13   prob_loss: 1.78   total_loss: 700.70\n",
      "=> STEP  480/6250   lr: 0.000997   giou_loss: 1.74   conf_loss: 696.38   prob_loss: 1.80   total_loss: 699.92\n",
      "=> STEP  481/6250   lr: 0.000997   giou_loss: 1.81   conf_loss: 695.65   prob_loss: 1.78   total_loss: 699.24\n",
      "=> STEP  482/6250   lr: 0.000996   giou_loss: 1.62   conf_loss: 694.95   prob_loss: 1.77   total_loss: 698.33\n",
      "=> STEP  483/6250   lr: 0.000996   giou_loss: 1.92   conf_loss: 694.27   prob_loss: 1.76   total_loss: 697.95\n",
      "=> STEP  484/6250   lr: 0.000996   giou_loss: 1.96   conf_loss: 693.51   prob_loss: 1.75   total_loss: 697.23\n",
      "=> STEP  485/6250   lr: 0.000996   giou_loss: 1.67   conf_loss: 692.78   prob_loss: 1.76   total_loss: 696.21\n",
      "=> STEP  486/6250   lr: 0.000996   giou_loss: 1.81   conf_loss: 692.06   prob_loss: 1.78   total_loss: 695.65\n",
      "=> STEP  487/6250   lr: 0.000996   giou_loss: 1.85   conf_loss: 691.32   prob_loss: 1.77   total_loss: 694.95\n",
      "=> STEP  488/6250   lr: 0.000996   giou_loss: 1.67   conf_loss: 690.64   prob_loss: 1.77   total_loss: 694.07\n",
      "=> STEP  489/6250   lr: 0.000996   giou_loss: 1.66   conf_loss: 689.96   prob_loss: 1.74   total_loss: 693.36\n",
      "=> STEP  490/6250   lr: 0.000996   giou_loss: 1.67   conf_loss: 689.25   prob_loss: 1.74   total_loss: 692.66\n",
      "=> STEP  491/6250   lr: 0.000996   giou_loss: 1.59   conf_loss: 688.52   prob_loss: 1.75   total_loss: 691.86\n",
      "=> STEP  492/6250   lr: 0.000996   giou_loss: 1.56   conf_loss: 687.77   prob_loss: 1.75   total_loss: 691.08\n",
      "=> STEP  493/6250   lr: 0.000996   giou_loss: 1.56   conf_loss: 687.08   prob_loss: 1.74   total_loss: 690.39\n",
      "=> STEP  494/6250   lr: 0.000996   giou_loss: 1.57   conf_loss: 686.38   prob_loss: 1.74   total_loss: 689.68\n",
      "=> STEP  495/6250   lr: 0.000996   giou_loss: 1.71   conf_loss: 685.66   prob_loss: 1.74   total_loss: 689.11\n",
      "=> STEP  496/6250   lr: 0.000996   giou_loss: 1.70   conf_loss: 684.96   prob_loss: 1.74   total_loss: 688.39\n",
      "=> STEP  497/6250   lr: 0.000996   giou_loss: 1.56   conf_loss: 684.26   prob_loss: 1.72   total_loss: 687.55\n",
      "=> STEP  498/6250   lr: 0.000996   giou_loss: 1.56   conf_loss: 683.56   prob_loss: 1.73   total_loss: 686.85\n",
      "=> STEP  499/6250   lr: 0.000996   giou_loss: 1.61   conf_loss: 682.83   prob_loss: 1.73   total_loss: 686.17\n",
      "=> STEP  500/6250   lr: 0.000996   giou_loss: 1.61   conf_loss: 682.15   prob_loss: 1.73   total_loss: 685.48\n",
      "=> STEP  501/6250   lr: 0.000996   giou_loss: 1.56   conf_loss: 681.41   prob_loss: 1.73   total_loss: 684.70\n",
      "=> STEP  502/6250   lr: 0.000996   giou_loss: 1.63   conf_loss: 680.72   prob_loss: 1.73   total_loss: 684.08\n",
      "=> STEP  503/6250   lr: 0.000996   giou_loss: 1.59   conf_loss: 680.03   prob_loss: 1.72   total_loss: 683.34\n",
      "=> STEP  504/6250   lr: 0.000996   giou_loss: 1.73   conf_loss: 679.33   prob_loss: 1.72   total_loss: 682.78\n",
      "=> STEP  505/6250   lr: 0.000996   giou_loss: 1.61   conf_loss: 678.63   prob_loss: 1.72   total_loss: 681.96\n",
      "=> STEP  506/6250   lr: 0.000996   giou_loss: 1.71   conf_loss: 677.95   prob_loss: 1.73   total_loss: 681.38\n",
      "=> STEP  507/6250   lr: 0.000996   giou_loss: 1.63   conf_loss: 677.24   prob_loss: 1.74   total_loss: 680.61\n",
      "=> STEP  508/6250   lr: 0.000996   giou_loss: 1.62   conf_loss: 676.55   prob_loss: 1.72   total_loss: 679.88\n",
      "=> STEP  509/6250   lr: 0.000996   giou_loss: 1.58   conf_loss: 675.90   prob_loss: 1.71   total_loss: 679.19\n",
      "=> STEP  510/6250   lr: 0.000996   giou_loss: 1.65   conf_loss: 675.15   prob_loss: 1.72   total_loss: 678.52\n",
      "=> STEP  511/6250   lr: 0.000996   giou_loss: 1.60   conf_loss: 674.47   prob_loss: 1.72   total_loss: 677.80\n",
      "=> STEP  512/6250   lr: 0.000996   giou_loss: 1.71   conf_loss: 673.80   prob_loss: 1.71   total_loss: 677.22\n",
      "=> STEP  513/6250   lr: 0.000996   giou_loss: 1.73   conf_loss: 673.13   prob_loss: 1.71   total_loss: 676.57\n",
      "=> STEP  514/6250   lr: 0.000995   giou_loss: 1.67   conf_loss: 672.40   prob_loss: 1.72   total_loss: 675.79\n",
      "=> STEP  515/6250   lr: 0.000995   giou_loss: 1.60   conf_loss: 671.73   prob_loss: 1.72   total_loss: 675.05\n",
      "=> STEP  516/6250   lr: 0.000995   giou_loss: 1.69   conf_loss: 671.06   prob_loss: 1.72   total_loss: 674.47\n",
      "=> STEP  517/6250   lr: 0.000995   giou_loss: 1.70   conf_loss: 670.42   prob_loss: 1.72   total_loss: 673.84\n",
      "=> STEP  518/6250   lr: 0.000995   giou_loss: 1.73   conf_loss: 669.71   prob_loss: 1.72   total_loss: 673.16\n",
      "=> STEP  519/6250   lr: 0.000995   giou_loss: 1.86   conf_loss: 669.05   prob_loss: 1.70   total_loss: 672.61\n",
      "=> STEP  520/6250   lr: 0.000995   giou_loss: 1.64   conf_loss: 668.37   prob_loss: 1.69   total_loss: 671.70\n",
      "=> STEP  521/6250   lr: 0.000995   giou_loss: 1.76   conf_loss: 667.68   prob_loss: 1.70   total_loss: 671.14\n",
      "=> STEP  522/6250   lr: 0.000995   giou_loss: 1.69   conf_loss: 667.00   prob_loss: 1.71   total_loss: 670.40\n",
      "=> STEP  523/6250   lr: 0.000995   giou_loss: 1.78   conf_loss: 666.28   prob_loss: 1.71   total_loss: 669.76\n",
      "=> STEP  524/6250   lr: 0.000995   giou_loss: 1.82   conf_loss: 665.62   prob_loss: 1.71   total_loss: 669.14\n",
      "=> STEP  525/6250   lr: 0.000995   giou_loss: 1.69   conf_loss: 664.92   prob_loss: 1.71   total_loss: 668.31\n",
      "=> STEP  526/6250   lr: 0.000995   giou_loss: 1.76   conf_loss: 664.32   prob_loss: 1.69   total_loss: 667.77\n",
      "=> STEP  527/6250   lr: 0.000995   giou_loss: 1.79   conf_loss: 663.60   prob_loss: 1.68   total_loss: 667.08\n",
      "=> STEP  528/6250   lr: 0.000995   giou_loss: 1.62   conf_loss: 662.96   prob_loss: 1.70   total_loss: 666.27\n",
      "=> STEP  529/6250   lr: 0.000995   giou_loss: 1.62   conf_loss: 662.23   prob_loss: 1.72   total_loss: 665.57\n",
      "=> STEP  530/6250   lr: 0.000995   giou_loss: 1.56   conf_loss: 661.62   prob_loss: 1.71   total_loss: 664.88\n",
      "=> STEP  531/6250   lr: 0.000995   giou_loss: 1.60   conf_loss: 660.89   prob_loss: 1.69   total_loss: 664.18\n",
      "=> STEP  532/6250   lr: 0.000995   giou_loss: 1.63   conf_loss: 660.28   prob_loss: 1.69   total_loss: 663.59\n",
      "=> STEP  533/6250   lr: 0.000995   giou_loss: 1.57   conf_loss: 659.55   prob_loss: 1.68   total_loss: 662.80\n",
      "=> STEP  534/6250   lr: 0.000995   giou_loss: 1.76   conf_loss: 658.93   prob_loss: 1.68   total_loss: 662.37\n",
      "=> STEP  535/6250   lr: 0.000995   giou_loss: 1.77   conf_loss: 658.22   prob_loss: 1.68   total_loss: 661.67\n",
      "=> STEP  536/6250   lr: 0.000995   giou_loss: 1.58   conf_loss: 657.56   prob_loss: 1.69   total_loss: 660.83\n",
      "=> STEP  537/6250   lr: 0.000995   giou_loss: 1.83   conf_loss: 656.87   prob_loss: 1.72   total_loss: 660.42\n",
      "=> STEP  538/6250   lr: 0.000995   giou_loss: 1.95   conf_loss: 656.21   prob_loss: 1.71   total_loss: 659.86\n",
      "=> STEP  539/6250   lr: 0.000995   giou_loss: 1.95   conf_loss: 655.54   prob_loss: 1.69   total_loss: 659.18\n",
      "=> STEP  540/6250   lr: 0.000995   giou_loss: 1.77   conf_loss: 654.89   prob_loss: 1.67   total_loss: 658.33\n",
      "=> STEP  541/6250   lr: 0.000994   giou_loss: 1.67   conf_loss: 654.32   prob_loss: 1.65   total_loss: 657.64\n",
      "=> STEP  542/6250   lr: 0.000994   giou_loss: 1.67   conf_loss: 653.61   prob_loss: 1.66   total_loss: 656.95\n",
      "=> STEP  543/6250   lr: 0.000994   giou_loss: 1.62   conf_loss: 652.97   prob_loss: 1.68   total_loss: 656.27\n",
      "=> STEP  544/6250   lr: 0.000994   giou_loss: 1.68   conf_loss: 652.26   prob_loss: 1.69   total_loss: 655.63\n",
      "=> STEP  545/6250   lr: 0.000994   giou_loss: 1.61   conf_loss: 651.63   prob_loss: 1.68   total_loss: 654.93\n",
      "=> STEP  546/6250   lr: 0.000994   giou_loss: 1.70   conf_loss: 650.99   prob_loss: 1.66   total_loss: 654.35\n",
      "=> STEP  547/6250   lr: 0.000994   giou_loss: 1.71   conf_loss: 650.35   prob_loss: 1.66   total_loss: 653.72\n",
      "=> STEP  548/6250   lr: 0.000994   giou_loss: 1.56   conf_loss: 649.64   prob_loss: 1.67   total_loss: 652.87\n",
      "=> STEP  549/6250   lr: 0.000994   giou_loss: 1.71   conf_loss: 648.99   prob_loss: 1.68   total_loss: 652.37\n",
      "=> STEP  550/6250   lr: 0.000994   giou_loss: 1.67   conf_loss: 648.32   prob_loss: 1.66   total_loss: 651.65\n",
      "=> STEP  551/6250   lr: 0.000994   giou_loss: 1.63   conf_loss: 647.72   prob_loss: 1.65   total_loss: 650.99\n",
      "=> STEP  552/6250   lr: 0.000994   giou_loss: 1.62   conf_loss: 647.03   prob_loss: 1.65   total_loss: 650.30\n",
      "=> STEP  553/6250   lr: 0.000994   giou_loss: 1.68   conf_loss: 646.37   prob_loss: 1.66   total_loss: 649.71\n",
      "=> STEP  554/6250   lr: 0.000994   giou_loss: 1.70   conf_loss: 645.70   prob_loss: 1.66   total_loss: 649.06\n",
      "=> STEP  555/6250   lr: 0.000994   giou_loss: 1.56   conf_loss: 645.06   prob_loss: 1.65   total_loss: 648.28\n",
      "=> STEP  556/6250   lr: 0.000994   giou_loss: 1.71   conf_loss: 644.43   prob_loss: 1.64   total_loss: 647.78\n",
      "=> STEP  557/6250   lr: 0.000994   giou_loss: 1.66   conf_loss: 643.78   prob_loss: 1.64   total_loss: 647.08\n",
      "=> STEP  558/6250   lr: 0.000994   giou_loss: 1.80   conf_loss: 643.12   prob_loss: 1.66   total_loss: 646.57\n",
      "=> STEP  559/6250   lr: 0.000994   giou_loss: 1.81   conf_loss: 642.47   prob_loss: 1.67   total_loss: 645.94\n",
      "=> STEP  560/6250   lr: 0.000994   giou_loss: 1.70   conf_loss: 641.85   prob_loss: 1.66   total_loss: 645.20\n",
      "=> STEP  561/6250   lr: 0.000994   giou_loss: 1.66   conf_loss: 641.24   prob_loss: 1.64   total_loss: 644.54\n",
      "=> STEP  562/6250   lr: 0.000994   giou_loss: 1.69   conf_loss: 640.61   prob_loss: 1.63   total_loss: 643.93\n",
      "=> STEP  563/6250   lr: 0.000994   giou_loss: 1.56   conf_loss: 639.93   prob_loss: 1.65   total_loss: 643.15\n",
      "=> STEP  564/6250   lr: 0.000994   giou_loss: 1.71   conf_loss: 639.28   prob_loss: 1.65   total_loss: 642.64\n",
      "=> STEP  565/6250   lr: 0.000994   giou_loss: 1.65   conf_loss: 638.64   prob_loss: 1.64   total_loss: 641.93\n",
      "=> STEP  566/6250   lr: 0.000994   giou_loss: 1.67   conf_loss: 638.04   prob_loss: 1.63   total_loss: 641.33\n",
      "=> STEP  567/6250   lr: 0.000993   giou_loss: 1.66   conf_loss: 637.39   prob_loss: 1.62   total_loss: 640.68\n",
      "=> STEP  568/6250   lr: 0.000993   giou_loss: 1.70   conf_loss: 636.74   prob_loss: 1.64   total_loss: 640.08\n",
      "=> STEP  569/6250   lr: 0.000993   giou_loss: 1.70   conf_loss: 636.10   prob_loss: 1.65   total_loss: 639.44\n",
      "=> STEP  570/6250   lr: 0.000993   giou_loss: 1.73   conf_loss: 635.49   prob_loss: 1.64   total_loss: 638.86\n",
      "=> STEP  571/6250   lr: 0.000993   giou_loss: 1.56   conf_loss: 634.84   prob_loss: 1.63   total_loss: 638.03\n",
      "=> STEP  572/6250   lr: 0.000993   giou_loss: 1.71   conf_loss: 634.22   prob_loss: 1.62   total_loss: 637.55\n",
      "=> STEP  573/6250   lr: 0.000993   giou_loss: 1.68   conf_loss: 633.57   prob_loss: 1.62   total_loss: 636.88\n",
      "=> STEP  574/6250   lr: 0.000993   giou_loss: 1.73   conf_loss: 632.97   prob_loss: 1.63   total_loss: 636.33\n",
      "=> STEP  575/6250   lr: 0.000993   giou_loss: 1.68   conf_loss: 632.30   prob_loss: 1.63   total_loss: 635.61\n",
      "=> STEP  576/6250   lr: 0.000993   giou_loss: 1.78   conf_loss: 631.67   prob_loss: 1.63   total_loss: 635.08\n",
      "=> STEP  577/6250   lr: 0.000993   giou_loss: 1.81   conf_loss: 631.04   prob_loss: 1.63   total_loss: 634.48\n",
      "=> STEP  578/6250   lr: 0.000993   giou_loss: 1.67   conf_loss: 630.41   prob_loss: 1.62   total_loss: 633.71\n",
      "=> STEP  579/6250   lr: 0.000993   giou_loss: 1.76   conf_loss: 629.82   prob_loss: 1.62   total_loss: 633.20\n",
      "=> STEP  580/6250   lr: 0.000993   giou_loss: 1.79   conf_loss: 629.18   prob_loss: 1.61   total_loss: 632.59\n",
      "=> STEP  581/6250   lr: 0.000993   giou_loss: 1.67   conf_loss: 628.56   prob_loss: 1.62   total_loss: 631.85\n",
      "=> STEP  582/6250   lr: 0.000993   giou_loss: 1.75   conf_loss: 627.91   prob_loss: 1.64   total_loss: 631.30\n",
      "=> STEP  583/6250   lr: 0.000993   giou_loss: 1.88   conf_loss: 627.29   prob_loss: 1.65   total_loss: 630.82\n",
      "=> STEP  584/6250   lr: 0.000993   giou_loss: 1.80   conf_loss: 626.66   prob_loss: 1.63   total_loss: 630.09\n",
      "=> STEP  585/6250   lr: 0.000993   giou_loss: 1.57   conf_loss: 626.07   prob_loss: 1.60   total_loss: 629.24\n",
      "=> STEP  586/6250   lr: 0.000993   giou_loss: 1.82   conf_loss: 625.48   prob_loss: 1.59   total_loss: 628.89\n",
      "=> STEP  587/6250   lr: 0.000993   giou_loss: 1.77   conf_loss: 624.81   prob_loss: 1.61   total_loss: 628.19\n",
      "=> STEP  588/6250   lr: 0.000993   giou_loss: 1.60   conf_loss: 624.19   prob_loss: 1.63   total_loss: 627.42\n",
      "=> STEP  589/6250   lr: 0.000993   giou_loss: 1.68   conf_loss: 623.55   prob_loss: 1.64   total_loss: 626.87\n",
      "=> STEP  590/6250   lr: 0.000992   giou_loss: 1.64   conf_loss: 622.93   prob_loss: 1.62   total_loss: 626.19\n",
      "=> STEP  591/6250   lr: 0.000992   giou_loss: 1.65   conf_loss: 622.32   prob_loss: 1.61   total_loss: 625.58\n",
      "=> STEP  592/6250   lr: 0.000992   giou_loss: 1.68   conf_loss: 621.72   prob_loss: 1.60   total_loss: 625.00\n",
      "=> STEP  593/6250   lr: 0.000992   giou_loss: 1.69   conf_loss: 621.09   prob_loss: 1.61   total_loss: 624.38\n",
      "=> STEP  594/6250   lr: 0.000992   giou_loss: 1.65   conf_loss: 620.46   prob_loss: 1.62   total_loss: 623.73\n",
      "=> STEP  595/6250   lr: 0.000992   giou_loss: 1.67   conf_loss: 619.83   prob_loss: 1.62   total_loss: 623.12\n",
      "=> STEP  596/6250   lr: 0.000992   giou_loss: 1.56   conf_loss: 619.22   prob_loss: 1.60   total_loss: 622.39\n",
      "=> STEP  597/6250   lr: 0.000992   giou_loss: 1.70   conf_loss: 618.63   prob_loss: 1.59   total_loss: 621.93\n",
      "=> STEP  598/6250   lr: 0.000992   giou_loss: 1.63   conf_loss: 617.99   prob_loss: 1.60   total_loss: 621.22\n",
      "=> STEP  599/6250   lr: 0.000992   giou_loss: 1.70   conf_loss: 617.37   prob_loss: 1.61   total_loss: 620.69\n",
      "=> STEP  600/6250   lr: 0.000992   giou_loss: 1.76   conf_loss: 616.75   prob_loss: 1.61   total_loss: 620.13\n",
      "=> STEP  601/6250   lr: 0.000992   giou_loss: 1.63   conf_loss: 616.15   prob_loss: 1.60   total_loss: 619.38\n",
      "=> STEP  602/6250   lr: 0.000992   giou_loss: 1.75   conf_loss: 615.59   prob_loss: 1.59   total_loss: 618.93\n",
      "=> STEP  603/6250   lr: 0.000992   giou_loss: 1.79   conf_loss: 614.98   prob_loss: 1.59   total_loss: 618.36\n",
      "=> STEP  604/6250   lr: 0.000992   giou_loss: 1.68   conf_loss: 614.36   prob_loss: 1.60   total_loss: 617.64\n",
      "=> STEP  605/6250   lr: 0.000992   giou_loss: 1.67   conf_loss: 613.74   prob_loss: 1.62   total_loss: 617.03\n",
      "=> STEP  606/6250   lr: 0.000992   giou_loss: 1.83   conf_loss: 613.14   prob_loss: 1.62   total_loss: 616.58\n",
      "=> STEP  607/6250   lr: 0.000992   giou_loss: 1.73   conf_loss: 612.54   prob_loss: 1.60   total_loss: 615.87\n",
      "=> STEP  608/6250   lr: 0.000992   giou_loss: 1.66   conf_loss: 611.97   prob_loss: 1.58   total_loss: 615.21\n",
      "=> STEP  609/6250   lr: 0.000992   giou_loss: 1.77   conf_loss: 611.36   prob_loss: 1.57   total_loss: 614.70\n",
      "=> STEP  610/6250   lr: 0.000992   giou_loss: 1.72   conf_loss: 610.74   prob_loss: 1.59   total_loss: 614.05\n",
      "=> STEP  611/6250   lr: 0.000992   giou_loss: 1.56   conf_loss: 610.14   prob_loss: 1.60   total_loss: 613.30\n",
      "=> STEP  612/6250   lr: 0.000991   giou_loss: 1.74   conf_loss: 609.56   prob_loss: 1.60   total_loss: 612.90\n",
      "=> STEP  613/6250   lr: 0.000991   giou_loss: 1.73   conf_loss: 608.94   prob_loss: 1.60   total_loss: 612.27\n",
      "=> STEP  614/6250   lr: 0.000991   giou_loss: 1.65   conf_loss: 608.34   prob_loss: 1.59   total_loss: 611.58\n",
      "=> STEP  615/6250   lr: 0.000991   giou_loss: 1.63   conf_loss: 607.76   prob_loss: 1.58   total_loss: 610.96\n",
      "=> STEP  616/6250   lr: 0.000991   giou_loss: 1.61   conf_loss: 607.15   prob_loss: 1.57   total_loss: 610.33\n",
      "=> STEP  617/6250   lr: 0.000991   giou_loss: 1.68   conf_loss: 606.53   prob_loss: 1.58   total_loss: 609.79\n",
      "=> STEP  618/6250   lr: 0.000991   giou_loss: 1.68   conf_loss: 605.93   prob_loss: 1.58   total_loss: 609.20\n",
      "=> STEP  619/6250   lr: 0.000991   giou_loss: 1.56   conf_loss: 605.34   prob_loss: 1.58   total_loss: 608.48\n",
      "=> STEP  620/6250   lr: 0.000991   giou_loss: 1.69   conf_loss: 604.75   prob_loss: 1.57   total_loss: 608.01\n",
      "=> STEP  621/6250   lr: 0.000991   giou_loss: 1.62   conf_loss: 604.14   prob_loss: 1.57   total_loss: 607.34\n",
      "=> STEP  622/6250   lr: 0.000991   giou_loss: 1.70   conf_loss: 603.54   prob_loss: 1.58   total_loss: 606.82\n",
      "=> STEP  623/6250   lr: 0.000991   giou_loss: 1.74   conf_loss: 602.94   prob_loss: 1.58   total_loss: 606.26\n",
      "=> STEP  624/6250   lr: 0.000991   giou_loss: 1.61   conf_loss: 602.37   prob_loss: 1.57   total_loss: 605.55\n",
      "=> STEP  625/6250   lr: 0.000991   giou_loss: 1.82   conf_loss: 601.84   prob_loss: 1.56   total_loss: 605.22\n",
      "=> STEP  626/6250   lr: 0.000991   giou_loss: 1.81   conf_loss: 601.26   prob_loss: 1.56   total_loss: 604.62\n",
      "=> STEP  627/6250   lr: 0.000991   giou_loss: 1.79   conf_loss: 600.64   prob_loss: 1.56   total_loss: 603.99\n",
      "=> STEP  628/6250   lr: 0.000991   giou_loss: 1.76   conf_loss: 600.05   prob_loss: 1.58   total_loss: 603.39\n",
      "=> STEP  629/6250   lr: 0.000991   giou_loss: 1.70   conf_loss: 599.44   prob_loss: 1.58   total_loss: 602.72\n",
      "=> STEP  630/6250   lr: 0.000991   giou_loss: 1.58   conf_loss: 598.90   prob_loss: 1.57   total_loss: 602.05\n",
      "=> STEP  631/6250   lr: 0.000991   giou_loss: 1.61   conf_loss: 598.29   prob_loss: 1.56   total_loss: 601.45\n",
      "=> STEP  632/6250   lr: 0.000991   giou_loss: 1.63   conf_loss: 597.74   prob_loss: 1.55   total_loss: 600.93\n",
      "=> STEP  633/6250   lr: 0.000990   giou_loss: 1.57   conf_loss: 597.13   prob_loss: 1.56   total_loss: 600.26\n",
      "=> STEP  634/6250   lr: 0.000990   giou_loss: 1.68   conf_loss: 596.54   prob_loss: 1.57   total_loss: 599.79\n",
      "=> STEP  635/6250   lr: 0.000990   giou_loss: 1.60   conf_loss: 595.97   prob_loss: 1.56   total_loss: 599.13\n",
      "=> STEP  636/6250   lr: 0.000990   giou_loss: 1.73   conf_loss: 595.44   prob_loss: 1.54   total_loss: 598.71\n",
      "=> STEP  637/6250   lr: 0.000990   giou_loss: 1.75   conf_loss: 594.89   prob_loss: 1.54   total_loss: 598.19\n",
      "=> STEP  638/6250   lr: 0.000990   giou_loss: 1.58   conf_loss: 594.28   prob_loss: 1.55   total_loss: 597.41\n",
      "=> STEP  639/6250   lr: 0.000990   giou_loss: 1.68   conf_loss: 593.71   prob_loss: 1.56   total_loss: 596.94\n",
      "=> STEP  640/6250   lr: 0.000990   giou_loss: 1.65   conf_loss: 593.10   prob_loss: 1.55   total_loss: 596.31\n",
      "=> STEP  641/6250   lr: 0.000990   giou_loss: 1.56   conf_loss: 592.56   prob_loss: 1.54   total_loss: 595.67\n",
      "=> STEP  642/6250   lr: 0.000990   giou_loss: 1.68   conf_loss: 591.98   prob_loss: 1.53   total_loss: 595.20\n",
      "=> STEP  643/6250   lr: 0.000990   giou_loss: 1.65   conf_loss: 591.41   prob_loss: 1.54   total_loss: 594.59\n",
      "=> STEP  644/6250   lr: 0.000990   giou_loss: 1.62   conf_loss: 590.80   prob_loss: 1.55   total_loss: 593.97\n",
      "=> STEP  645/6250   lr: 0.000990   giou_loss: 1.60   conf_loss: 590.25   prob_loss: 1.55   total_loss: 593.40\n",
      "=> STEP  646/6250   lr: 0.000990   giou_loss: 1.59   conf_loss: 589.65   prob_loss: 1.54   total_loss: 592.78\n",
      "=> STEP  647/6250   lr: 0.000990   giou_loss: 1.57   conf_loss: 589.12   prob_loss: 1.54   total_loss: 592.22\n",
      "=> STEP  648/6250   lr: 0.000990   giou_loss: 1.56   conf_loss: 588.50   prob_loss: 1.53   total_loss: 591.60\n",
      "=> STEP  649/6250   lr: 0.000990   giou_loss: 1.56   conf_loss: 587.95   prob_loss: 1.54   total_loss: 591.05\n",
      "=> STEP  650/6250   lr: 0.000990   giou_loss: 1.70   conf_loss: 587.34   prob_loss: 1.54   total_loss: 590.58\n",
      "=> STEP  651/6250   lr: 0.000990   giou_loss: 1.69   conf_loss: 586.78   prob_loss: 1.53   total_loss: 590.00\n",
      "=> STEP  652/6250   lr: 0.000990   giou_loss: 1.58   conf_loss: 586.21   prob_loss: 1.53   total_loss: 589.32\n",
      "=> STEP  653/6250   lr: 0.000989   giou_loss: 1.66   conf_loss: 585.65   prob_loss: 1.53   total_loss: 588.84\n",
      "=> STEP  654/6250   lr: 0.000989   giou_loss: 1.64   conf_loss: 585.08   prob_loss: 1.53   total_loss: 588.24\n",
      "=> STEP  655/6250   lr: 0.000989   giou_loss: 1.66   conf_loss: 584.55   prob_loss: 1.54   total_loss: 587.74\n",
      "=> STEP  656/6250   lr: 0.000989   giou_loss: 1.75   conf_loss: 583.96   prob_loss: 1.55   total_loss: 587.25\n",
      "=> STEP  657/6250   lr: 0.000989   giou_loss: 1.80   conf_loss: 583.40   prob_loss: 1.54   total_loss: 586.74\n",
      "=> STEP  658/6250   lr: 0.000989   giou_loss: 1.70   conf_loss: 582.84   prob_loss: 1.53   total_loss: 586.07\n",
      "=> STEP  659/6250   lr: 0.000989   giou_loss: 1.64   conf_loss: 582.29   prob_loss: 1.51   total_loss: 585.44\n",
      "=> STEP  660/6250   lr: 0.000989   giou_loss: 1.66   conf_loss: 581.73   prob_loss: 1.52   total_loss: 584.90\n",
      "=> STEP  661/6250   lr: 0.000989   giou_loss: 1.57   conf_loss: 581.14   prob_loss: 1.53   total_loss: 584.24\n",
      "=> STEP  662/6250   lr: 0.000989   giou_loss: 1.56   conf_loss: 580.57   prob_loss: 1.53   total_loss: 583.66\n",
      "=> STEP  663/6250   lr: 0.000989   giou_loss: 1.59   conf_loss: 580.00   prob_loss: 1.53   total_loss: 583.12\n",
      "=> STEP  664/6250   lr: 0.000989   giou_loss: 1.64   conf_loss: 579.42   prob_loss: 1.53   total_loss: 582.59\n",
      "=> STEP  665/6250   lr: 0.000989   giou_loss: 1.59   conf_loss: 578.86   prob_loss: 1.53   total_loss: 581.99\n",
      "=> STEP  666/6250   lr: 0.000989   giou_loss: 1.74   conf_loss: 578.32   prob_loss: 1.52   total_loss: 581.59\n",
      "=> STEP  667/6250   lr: 0.000989   giou_loss: 1.67   conf_loss: 577.75   prob_loss: 1.52   total_loss: 580.95\n",
      "=> STEP  668/6250   lr: 0.000989   giou_loss: 1.80   conf_loss: 577.20   prob_loss: 1.53   total_loss: 580.52\n",
      "=> STEP  669/6250   lr: 0.000989   giou_loss: 1.73   conf_loss: 576.62   prob_loss: 1.53   total_loss: 579.89\n",
      "=> STEP  670/6250   lr: 0.000989   giou_loss: 1.70   conf_loss: 576.09   prob_loss: 1.53   total_loss: 579.32\n",
      "=> STEP  671/6250   lr: 0.000989   giou_loss: 1.66   conf_loss: 575.52   prob_loss: 1.52   total_loss: 578.70\n",
      "=> STEP  672/6250   lr: 0.000988   giou_loss: 1.79   conf_loss: 574.95   prob_loss: 1.52   total_loss: 578.27\n",
      "=> STEP  673/6250   lr: 0.000988   giou_loss: 1.83   conf_loss: 574.41   prob_loss: 1.52   total_loss: 577.76\n",
      "=> STEP  674/6250   lr: 0.000988   giou_loss: 1.58   conf_loss: 573.85   prob_loss: 1.52   total_loss: 576.95\n",
      "=> STEP  675/6250   lr: 0.000988   giou_loss: 1.97   conf_loss: 573.35   prob_loss: 1.51   total_loss: 576.83\n",
      "=> STEP  676/6250   lr: 0.000988   giou_loss: 2.05   conf_loss: 572.79   prob_loss: 1.51   total_loss: 576.35\n",
      "=> STEP  677/6250   lr: 0.000988   giou_loss: 1.85   conf_loss: 572.22   prob_loss: 1.51   total_loss: 575.58\n",
      "=> STEP  678/6250   lr: 0.000988   giou_loss: 1.76   conf_loss: 571.67   prob_loss: 1.52   total_loss: 574.95\n",
      "=> STEP  679/6250   lr: 0.000988   giou_loss: 1.98   conf_loss: 571.10   prob_loss: 1.52   total_loss: 574.61\n",
      "=> STEP  680/6250   lr: 0.000988   giou_loss: 1.94   conf_loss: 570.55   prob_loss: 1.52   total_loss: 574.01\n",
      "=> STEP  681/6250   lr: 0.000988   giou_loss: 1.59   conf_loss: 570.00   prob_loss: 1.52   total_loss: 573.11\n",
      "=> STEP  682/6250   lr: 0.000988   giou_loss: 2.06   conf_loss: 569.49   prob_loss: 1.52   total_loss: 573.07\n",
      "=> STEP  683/6250   lr: 0.000988   giou_loss: 2.27   conf_loss: 568.95   prob_loss: 1.52   total_loss: 572.74\n",
      "=> STEP  684/6250   lr: 0.000988   giou_loss: 2.29   conf_loss: 568.39   prob_loss: 1.53   total_loss: 572.21\n",
      "=> STEP  685/6250   lr: 0.000988   giou_loss: 2.19   conf_loss: 567.82   prob_loss: 1.53   total_loss: 571.54\n",
      "=> STEP  686/6250   lr: 0.000988   giou_loss: 1.88   conf_loss: 567.27   prob_loss: 1.53   total_loss: 570.67\n",
      "=> STEP  687/6250   lr: 0.000988   giou_loss: 1.82   conf_loss: 566.73   prob_loss: 1.53   total_loss: 570.08\n",
      "=> STEP  688/6250   lr: 0.000988   giou_loss: 2.14   conf_loss: 566.20   prob_loss: 1.51   total_loss: 569.85\n",
      "=> STEP  689/6250   lr: 0.000988   giou_loss: 2.20   conf_loss: 565.66   prob_loss: 1.50   total_loss: 569.37\n",
      "=> STEP  690/6250   lr: 0.000987   giou_loss: 2.11   conf_loss: 565.12   prob_loss: 1.49   total_loss: 568.73\n",
      "=> STEP  691/6250   lr: 0.000987   giou_loss: 1.81   conf_loss: 564.58   prob_loss: 1.50   total_loss: 567.88\n",
      "=> STEP  692/6250   lr: 0.000987   giou_loss: 1.81   conf_loss: 564.07   prob_loss: 1.50   total_loss: 567.38\n",
      "=> STEP  693/6250   lr: 0.000987   giou_loss: 1.97   conf_loss: 563.54   prob_loss: 1.50   total_loss: 567.01\n",
      "=> STEP  694/6250   lr: 0.000987   giou_loss: 1.83   conf_loss: 562.99   prob_loss: 1.50   total_loss: 566.33\n",
      "=> STEP  695/6250   lr: 0.000987   giou_loss: 1.70   conf_loss: 562.45   prob_loss: 1.51   total_loss: 565.65\n",
      "=> STEP  696/6250   lr: 0.000987   giou_loss: 1.86   conf_loss: 561.88   prob_loss: 1.51   total_loss: 565.25\n",
      "=> STEP  697/6250   lr: 0.000987   giou_loss: 1.70   conf_loss: 561.35   prob_loss: 1.51   total_loss: 564.55\n",
      "=> STEP  698/6250   lr: 0.000987   giou_loss: 1.78   conf_loss: 560.86   prob_loss: 1.50   total_loss: 564.14\n",
      "=> STEP  699/6250   lr: 0.000987   giou_loss: 1.82   conf_loss: 560.30   prob_loss: 1.50   total_loss: 563.62\n",
      "=> STEP  700/6250   lr: 0.000987   giou_loss: 1.77   conf_loss: 559.73   prob_loss: 1.50   total_loss: 563.00\n",
      "=> STEP  701/6250   lr: 0.000987   giou_loss: 1.79   conf_loss: 559.23   prob_loss: 1.49   total_loss: 562.50\n",
      "=> STEP  702/6250   lr: 0.000987   giou_loss: 1.69   conf_loss: 558.71   prob_loss: 1.48   total_loss: 561.88\n",
      "=> STEP  703/6250   lr: 0.000987   giou_loss: 1.66   conf_loss: 558.17   prob_loss: 1.48   total_loss: 561.32\n",
      "=> STEP  704/6250   lr: 0.000987   giou_loss: 1.62   conf_loss: 557.63   prob_loss: 1.49   total_loss: 560.74\n",
      "=> STEP  705/6250   lr: 0.000987   giou_loss: 1.74   conf_loss: 557.08   prob_loss: 1.50   total_loss: 560.32\n",
      "=> STEP  706/6250   lr: 0.000987   giou_loss: 1.85   conf_loss: 556.54   prob_loss: 1.49   total_loss: 559.88\n",
      "=> STEP  707/6250   lr: 0.000986   giou_loss: 1.69   conf_loss: 556.03   prob_loss: 1.48   total_loss: 559.19\n",
      "=> STEP  708/6250   lr: 0.000986   giou_loss: 1.67   conf_loss: 555.52   prob_loss: 1.47   total_loss: 558.66\n",
      "=> STEP  709/6250   lr: 0.000986   giou_loss: 1.69   conf_loss: 555.00   prob_loss: 1.47   total_loss: 558.15\n",
      "=> STEP  710/6250   lr: 0.000986   giou_loss: 1.66   conf_loss: 554.44   prob_loss: 1.47   total_loss: 557.57\n",
      "=> STEP  711/6250   lr: 0.000986   giou_loss: 1.70   conf_loss: 553.92   prob_loss: 1.48   total_loss: 557.11\n",
      "=> STEP  712/6250   lr: 0.000986   giou_loss: 1.63   conf_loss: 553.37   prob_loss: 1.48   total_loss: 556.48\n",
      "=> STEP  713/6250   lr: 0.000986   giou_loss: 1.73   conf_loss: 552.89   prob_loss: 1.47   total_loss: 556.08\n",
      "=> STEP  714/6250   lr: 0.000986   giou_loss: 1.67   conf_loss: 552.34   prob_loss: 1.46   total_loss: 555.48\n",
      "=> STEP  715/6250   lr: 0.000986   giou_loss: 1.62   conf_loss: 551.82   prob_loss: 1.47   total_loss: 554.90\n",
      "=> STEP  716/6250   lr: 0.000986   giou_loss: 1.56   conf_loss: 551.27   prob_loss: 1.47   total_loss: 554.31\n",
      "=> STEP  717/6250   lr: 0.000986   giou_loss: 1.61   conf_loss: 550.75   prob_loss: 1.47   total_loss: 553.83\n",
      "=> STEP  718/6250   lr: 0.000986   giou_loss: 1.56   conf_loss: 550.24   prob_loss: 1.47   total_loss: 553.27\n",
      "=> STEP  719/6250   lr: 0.000986   giou_loss: 1.59   conf_loss: 549.72   prob_loss: 1.46   total_loss: 552.76\n",
      "=> STEP  720/6250   lr: 0.000986   giou_loss: 1.56   conf_loss: 549.21   prob_loss: 1.46   total_loss: 552.23\n",
      "=> STEP  721/6250   lr: 0.000986   giou_loss: 1.64   conf_loss: 548.67   prob_loss: 1.46   total_loss: 551.77\n",
      "=> STEP  722/6250   lr: 0.000986   giou_loss: 1.69   conf_loss: 548.16   prob_loss: 1.47   total_loss: 551.32\n",
      "=> STEP  723/6250   lr: 0.000986   giou_loss: 1.70   conf_loss: 547.63   prob_loss: 1.46   total_loss: 550.79\n",
      "=> STEP  724/6250   lr: 0.000985   giou_loss: 1.57   conf_loss: 547.18   prob_loss: 1.45   total_loss: 550.20\n",
      "=> STEP  725/6250   lr: 0.000985   giou_loss: 1.63   conf_loss: 546.66   prob_loss: 1.46   total_loss: 549.75\n",
      "=> STEP  726/6250   lr: 0.000985   giou_loss: 1.58   conf_loss: 546.13   prob_loss: 1.45   total_loss: 549.17\n",
      "=> STEP  727/6250   lr: 0.000985   giou_loss: 1.63   conf_loss: 545.64   prob_loss: 1.45   total_loss: 548.72\n",
      "=> STEP  728/6250   lr: 0.000985   giou_loss: 1.56   conf_loss: 545.11   prob_loss: 1.45   total_loss: 548.13\n",
      "=> STEP  729/6250   lr: 0.000985   giou_loss: 1.63   conf_loss: 544.61   prob_loss: 1.46   total_loss: 547.70\n",
      "=> STEP  730/6250   lr: 0.000985   giou_loss: 1.62   conf_loss: 544.08   prob_loss: 1.46   total_loss: 547.15\n",
      "=> STEP  731/6250   lr: 0.000985   giou_loss: 1.58   conf_loss: 543.57   prob_loss: 1.44   total_loss: 546.59\n",
      "=> STEP  732/6250   lr: 0.000985   giou_loss: 1.66   conf_loss: 543.06   prob_loss: 1.44   total_loss: 546.15\n",
      "=> STEP  733/6250   lr: 0.000985   giou_loss: 1.60   conf_loss: 542.54   prob_loss: 1.44   total_loss: 545.58\n",
      "=> STEP  734/6250   lr: 0.000985   giou_loss: 1.58   conf_loss: 542.01   prob_loss: 1.46   total_loss: 545.05\n",
      "=> STEP  735/6250   lr: 0.000985   giou_loss: 1.68   conf_loss: 541.51   prob_loss: 1.46   total_loss: 544.65\n",
      "=> STEP  736/6250   lr: 0.000985   giou_loss: 1.63   conf_loss: 540.99   prob_loss: 1.44   total_loss: 544.06\n",
      "=> STEP  737/6250   lr: 0.000985   giou_loss: 1.76   conf_loss: 540.54   prob_loss: 1.43   total_loss: 543.73\n",
      "=> STEP  738/6250   lr: 0.000985   giou_loss: 1.88   conf_loss: 539.98   prob_loss: 1.44   total_loss: 543.31\n",
      "=> STEP  739/6250   lr: 0.000985   giou_loss: 1.67   conf_loss: 539.49   prob_loss: 1.45   total_loss: 542.61\n",
      "=> STEP  740/6250   lr: 0.000984   giou_loss: 1.85   conf_loss: 538.95   prob_loss: 1.46   total_loss: 542.26\n",
      "=> STEP  741/6250   lr: 0.000984   giou_loss: 1.97   conf_loss: 538.46   prob_loss: 1.45   total_loss: 541.89\n",
      "=> STEP  742/6250   lr: 0.000984   giou_loss: 1.85   conf_loss: 537.94   prob_loss: 1.44   total_loss: 541.24\n",
      "=> STEP  743/6250   lr: 0.000984   giou_loss: 1.58   conf_loss: 537.46   prob_loss: 1.44   total_loss: 540.47\n",
      "=> STEP  744/6250   lr: 0.000984   giou_loss: 1.73   conf_loss: 536.97   prob_loss: 1.43   total_loss: 540.13\n",
      "=> STEP  745/6250   lr: 0.000984   giou_loss: 1.66   conf_loss: 536.44   prob_loss: 1.42   total_loss: 539.52\n",
      "=> STEP  746/6250   lr: 0.000984   giou_loss: 1.67   conf_loss: 535.98   prob_loss: 1.43   total_loss: 539.08\n",
      "=> STEP  747/6250   lr: 0.000984   giou_loss: 1.68   conf_loss: 535.42   prob_loss: 1.44   total_loss: 538.54\n",
      "=> STEP  748/6250   lr: 0.000984   giou_loss: 1.56   conf_loss: 534.94   prob_loss: 1.43   total_loss: 537.93\n",
      "=> STEP  749/6250   lr: 0.000984   giou_loss: 1.70   conf_loss: 534.45   prob_loss: 1.42   total_loss: 537.57\n",
      "=> STEP  750/6250   lr: 0.000984   giou_loss: 1.63   conf_loss: 533.93   prob_loss: 1.42   total_loss: 536.98\n",
      "=> STEP  751/6250   lr: 0.000984   giou_loss: 1.78   conf_loss: 533.42   prob_loss: 1.43   total_loss: 536.63\n",
      "=> STEP  752/6250   lr: 0.000984   giou_loss: 1.83   conf_loss: 532.92   prob_loss: 1.43   total_loss: 536.18\n",
      "=> STEP  753/6250   lr: 0.000984   giou_loss: 1.68   conf_loss: 532.42   prob_loss: 1.43   total_loss: 535.53\n",
      "=> STEP  754/6250   lr: 0.000984   giou_loss: 1.66   conf_loss: 531.97   prob_loss: 1.42   total_loss: 535.05\n",
      "=> STEP  755/6250   lr: 0.000983   giou_loss: 1.69   conf_loss: 531.46   prob_loss: 1.42   total_loss: 534.57\n",
      "=> STEP  756/6250   lr: 0.000983   giou_loss: 1.56   conf_loss: 530.94   prob_loss: 1.42   total_loss: 533.93\n",
      "=> STEP  757/6250   lr: 0.000983   giou_loss: 1.67   conf_loss: 530.44   prob_loss: 1.43   total_loss: 533.54\n",
      "=> STEP  758/6250   lr: 0.000983   giou_loss: 1.61   conf_loss: 529.93   prob_loss: 1.43   total_loss: 532.97\n",
      "=> STEP  759/6250   lr: 0.000983   giou_loss: 1.69   conf_loss: 529.45   prob_loss: 1.42   total_loss: 532.56\n",
      "=> STEP  760/6250   lr: 0.000983   giou_loss: 1.71   conf_loss: 528.97   prob_loss: 1.41   total_loss: 532.09\n",
      "=> STEP  761/6250   lr: 0.000983   giou_loss: 1.56   conf_loss: 528.46   prob_loss: 1.42   total_loss: 531.44\n",
      "=> STEP  762/6250   lr: 0.000983   giou_loss: 1.69   conf_loss: 527.94   prob_loss: 1.43   total_loss: 531.06\n",
      "=> STEP  763/6250   lr: 0.000983   giou_loss: 1.64   conf_loss: 527.46   prob_loss: 1.42   total_loss: 530.52\n",
      "=> STEP  764/6250   lr: 0.000983   giou_loss: 1.65   conf_loss: 526.99   prob_loss: 1.41   total_loss: 530.05\n",
      "=> STEP  765/6250   lr: 0.000983   giou_loss: 1.64   conf_loss: 526.50   prob_loss: 1.41   total_loss: 529.55\n",
      "=> STEP  766/6250   lr: 0.000983   giou_loss: 1.62   conf_loss: 525.99   prob_loss: 1.42   total_loss: 529.02\n",
      "=> STEP  767/6250   lr: 0.000983   giou_loss: 1.60   conf_loss: 525.51   prob_loss: 1.42   total_loss: 528.52\n",
      "=> STEP  768/6250   lr: 0.000983   giou_loss: 1.67   conf_loss: 525.03   prob_loss: 1.41   total_loss: 528.11\n",
      "=> STEP  769/6250   lr: 0.000983   giou_loss: 1.68   conf_loss: 524.56   prob_loss: 1.40   total_loss: 527.64\n",
      "=> STEP  770/6250   lr: 0.000983   giou_loss: 1.65   conf_loss: 524.04   prob_loss: 1.41   total_loss: 527.10\n",
      "=> STEP  771/6250   lr: 0.000982   giou_loss: 1.69   conf_loss: 523.56   prob_loss: 1.42   total_loss: 526.68\n",
      "=> STEP  772/6250   lr: 0.000982   giou_loss: 1.73   conf_loss: 523.11   prob_loss: 1.41   total_loss: 526.25\n",
      "=> STEP  773/6250   lr: 0.000982   giou_loss: 1.60   conf_loss: 522.65   prob_loss: 1.41   total_loss: 525.65\n",
      "=> STEP  774/6250   lr: 0.000982   giou_loss: 1.81   conf_loss: 522.14   prob_loss: 1.40   total_loss: 525.36\n",
      "=> STEP  775/6250   lr: 0.000982   giou_loss: 1.80   conf_loss: 521.66   prob_loss: 1.41   total_loss: 524.87\n",
      "=> STEP  776/6250   lr: 0.000982   giou_loss: 1.58   conf_loss: 521.15   prob_loss: 1.41   total_loss: 524.14\n",
      "=> STEP  777/6250   lr: 0.000982   giou_loss: 1.78   conf_loss: 520.71   prob_loss: 1.40   total_loss: 523.88\n",
      "=> STEP  778/6250   lr: 0.000982   giou_loss: 1.85   conf_loss: 520.21   prob_loss: 1.39   total_loss: 523.46\n",
      "=> STEP  779/6250   lr: 0.000982   giou_loss: 1.80   conf_loss: 519.72   prob_loss: 1.39   total_loss: 522.91\n",
      "=> STEP  780/6250   lr: 0.000982   giou_loss: 1.62   conf_loss: 519.22   prob_loss: 1.41   total_loss: 522.25\n",
      "=> STEP  781/6250   lr: 0.000982   giou_loss: 1.61   conf_loss: 518.70   prob_loss: 1.43   total_loss: 521.74\n",
      "=> STEP  782/6250   lr: 0.000982   giou_loss: 1.65   conf_loss: 518.25   prob_loss: 1.41   total_loss: 521.31\n",
      "=> STEP  783/6250   lr: 0.000982   giou_loss: 1.62   conf_loss: 517.77   prob_loss: 1.40   total_loss: 520.78\n",
      "=> STEP  784/6250   lr: 0.000982   giou_loss: 1.60   conf_loss: 517.29   prob_loss: 1.40   total_loss: 520.29\n",
      "=> STEP  785/6250   lr: 0.000981   giou_loss: 1.73   conf_loss: 516.84   prob_loss: 1.40   total_loss: 519.97\n",
      "=> STEP  786/6250   lr: 0.000981   giou_loss: 1.64   conf_loss: 516.36   prob_loss: 1.41   total_loss: 519.41\n",
      "=> STEP  787/6250   lr: 0.000981   giou_loss: 1.77   conf_loss: 515.92   prob_loss: 1.41   total_loss: 519.10\n",
      "=> STEP  788/6250   lr: 0.000981   giou_loss: 1.79   conf_loss: 515.40   prob_loss: 1.41   total_loss: 518.60\n",
      "=> STEP  789/6250   lr: 0.000981   giou_loss: 1.67   conf_loss: 514.96   prob_loss: 1.40   total_loss: 518.03\n",
      "=> STEP  790/6250   lr: 0.000981   giou_loss: 1.71   conf_loss: 514.47   prob_loss: 1.39   total_loss: 517.57\n",
      "=> STEP  791/6250   lr: 0.000981   giou_loss: 1.76   conf_loss: 514.02   prob_loss: 1.39   total_loss: 517.18\n",
      "=> STEP  792/6250   lr: 0.000981   giou_loss: 1.63   conf_loss: 513.49   prob_loss: 1.40   total_loss: 516.53\n",
      "=> STEP  793/6250   lr: 0.000981   giou_loss: 1.73   conf_loss: 513.02   prob_loss: 1.41   total_loss: 516.15\n",
      "=> STEP  794/6250   lr: 0.000981   giou_loss: 1.81   conf_loss: 512.52   prob_loss: 1.41   total_loss: 515.75\n",
      "=> STEP  795/6250   lr: 0.000981   giou_loss: 1.73   conf_loss: 512.05   prob_loss: 1.41   total_loss: 515.19\n",
      "=> STEP  796/6250   lr: 0.000981   giou_loss: 1.57   conf_loss: 511.61   prob_loss: 1.39   total_loss: 514.57\n",
      "=> STEP  797/6250   lr: 0.000981   giou_loss: 1.58   conf_loss: 511.12   prob_loss: 1.38   total_loss: 514.08\n",
      "=> STEP  798/6250   lr: 0.000981   giou_loss: 1.68   conf_loss: 510.64   prob_loss: 1.39   total_loss: 513.71\n",
      "=> STEP  799/6250   lr: 0.000981   giou_loss: 1.69   conf_loss: 510.17   prob_loss: 1.39   total_loss: 513.25\n",
      "=> STEP  800/6250   lr: 0.000980   giou_loss: 1.61   conf_loss: 509.68   prob_loss: 1.39   total_loss: 512.68\n",
      "=> STEP  801/6250   lr: 0.000980   giou_loss: 1.56   conf_loss: 509.22   prob_loss: 1.39   total_loss: 512.17\n",
      "=> STEP  802/6250   lr: 0.000980   giou_loss: 1.58   conf_loss: 508.74   prob_loss: 1.38   total_loss: 511.70\n",
      "=> STEP  803/6250   lr: 0.000980   giou_loss: 1.64   conf_loss: 508.31   prob_loss: 1.38   total_loss: 511.32\n",
      "=> STEP  804/6250   lr: 0.000980   giou_loss: 1.58   conf_loss: 507.84   prob_loss: 1.38   total_loss: 510.79\n",
      "=> STEP  805/6250   lr: 0.000980   giou_loss: 1.73   conf_loss: 507.38   prob_loss: 1.40   total_loss: 510.51\n",
      "=> STEP  806/6250   lr: 0.000980   giou_loss: 1.87   conf_loss: 506.89   prob_loss: 1.40   total_loss: 510.17\n",
      "=> STEP  807/6250   lr: 0.000980   giou_loss: 1.74   conf_loss: 506.45   prob_loss: 1.39   total_loss: 509.57\n",
      "=> STEP  808/6250   lr: 0.000980   giou_loss: 1.67   conf_loss: 506.01   prob_loss: 1.37   total_loss: 509.04\n",
      "=> STEP  809/6250   lr: 0.000980   giou_loss: 1.80   conf_loss: 505.55   prob_loss: 1.36   total_loss: 508.71\n",
      "=> STEP  810/6250   lr: 0.000980   giou_loss: 1.68   conf_loss: 505.07   prob_loss: 1.37   total_loss: 508.13\n",
      "=> STEP  811/6250   lr: 0.000980   giou_loss: 1.65   conf_loss: 504.57   prob_loss: 1.39   total_loss: 507.61\n",
      "=> STEP  812/6250   lr: 0.000980   giou_loss: 1.67   conf_loss: 504.12   prob_loss: 1.40   total_loss: 507.18\n",
      "=> STEP  813/6250   lr: 0.000980   giou_loss: 1.64   conf_loss: 503.64   prob_loss: 1.39   total_loss: 506.67\n",
      "=> STEP  814/6250   lr: 0.000979   giou_loss: 1.58   conf_loss: 503.18   prob_loss: 1.38   total_loss: 506.14\n",
      "=> STEP  815/6250   lr: 0.000979   giou_loss: 1.68   conf_loss: 502.72   prob_loss: 1.38   total_loss: 505.78\n",
      "=> STEP  816/6250   lr: 0.000979   giou_loss: 1.63   conf_loss: 502.25   prob_loss: 1.38   total_loss: 505.26\n",
      "=> STEP  817/6250   lr: 0.000979   giou_loss: 1.70   conf_loss: 501.81   prob_loss: 1.38   total_loss: 504.88\n",
      "=> STEP  818/6250   lr: 0.000979   giou_loss: 1.71   conf_loss: 501.35   prob_loss: 1.37   total_loss: 504.43\n",
      "=> STEP  819/6250   lr: 0.000979   giou_loss: 1.67   conf_loss: 500.88   prob_loss: 1.38   total_loss: 503.92\n",
      "=> STEP  820/6250   lr: 0.000979   giou_loss: 1.56   conf_loss: 500.44   prob_loss: 1.38   total_loss: 503.38\n",
      "=> STEP  821/6250   lr: 0.000979   giou_loss: 1.87   conf_loss: 499.96   prob_loss: 1.38   total_loss: 503.20\n",
      "=> STEP  822/6250   lr: 0.000979   giou_loss: 1.86   conf_loss: 499.53   prob_loss: 1.38   total_loss: 502.77\n",
      "=> STEP  823/6250   lr: 0.000979   giou_loss: 1.56   conf_loss: 499.05   prob_loss: 1.37   total_loss: 501.99\n",
      "=> STEP  824/6250   lr: 0.000979   giou_loss: 1.92   conf_loss: 498.59   prob_loss: 1.38   total_loss: 501.88\n",
      "=> STEP  825/6250   lr: 0.000979   giou_loss: 2.02   conf_loss: 498.14   prob_loss: 1.38   total_loss: 501.54\n",
      "=> STEP  826/6250   lr: 0.000979   giou_loss: 1.93   conf_loss: 497.65   prob_loss: 1.38   total_loss: 500.96\n",
      "=> STEP  827/6250   lr: 0.000978   giou_loss: 1.82   conf_loss: 497.22   prob_loss: 1.38   total_loss: 500.42\n",
      "=> STEP  828/6250   lr: 0.000978   giou_loss: 1.57   conf_loss: 496.78   prob_loss: 1.36   total_loss: 499.71\n",
      "=> STEP  829/6250   lr: 0.000978   giou_loss: 1.87   conf_loss: 496.35   prob_loss: 1.35   total_loss: 499.56\n",
      "=> STEP  830/6250   lr: 0.000978   giou_loss: 1.92   conf_loss: 495.90   prob_loss: 1.35   total_loss: 499.18\n",
      "=> STEP  831/6250   lr: 0.000978   giou_loss: 1.71   conf_loss: 495.41   prob_loss: 1.37   total_loss: 498.48\n",
      "=> STEP  832/6250   lr: 0.000978   giou_loss: 1.73   conf_loss: 494.96   prob_loss: 1.40   total_loss: 498.09\n",
      "=> STEP  833/6250   lr: 0.000978   giou_loss: 1.83   conf_loss: 494.50   prob_loss: 1.39   total_loss: 497.72\n",
      "=> STEP  834/6250   lr: 0.000978   giou_loss: 1.66   conf_loss: 494.06   prob_loss: 1.37   total_loss: 497.09\n",
      "=> STEP  835/6250   lr: 0.000978   giou_loss: 1.74   conf_loss: 493.65   prob_loss: 1.34   total_loss: 496.73\n",
      "=> STEP  836/6250   lr: 0.000978   giou_loss: 1.80   conf_loss: 493.22   prob_loss: 1.34   total_loss: 496.36\n",
      "=> STEP  837/6250   lr: 0.000978   giou_loss: 1.65   conf_loss: 492.73   prob_loss: 1.36   total_loss: 495.74\n",
      "=> STEP  838/6250   lr: 0.000978   giou_loss: 1.67   conf_loss: 492.29   prob_loss: 1.37   total_loss: 495.33\n",
      "=> STEP  839/6250   lr: 0.000978   giou_loss: 1.74   conf_loss: 491.86   prob_loss: 1.37   total_loss: 494.97\n",
      "=> STEP  840/6250   lr: 0.000978   giou_loss: 1.63   conf_loss: 491.38   prob_loss: 1.36   total_loss: 494.37\n",
      "=> STEP  841/6250   lr: 0.000977   giou_loss: 1.78   conf_loss: 490.95   prob_loss: 1.35   total_loss: 494.08\n",
      "=> STEP  842/6250   lr: 0.000977   giou_loss: 1.92   conf_loss: 490.54   prob_loss: 1.34   total_loss: 493.80\n",
      "=> STEP  843/6250   lr: 0.000977   giou_loss: 1.84   conf_loss: 490.08   prob_loss: 1.34   total_loss: 493.27\n",
      "=> STEP  844/6250   lr: 0.000977   giou_loss: 1.56   conf_loss: 489.63   prob_loss: 1.35   total_loss: 492.54\n",
      "=> STEP  845/6250   lr: 0.000977   giou_loss: 1.82   conf_loss: 489.21   prob_loss: 1.36   total_loss: 492.39\n",
      "=> STEP  846/6250   lr: 0.000977   giou_loss: 1.73   conf_loss: 488.75   prob_loss: 1.35   total_loss: 491.83\n",
      "=> STEP  847/6250   lr: 0.000977   giou_loss: 1.67   conf_loss: 488.33   prob_loss: 1.34   total_loss: 491.34\n",
      "=> STEP  848/6250   lr: 0.000977   giou_loss: 1.72   conf_loss: 487.88   prob_loss: 1.33   total_loss: 490.94\n",
      "=> STEP  849/6250   lr: 0.000977   giou_loss: 1.68   conf_loss: 487.45   prob_loss: 1.34   total_loss: 490.47\n",
      "=> STEP  850/6250   lr: 0.000977   giou_loss: 1.78   conf_loss: 486.98   prob_loss: 1.35   total_loss: 490.10\n",
      "=> STEP  851/6250   lr: 0.000977   giou_loss: 1.74   conf_loss: 486.56   prob_loss: 1.34   total_loss: 489.65\n",
      "=> STEP  852/6250   lr: 0.000977   giou_loss: 1.57   conf_loss: 486.10   prob_loss: 1.34   total_loss: 489.00\n",
      "=> STEP  853/6250   lr: 0.000977   giou_loss: 1.82   conf_loss: 485.66   prob_loss: 1.34   total_loss: 488.81\n",
      "=> STEP  854/6250   lr: 0.000976   giou_loss: 1.82   conf_loss: 485.21   prob_loss: 1.33   total_loss: 488.36\n",
      "=> STEP  855/6250   lr: 0.000976   giou_loss: 1.60   conf_loss: 484.77   prob_loss: 1.33   total_loss: 487.71\n",
      "=> STEP  856/6250   lr: 0.000976   giou_loss: 1.77   conf_loss: 484.37   prob_loss: 1.33   total_loss: 487.47\n",
      "=> STEP  857/6250   lr: 0.000976   giou_loss: 1.84   conf_loss: 483.92   prob_loss: 1.33   total_loss: 487.09\n",
      "=> STEP  858/6250   lr: 0.000976   giou_loss: 1.74   conf_loss: 483.48   prob_loss: 1.33   total_loss: 486.55\n",
      "=> STEP  859/6250   lr: 0.000976   giou_loss: 1.59   conf_loss: 483.03   prob_loss: 1.34   total_loss: 485.96\n",
      "=> STEP  860/6250   lr: 0.000976   giou_loss: 1.63   conf_loss: 482.58   prob_loss: 1.34   total_loss: 485.55\n",
      "=> STEP  861/6250   lr: 0.000976   giou_loss: 1.59   conf_loss: 482.15   prob_loss: 1.33   total_loss: 485.07\n",
      "=> STEP  862/6250   lr: 0.000976   giou_loss: 1.56   conf_loss: 481.72   prob_loss: 1.33   total_loss: 484.61\n",
      "=> STEP  863/6250   lr: 0.000976   giou_loss: 1.61   conf_loss: 481.27   prob_loss: 1.33   total_loss: 484.20\n",
      "=> STEP  864/6250   lr: 0.000976   giou_loss: 1.61   conf_loss: 480.84   prob_loss: 1.33   total_loss: 483.78\n",
      "=> STEP  865/6250   lr: 0.000976   giou_loss: 1.65   conf_loss: 480.42   prob_loss: 1.33   total_loss: 483.40\n",
      "=> STEP  866/6250   lr: 0.000976   giou_loss: 1.60   conf_loss: 479.97   prob_loss: 1.33   total_loss: 482.90\n",
      "=> STEP  867/6250   lr: 0.000975   giou_loss: 1.64   conf_loss: 479.56   prob_loss: 1.33   total_loss: 482.54\n",
      "=> STEP  868/6250   lr: 0.000975   giou_loss: 1.71   conf_loss: 479.10   prob_loss: 1.33   total_loss: 482.14\n",
      "=> STEP  869/6250   lr: 0.000975   giou_loss: 1.58   conf_loss: 478.70   prob_loss: 1.33   total_loss: 481.61\n",
      "=> STEP  870/6250   lr: 0.000975   giou_loss: 1.78   conf_loss: 478.26   prob_loss: 1.32   total_loss: 481.36\n",
      "=> STEP  871/6250   lr: 0.000975   giou_loss: 1.83   conf_loss: 477.85   prob_loss: 1.32   total_loss: 481.00\n",
      "=> STEP  872/6250   lr: 0.000975   giou_loss: 1.72   conf_loss: 477.40   prob_loss: 1.33   total_loss: 480.44\n",
      "=> STEP  873/6250   lr: 0.000975   giou_loss: 1.64   conf_loss: 476.95   prob_loss: 1.34   total_loss: 479.93\n",
      "=> STEP  874/6250   lr: 0.000975   giou_loss: 1.70   conf_loss: 476.52   prob_loss: 1.34   total_loss: 479.56\n",
      "=> STEP  875/6250   lr: 0.000975   giou_loss: 1.59   conf_loss: 476.10   prob_loss: 1.33   total_loss: 479.02\n",
      "=> STEP  876/6250   lr: 0.000975   giou_loss: 1.76   conf_loss: 475.70   prob_loss: 1.32   total_loss: 478.78\n",
      "=> STEP  877/6250   lr: 0.000975   giou_loss: 1.88   conf_loss: 475.28   prob_loss: 1.31   total_loss: 478.47\n",
      "=> STEP  878/6250   lr: 0.000975   giou_loss: 1.74   conf_loss: 474.82   prob_loss: 1.32   total_loss: 477.88\n",
      "=> STEP  879/6250   lr: 0.000974   giou_loss: 1.63   conf_loss: 474.40   prob_loss: 1.34   total_loss: 477.37\n",
      "=> STEP  880/6250   lr: 0.000974   giou_loss: 1.68   conf_loss: 473.95   prob_loss: 1.33   total_loss: 476.96\n",
      "=> STEP  881/6250   lr: 0.000974   giou_loss: 1.57   conf_loss: 473.55   prob_loss: 1.32   total_loss: 476.44\n",
      "=> STEP  882/6250   lr: 0.000974   giou_loss: 1.74   conf_loss: 473.13   prob_loss: 1.32   total_loss: 476.18\n",
      "=> STEP  883/6250   lr: 0.000974   giou_loss: 1.64   conf_loss: 472.70   prob_loss: 1.32   total_loss: 475.66\n",
      "=> STEP  884/6250   lr: 0.000974   giou_loss: 1.59   conf_loss: 472.28   prob_loss: 1.33   total_loss: 475.20\n",
      "=> STEP  885/6250   lr: 0.000974   giou_loss: 1.64   conf_loss: 471.85   prob_loss: 1.32   total_loss: 474.82\n",
      "=> STEP  886/6250   lr: 0.000974   giou_loss: 1.61   conf_loss: 471.45   prob_loss: 1.32   total_loss: 474.38\n",
      "=> STEP  887/6250   lr: 0.000974   giou_loss: 1.61   conf_loss: 471.01   prob_loss: 1.31   total_loss: 473.93\n",
      "=> STEP  888/6250   lr: 0.000974   giou_loss: 1.56   conf_loss: 470.61   prob_loss: 1.31   total_loss: 473.49\n",
      "=> STEP  889/6250   lr: 0.000974   giou_loss: 1.65   conf_loss: 470.17   prob_loss: 1.32   total_loss: 473.13\n",
      "=> STEP  890/6250   lr: 0.000974   giou_loss: 1.56   conf_loss: 469.75   prob_loss: 1.32   total_loss: 472.63\n",
      "=> STEP  891/6250   lr: 0.000973   giou_loss: 1.62   conf_loss: 469.32   prob_loss: 1.31   total_loss: 472.26\n",
      "=> STEP  892/6250   lr: 0.000973   giou_loss: 1.65   conf_loss: 468.91   prob_loss: 1.31   total_loss: 471.86\n",
      "=> STEP  893/6250   lr: 0.000973   giou_loss: 1.57   conf_loss: 468.49   prob_loss: 1.31   total_loss: 471.36\n",
      "=> STEP  894/6250   lr: 0.000973   giou_loss: 1.63   conf_loss: 468.06   prob_loss: 1.31   total_loss: 471.00\n",
      "=> STEP  895/6250   lr: 0.000973   giou_loss: 1.57   conf_loss: 467.63   prob_loss: 1.32   total_loss: 470.52\n",
      "=> STEP  896/6250   lr: 0.000973   giou_loss: 1.57   conf_loss: 467.21   prob_loss: 1.31   total_loss: 470.09\n",
      "=> STEP  897/6250   lr: 0.000973   giou_loss: 1.62   conf_loss: 466.79   prob_loss: 1.31   total_loss: 469.73\n",
      "=> STEP  898/6250   lr: 0.000973   giou_loss: 1.59   conf_loss: 466.37   prob_loss: 1.31   total_loss: 469.27\n",
      "=> STEP  899/6250   lr: 0.000973   giou_loss: 1.58   conf_loss: 465.97   prob_loss: 1.30   total_loss: 468.86\n",
      "=> STEP  900/6250   lr: 0.000973   giou_loss: 1.60   conf_loss: 465.55   prob_loss: 1.30   total_loss: 468.46\n",
      "=> STEP  901/6250   lr: 0.000973   giou_loss: 1.61   conf_loss: 465.13   prob_loss: 1.30   total_loss: 468.05\n",
      "=> STEP  902/6250   lr: 0.000973   giou_loss: 1.56   conf_loss: 464.73   prob_loss: 1.30   total_loss: 467.59\n",
      "=> STEP  903/6250   lr: 0.000973   giou_loss: 1.59   conf_loss: 464.31   prob_loss: 1.30   total_loss: 467.20\n",
      "=> STEP  904/6250   lr: 0.000972   giou_loss: 1.57   conf_loss: 463.91   prob_loss: 1.30   total_loss: 466.78\n",
      "=> STEP  905/6250   lr: 0.000972   giou_loss: 1.67   conf_loss: 463.49   prob_loss: 1.30   total_loss: 466.46\n",
      "=> STEP  906/6250   lr: 0.000972   giou_loss: 1.63   conf_loss: 463.09   prob_loss: 1.30   total_loss: 466.02\n",
      "=> STEP  907/6250   lr: 0.000972   giou_loss: 1.70   conf_loss: 462.68   prob_loss: 1.29   total_loss: 465.67\n",
      "=> STEP  908/6250   lr: 0.000972   giou_loss: 1.64   conf_loss: 462.28   prob_loss: 1.29   total_loss: 465.21\n",
      "=> STEP  909/6250   lr: 0.000972   giou_loss: 1.77   conf_loss: 461.85   prob_loss: 1.29   total_loss: 464.91\n",
      "=> STEP  910/6250   lr: 0.000972   giou_loss: 1.70   conf_loss: 461.46   prob_loss: 1.29   total_loss: 464.46\n",
      "=> STEP  911/6250   lr: 0.000972   giou_loss: 1.69   conf_loss: 461.06   prob_loss: 1.30   total_loss: 464.04\n",
      "=> STEP  912/6250   lr: 0.000972   giou_loss: 1.66   conf_loss: 460.65   prob_loss: 1.30   total_loss: 463.62\n",
      "=> STEP  913/6250   lr: 0.000972   giou_loss: 1.73   conf_loss: 460.24   prob_loss: 1.30   total_loss: 463.27\n",
      "=> STEP  914/6250   lr: 0.000972   giou_loss: 1.86   conf_loss: 459.82   prob_loss: 1.29   total_loss: 462.98\n",
      "=> STEP  915/6250   lr: 0.000971   giou_loss: 1.69   conf_loss: 459.45   prob_loss: 1.29   total_loss: 462.42\n",
      "=> STEP  916/6250   lr: 0.000971   giou_loss: 1.80   conf_loss: 459.03   prob_loss: 1.29   total_loss: 462.12\n",
      "=> STEP  917/6250   lr: 0.000971   giou_loss: 1.88   conf_loss: 458.64   prob_loss: 1.29   total_loss: 461.81\n",
      "=> STEP  918/6250   lr: 0.000971   giou_loss: 1.74   conf_loss: 458.22   prob_loss: 1.29   total_loss: 461.25\n",
      "=> STEP  919/6250   lr: 0.000971   giou_loss: 1.69   conf_loss: 457.80   prob_loss: 1.29   total_loss: 460.79\n",
      "=> STEP  920/6250   lr: 0.000971   giou_loss: 1.65   conf_loss: 457.42   prob_loss: 1.30   total_loss: 460.37\n",
      "=> STEP  921/6250   lr: 0.000971   giou_loss: 1.70   conf_loss: 456.99   prob_loss: 1.30   total_loss: 459.99\n",
      "=> STEP  922/6250   lr: 0.000971   giou_loss: 1.68   conf_loss: 456.60   prob_loss: 1.30   total_loss: 459.58\n",
      "=> STEP  923/6250   lr: 0.000971   giou_loss: 1.59   conf_loss: 456.20   prob_loss: 1.29   total_loss: 459.08\n",
      "=> STEP  924/6250   lr: 0.000971   giou_loss: 1.61   conf_loss: 455.80   prob_loss: 1.28   total_loss: 458.68\n",
      "=> STEP  925/6250   lr: 0.000971   giou_loss: 1.56   conf_loss: 455.42   prob_loss: 1.28   total_loss: 458.27\n",
      "=> STEP  926/6250   lr: 0.000971   giou_loss: 1.57   conf_loss: 454.97   prob_loss: 1.29   total_loss: 457.83\n",
      "=> STEP  927/6250   lr: 0.000970   giou_loss: 1.61   conf_loss: 454.60   prob_loss: 1.29   total_loss: 457.49\n",
      "=> STEP  928/6250   lr: 0.000970   giou_loss: 1.67   conf_loss: 454.19   prob_loss: 1.29   total_loss: 457.14\n",
      "=> STEP  929/6250   lr: 0.000970   giou_loss: 1.63   conf_loss: 453.77   prob_loss: 1.29   total_loss: 456.69\n",
      "=> STEP  930/6250   lr: 0.000970   giou_loss: 1.72   conf_loss: 453.41   prob_loss: 1.29   total_loss: 456.42\n",
      "=> STEP  931/6250   lr: 0.000970   giou_loss: 1.71   conf_loss: 453.00   prob_loss: 1.29   total_loss: 456.00\n",
      "=> STEP  932/6250   lr: 0.000970   giou_loss: 1.62   conf_loss: 452.65   prob_loss: 1.28   total_loss: 455.54\n",
      "=> STEP  933/6250   lr: 0.000970   giou_loss: 1.56   conf_loss: 452.21   prob_loss: 1.27   total_loss: 455.04\n",
      "=> STEP  934/6250   lr: 0.000970   giou_loss: 1.71   conf_loss: 451.85   prob_loss: 1.26   total_loss: 454.81\n",
      "=> STEP  935/6250   lr: 0.000970   giou_loss: 1.58   conf_loss: 451.44   prob_loss: 1.27   total_loss: 454.29\n",
      "=> STEP  936/6250   lr: 0.000970   giou_loss: 1.75   conf_loss: 451.01   prob_loss: 1.29   total_loss: 454.05\n",
      "=> STEP  937/6250   lr: 0.000970   giou_loss: 1.83   conf_loss: 450.61   prob_loss: 1.30   total_loss: 453.73\n",
      "=> STEP  938/6250   lr: 0.000970   giou_loss: 1.74   conf_loss: 450.23   prob_loss: 1.28   total_loss: 453.25\n",
      "=> STEP  939/6250   lr: 0.000969   giou_loss: 1.56   conf_loss: 449.84   prob_loss: 1.27   total_loss: 452.67\n",
      "=> STEP  940/6250   lr: 0.000969   giou_loss: 1.78   conf_loss: 449.46   prob_loss: 1.26   total_loss: 452.50\n",
      "=> STEP  941/6250   lr: 0.000969   giou_loss: 1.77   conf_loss: 449.04   prob_loss: 1.27   total_loss: 452.08\n",
      "=> STEP  942/6250   lr: 0.000969   giou_loss: 1.66   conf_loss: 448.62   prob_loss: 1.28   total_loss: 451.56\n",
      "=> STEP  943/6250   lr: 0.000969   giou_loss: 1.79   conf_loss: 448.23   prob_loss: 1.29   total_loss: 451.31\n",
      "=> STEP  944/6250   lr: 0.000969   giou_loss: 1.96   conf_loss: 447.82   prob_loss: 1.28   total_loss: 451.07\n",
      "=> STEP  945/6250   lr: 0.000969   giou_loss: 1.89   conf_loss: 447.43   prob_loss: 1.28   total_loss: 450.60\n",
      "=> STEP  946/6250   lr: 0.000969   giou_loss: 1.74   conf_loss: 447.03   prob_loss: 1.27   total_loss: 450.04\n",
      "=> STEP  947/6250   lr: 0.000969   giou_loss: 1.69   conf_loss: 446.66   prob_loss: 1.27   total_loss: 449.62\n",
      "=> STEP  948/6250   lr: 0.000969   giou_loss: 1.73   conf_loss: 446.28   prob_loss: 1.26   total_loss: 449.26\n",
      "=> STEP  949/6250   lr: 0.000969   giou_loss: 1.67   conf_loss: 445.87   prob_loss: 1.26   total_loss: 448.80\n",
      "=> STEP  950/6250   lr: 0.000968   giou_loss: 1.75   conf_loss: 445.47   prob_loss: 1.27   total_loss: 448.50\n",
      "=> STEP  951/6250   lr: 0.000968   giou_loss: 1.84   conf_loss: 445.08   prob_loss: 1.28   total_loss: 448.21\n",
      "=> STEP  952/6250   lr: 0.000968   giou_loss: 1.78   conf_loss: 444.69   prob_loss: 1.28   total_loss: 447.75\n",
      "=> STEP  953/6250   lr: 0.000968   giou_loss: 1.62   conf_loss: 444.34   prob_loss: 1.27   total_loss: 447.22\n",
      "=> STEP  954/6250   lr: 0.000968   giou_loss: 1.73   conf_loss: 443.94   prob_loss: 1.25   total_loss: 446.93\n",
      "=> STEP  955/6250   lr: 0.000968   giou_loss: 1.89   conf_loss: 443.56   prob_loss: 1.25   total_loss: 446.70\n",
      "=> STEP  956/6250   lr: 0.000968   giou_loss: 1.82   conf_loss: 443.16   prob_loss: 1.26   total_loss: 446.25\n",
      "=> STEP  957/6250   lr: 0.000968   giou_loss: 1.58   conf_loss: 442.75   prob_loss: 1.27   total_loss: 445.60\n",
      "=> STEP  958/6250   lr: 0.000968   giou_loss: 1.66   conf_loss: 442.37   prob_loss: 1.28   total_loss: 445.31\n",
      "=> STEP  959/6250   lr: 0.000968   giou_loss: 1.65   conf_loss: 441.99   prob_loss: 1.27   total_loss: 444.91\n",
      "=> STEP  960/6250   lr: 0.000968   giou_loss: 1.58   conf_loss: 441.59   prob_loss: 1.26   total_loss: 444.42\n",
      "=> STEP  961/6250   lr: 0.000967   giou_loss: 1.72   conf_loss: 441.23   prob_loss: 1.26   total_loss: 444.21\n",
      "=> STEP  962/6250   lr: 0.000967   giou_loss: 1.66   conf_loss: 440.83   prob_loss: 1.26   total_loss: 443.75\n",
      "=> STEP  963/6250   lr: 0.000967   giou_loss: 1.66   conf_loss: 440.43   prob_loss: 1.27   total_loss: 443.36\n",
      "=> STEP  964/6250   lr: 0.000967   giou_loss: 1.71   conf_loss: 440.04   prob_loss: 1.27   total_loss: 443.02\n",
      "=> STEP  965/6250   lr: 0.000967   giou_loss: 1.85   conf_loss: 439.65   prob_loss: 1.28   total_loss: 442.78\n",
      "=> STEP  966/6250   lr: 0.000967   giou_loss: 1.86   conf_loss: 439.26   prob_loss: 1.27   total_loss: 442.39\n",
      "=> STEP  967/6250   lr: 0.000967   giou_loss: 1.90   conf_loss: 438.89   prob_loss: 1.26   total_loss: 442.05\n",
      "=> STEP  968/6250   lr: 0.000967   giou_loss: 1.68   conf_loss: 438.52   prob_loss: 1.25   total_loss: 441.46\n",
      "=> STEP  969/6250   lr: 0.000967   giou_loss: 1.83   conf_loss: 438.15   prob_loss: 1.25   total_loss: 441.23\n",
      "=> STEP  970/6250   lr: 0.000967   giou_loss: 1.96   conf_loss: 437.78   prob_loss: 1.26   total_loss: 440.99\n",
      "=> STEP  971/6250   lr: 0.000967   giou_loss: 1.90   conf_loss: 437.38   prob_loss: 1.26   total_loss: 440.54\n",
      "=> STEP  972/6250   lr: 0.000966   giou_loss: 1.80   conf_loss: 436.98   prob_loss: 1.26   total_loss: 440.04\n",
      "=> STEP  973/6250   lr: 0.000966   giou_loss: 1.69   conf_loss: 436.61   prob_loss: 1.28   total_loss: 439.58\n",
      "=> STEP  974/6250   lr: 0.000966   giou_loss: 1.74   conf_loss: 436.22   prob_loss: 1.28   total_loss: 439.24\n",
      "=> STEP  975/6250   lr: 0.000966   giou_loss: 1.69   conf_loss: 435.85   prob_loss: 1.27   total_loss: 438.80\n",
      "=> STEP  976/6250   lr: 0.000966   giou_loss: 1.64   conf_loss: 435.52   prob_loss: 1.25   total_loss: 438.41\n",
      "=> STEP  977/6250   lr: 0.000966   giou_loss: 1.69   conf_loss: 435.12   prob_loss: 1.24   total_loss: 438.05\n",
      "=> STEP  978/6250   lr: 0.000966   giou_loss: 1.59   conf_loss: 434.75   prob_loss: 1.25   total_loss: 437.59\n",
      "=> STEP  979/6250   lr: 0.000966   giou_loss: 1.56   conf_loss: 434.36   prob_loss: 1.25   total_loss: 437.17\n",
      "=> STEP  980/6250   lr: 0.000966   giou_loss: 1.56   conf_loss: 433.96   prob_loss: 1.25   total_loss: 436.78\n",
      "=> STEP  981/6250   lr: 0.000966   giou_loss: 1.62   conf_loss: 433.60   prob_loss: 1.25   total_loss: 436.47\n",
      "=> STEP  982/6250   lr: 0.000966   giou_loss: 1.67   conf_loss: 433.22   prob_loss: 1.25   total_loss: 436.13\n",
      "=> STEP  983/6250   lr: 0.000965   giou_loss: 1.74   conf_loss: 432.83   prob_loss: 1.24   total_loss: 435.82\n",
      "=> STEP  984/6250   lr: 0.000965   giou_loss: 1.61   conf_loss: 432.51   prob_loss: 1.24   total_loss: 435.36\n",
      "=> STEP  985/6250   lr: 0.000965   giou_loss: 1.58   conf_loss: 432.11   prob_loss: 1.25   total_loss: 434.93\n",
      "=> STEP  986/6250   lr: 0.000965   giou_loss: 1.70   conf_loss: 431.73   prob_loss: 1.25   total_loss: 434.68\n",
      "=> STEP  987/6250   lr: 0.000965   giou_loss: 1.70   conf_loss: 431.36   prob_loss: 1.25   total_loss: 434.31\n",
      "=> STEP  988/6250   lr: 0.000965   giou_loss: 1.61   conf_loss: 430.97   prob_loss: 1.24   total_loss: 433.83\n",
      "=> STEP  989/6250   lr: 0.000965   giou_loss: 1.64   conf_loss: 430.63   prob_loss: 1.24   total_loss: 433.50\n",
      "=> STEP  990/6250   lr: 0.000965   giou_loss: 1.65   conf_loss: 430.24   prob_loss: 1.23   total_loss: 433.13\n",
      "=> STEP  991/6250   lr: 0.000965   giou_loss: 1.61   conf_loss: 429.88   prob_loss: 1.24   total_loss: 432.72\n",
      "=> STEP  992/6250   lr: 0.000965   giou_loss: 1.73   conf_loss: 429.51   prob_loss: 1.23   total_loss: 432.48\n",
      "=> STEP  993/6250   lr: 0.000965   giou_loss: 1.70   conf_loss: 429.16   prob_loss: 1.24   total_loss: 432.09\n",
      "=> STEP  994/6250   lr: 0.000964   giou_loss: 1.74   conf_loss: 428.77   prob_loss: 1.24   total_loss: 431.74\n",
      "=> STEP  995/6250   lr: 0.000964   giou_loss: 1.81   conf_loss: 428.40   prob_loss: 1.23   total_loss: 431.45\n",
      "=> STEP  996/6250   lr: 0.000964   giou_loss: 1.66   conf_loss: 428.04   prob_loss: 1.23   total_loss: 430.93\n",
      "=> STEP  997/6250   lr: 0.000964   giou_loss: 1.69   conf_loss: 427.69   prob_loss: 1.22   total_loss: 430.60\n",
      "=> STEP  998/6250   lr: 0.000964   giou_loss: 1.78   conf_loss: 427.33   prob_loss: 1.23   total_loss: 430.34\n",
      "=> STEP  999/6250   lr: 0.000964   giou_loss: 1.62   conf_loss: 426.96   prob_loss: 1.23   total_loss: 429.81\n",
      "=> STEP 1000/6250   lr: 0.000964   giou_loss: 1.85   conf_loss: 426.57   prob_loss: 1.23   total_loss: 429.65\n",
      "=> STEP 1001/6250   lr: 0.000964   giou_loss: 1.88   conf_loss: 426.21   prob_loss: 1.24   total_loss: 429.33\n",
      "=> STEP 1002/6250   lr: 0.000964   giou_loss: 1.76   conf_loss: 425.84   prob_loss: 1.24   total_loss: 428.83\n",
      "=> STEP 1003/6250   lr: 0.000964   giou_loss: 1.58   conf_loss: 425.48   prob_loss: 1.23   total_loss: 428.29\n",
      "=> STEP 1004/6250   lr: 0.000963   giou_loss: 1.70   conf_loss: 425.14   prob_loss: 1.22   total_loss: 428.06\n",
      "=> STEP 1005/6250   lr: 0.000963   giou_loss: 1.65   conf_loss: 424.77   prob_loss: 1.22   total_loss: 427.64\n",
      "=> STEP 1006/6250   lr: 0.000963   giou_loss: 1.63   conf_loss: 424.38   prob_loss: 1.24   total_loss: 427.25\n",
      "=> STEP 1007/6250   lr: 0.000963   giou_loss: 1.60   conf_loss: 424.03   prob_loss: 1.24   total_loss: 426.87\n",
      "=> STEP 1008/6250   lr: 0.000963   giou_loss: 1.65   conf_loss: 423.64   prob_loss: 1.23   total_loss: 426.52\n",
      "=> STEP 1009/6250   lr: 0.000963   giou_loss: 1.64   conf_loss: 423.30   prob_loss: 1.23   total_loss: 426.16\n",
      "=> STEP 1010/6250   lr: 0.000963   giou_loss: 1.75   conf_loss: 422.90   prob_loss: 1.23   total_loss: 425.87\n",
      "=> STEP 1011/6250   lr: 0.000963   giou_loss: 1.68   conf_loss: 422.54   prob_loss: 1.24   total_loss: 425.46\n",
      "=> STEP 1012/6250   lr: 0.000963   giou_loss: 1.71   conf_loss: 422.18   prob_loss: 1.24   total_loss: 425.13\n",
      "=> STEP 1013/6250   lr: 0.000963   giou_loss: 1.77   conf_loss: 421.82   prob_loss: 1.23   total_loss: 424.82\n",
      "=> STEP 1014/6250   lr: 0.000962   giou_loss: 1.59   conf_loss: 421.49   prob_loss: 1.22   total_loss: 424.30\n",
      "=> STEP 1015/6250   lr: 0.000962   giou_loss: 1.88   conf_loss: 421.12   prob_loss: 1.22   total_loss: 424.22\n",
      "=> STEP 1016/6250   lr: 0.000962   giou_loss: 1.93   conf_loss: 420.76   prob_loss: 1.22   total_loss: 423.92\n",
      "=> STEP 1017/6250   lr: 0.000962   giou_loss: 1.74   conf_loss: 420.40   prob_loss: 1.22   total_loss: 423.36\n",
      "=> STEP 1018/6250   lr: 0.000962   giou_loss: 1.75   conf_loss: 420.03   prob_loss: 1.23   total_loss: 423.00\n",
      "=> STEP 1019/6250   lr: 0.000962   giou_loss: 1.85   conf_loss: 419.68   prob_loss: 1.23   total_loss: 422.75\n",
      "=> STEP 1020/6250   lr: 0.000962   giou_loss: 1.68   conf_loss: 419.32   prob_loss: 1.23   total_loss: 422.22\n",
      "=> STEP 1021/6250   lr: 0.000962   giou_loss: 1.68   conf_loss: 418.94   prob_loss: 1.22   total_loss: 421.84\n",
      "=> STEP 1022/6250   lr: 0.000962   giou_loss: 1.63   conf_loss: 418.60   prob_loss: 1.22   total_loss: 421.45\n",
      "=> STEP 1023/6250   lr: 0.000962   giou_loss: 1.72   conf_loss: 418.25   prob_loss: 1.22   total_loss: 421.19\n",
      "=> STEP 1024/6250   lr: 0.000962   giou_loss: 1.74   conf_loss: 417.87   prob_loss: 1.22   total_loss: 420.83\n",
      "=> STEP 1025/6250   lr: 0.000961   giou_loss: 1.60   conf_loss: 417.52   prob_loss: 1.23   total_loss: 420.34\n",
      "=> STEP 1026/6250   lr: 0.000961   giou_loss: 1.79   conf_loss: 417.14   prob_loss: 1.23   total_loss: 420.16\n",
      "=> STEP 1027/6250   lr: 0.000961   giou_loss: 1.85   conf_loss: 416.78   prob_loss: 1.23   total_loss: 419.86\n",
      "=> STEP 1028/6250   lr: 0.000961   giou_loss: 1.86   conf_loss: 416.45   prob_loss: 1.23   total_loss: 419.53\n",
      "=> STEP 1029/6250   lr: 0.000961   giou_loss: 1.63   conf_loss: 416.08   prob_loss: 1.22   total_loss: 418.93\n",
      "=> STEP 1030/6250   lr: 0.000961   giou_loss: 1.81   conf_loss: 415.78   prob_loss: 1.21   total_loss: 418.79\n",
      "=> STEP 1031/6250   lr: 0.000961   giou_loss: 1.93   conf_loss: 415.44   prob_loss: 1.20   total_loss: 418.56\n",
      "=> STEP 1032/6250   lr: 0.000961   giou_loss: 1.83   conf_loss: 415.06   prob_loss: 1.22   total_loss: 418.11\n",
      "=> STEP 1033/6250   lr: 0.000961   giou_loss: 1.67   conf_loss: 414.70   prob_loss: 1.23   total_loss: 417.60\n",
      "=> STEP 1034/6250   lr: 0.000961   giou_loss: 1.75   conf_loss: 414.33   prob_loss: 1.24   total_loss: 417.32\n",
      "=> STEP 1035/6250   lr: 0.000960   giou_loss: 1.93   conf_loss: 413.97   prob_loss: 1.23   total_loss: 417.13\n",
      "=> STEP 1036/6250   lr: 0.000960   giou_loss: 1.86   conf_loss: 413.62   prob_loss: 1.22   total_loss: 416.70\n",
      "=> STEP 1037/6250   lr: 0.000960   giou_loss: 1.72   conf_loss: 413.28   prob_loss: 1.21   total_loss: 416.21\n",
      "=> STEP 1038/6250   lr: 0.000960   giou_loss: 1.68   conf_loss: 412.94   prob_loss: 1.21   total_loss: 415.83\n",
      "=> STEP 1039/6250   lr: 0.000960   giou_loss: 1.70   conf_loss: 412.59   prob_loss: 1.21   total_loss: 415.49\n",
      "=> STEP 1040/6250   lr: 0.000960   giou_loss: 1.63   conf_loss: 412.22   prob_loss: 1.21   total_loss: 415.06\n",
      "=> STEP 1041/6250   lr: 0.000960   giou_loss: 1.75   conf_loss: 411.86   prob_loss: 1.22   total_loss: 414.83\n",
      "=> STEP 1042/6250   lr: 0.000960   giou_loss: 1.81   conf_loss: 411.52   prob_loss: 1.22   total_loss: 414.54\n",
      "=> STEP 1043/6250   lr: 0.000960   giou_loss: 1.73   conf_loss: 411.18   prob_loss: 1.21   total_loss: 414.12\n",
      "=> STEP 1044/6250   lr: 0.000960   giou_loss: 1.56   conf_loss: 410.85   prob_loss: 1.20   total_loss: 413.61\n",
      "=> STEP 1045/6250   lr: 0.000959   giou_loss: 1.76   conf_loss: 410.51   prob_loss: 1.19   total_loss: 413.46\n",
      "=> STEP 1046/6250   lr: 0.000959   giou_loss: 1.76   conf_loss: 410.15   prob_loss: 1.20   total_loss: 413.11\n",
      "=> STEP 1047/6250   lr: 0.000959   giou_loss: 1.62   conf_loss: 409.79   prob_loss: 1.21   total_loss: 412.62\n",
      "=> STEP 1048/6250   lr: 0.000959   giou_loss: 1.79   conf_loss: 409.44   prob_loss: 1.22   total_loss: 412.45\n",
      "=> STEP 1049/6250   lr: 0.000959   giou_loss: 1.98   conf_loss: 409.08   prob_loss: 1.22   total_loss: 412.28\n",
      "=> STEP 1050/6250   lr: 0.000959   giou_loss: 1.93   conf_loss: 408.74   prob_loss: 1.21   total_loss: 411.88\n",
      "=> STEP 1051/6250   lr: 0.000959   giou_loss: 1.73   conf_loss: 408.41   prob_loss: 1.21   total_loss: 411.34\n",
      "=> STEP 1052/6250   lr: 0.000959   giou_loss: 1.81   conf_loss: 408.08   prob_loss: 1.21   total_loss: 411.09\n",
      "=> STEP 1053/6250   lr: 0.000959   giou_loss: 1.86   conf_loss: 407.74   prob_loss: 1.20   total_loss: 410.80\n",
      "=> STEP 1054/6250   lr: 0.000959   giou_loss: 1.62   conf_loss: 407.40   prob_loss: 1.20   total_loss: 410.22\n",
      "=> STEP 1055/6250   lr: 0.000958   giou_loss: 1.75   conf_loss: 407.04   prob_loss: 1.19   total_loss: 409.99\n",
      "=> STEP 1056/6250   lr: 0.000958   giou_loss: 1.77   conf_loss: 406.69   prob_loss: 1.20   total_loss: 409.66\n",
      "=> STEP 1057/6250   lr: 0.000958   giou_loss: 1.71   conf_loss: 406.37   prob_loss: 1.21   total_loss: 409.28\n",
      "=> STEP 1058/6250   lr: 0.000958   giou_loss: 1.74   conf_loss: 406.03   prob_loss: 1.21   total_loss: 408.98\n",
      "=> STEP 1059/6250   lr: 0.000958   giou_loss: 1.71   conf_loss: 405.67   prob_loss: 1.20   total_loss: 408.58\n",
      "=> STEP 1060/6250   lr: 0.000958   giou_loss: 1.68   conf_loss: 405.33   prob_loss: 1.19   total_loss: 408.20\n",
      "=> STEP 1061/6250   lr: 0.000958   giou_loss: 1.66   conf_loss: 404.97   prob_loss: 1.20   total_loss: 407.83\n",
      "=> STEP 1062/6250   lr: 0.000958   giou_loss: 1.63   conf_loss: 404.63   prob_loss: 1.21   total_loss: 407.47\n",
      "=> STEP 1063/6250   lr: 0.000958   giou_loss: 1.59   conf_loss: 404.26   prob_loss: 1.22   total_loss: 407.07\n",
      "=> STEP 1064/6250   lr: 0.000958   giou_loss: 1.73   conf_loss: 403.94   prob_loss: 1.21   total_loss: 406.87\n",
      "=> STEP 1065/6250   lr: 0.000957   giou_loss: 1.62   conf_loss: 403.59   prob_loss: 1.20   total_loss: 406.42\n",
      "=> STEP 1066/6250   lr: 0.000957   giou_loss: 1.81   conf_loss: 403.26   prob_loss: 1.21   total_loss: 406.29\n",
      "=> STEP 1067/6250   lr: 0.000957   giou_loss: 1.86   conf_loss: 402.93   prob_loss: 1.21   total_loss: 406.00\n",
      "=> STEP 1068/6250   lr: 0.000957   giou_loss: 1.64   conf_loss: 402.59   prob_loss: 1.20   total_loss: 405.43\n",
      "=> STEP 1069/6250   lr: 0.000957   giou_loss: 1.83   conf_loss: 402.25   prob_loss: 1.20   total_loss: 405.27\n",
      "=> STEP 1070/6250   lr: 0.000957   giou_loss: 1.89   conf_loss: 401.89   prob_loss: 1.20   total_loss: 404.97\n",
      "=> STEP 1071/6250   lr: 0.000957   giou_loss: 1.75   conf_loss: 401.57   prob_loss: 1.19   total_loss: 404.51\n",
      "=> STEP 1072/6250   lr: 0.000957   giou_loss: 1.68   conf_loss: 401.23   prob_loss: 1.19   total_loss: 404.10\n",
      "=> STEP 1073/6250   lr: 0.000957   giou_loss: 1.78   conf_loss: 400.90   prob_loss: 1.19   total_loss: 403.87\n",
      "=> STEP 1074/6250   lr: 0.000956   giou_loss: 1.68   conf_loss: 400.57   prob_loss: 1.18   total_loss: 403.43\n",
      "=> STEP 1075/6250   lr: 0.000956   giou_loss: 1.69   conf_loss: 400.22   prob_loss: 1.18   total_loss: 403.10\n",
      "=> STEP 1076/6250   lr: 0.000956   giou_loss: 1.66   conf_loss: 399.89   prob_loss: 1.18   total_loss: 402.73\n",
      "=> STEP 1077/6250   lr: 0.000956   giou_loss: 1.56   conf_loss: 399.53   prob_loss: 1.19   total_loss: 402.29\n",
      "=> STEP 1078/6250   lr: 0.000956   giou_loss: 1.69   conf_loss: 399.20   prob_loss: 1.19   total_loss: 402.08\n",
      "=> STEP 1079/6250   lr: 0.000956   giou_loss: 1.67   conf_loss: 398.87   prob_loss: 1.19   total_loss: 401.72\n",
      "=> STEP 1080/6250   lr: 0.000956   giou_loss: 1.56   conf_loss: 398.53   prob_loss: 1.18   total_loss: 401.27\n",
      "=> STEP 1081/6250   lr: 0.000956   giou_loss: 1.65   conf_loss: 398.20   prob_loss: 1.17   total_loss: 401.03\n",
      "=> STEP 1082/6250   lr: 0.000956   giou_loss: 1.62   conf_loss: 397.86   prob_loss: 1.17   total_loss: 400.66\n",
      "=> STEP 1083/6250   lr: 0.000956   giou_loss: 1.57   conf_loss: 397.51   prob_loss: 1.19   total_loss: 400.27\n",
      "=> STEP 1084/6250   lr: 0.000955   giou_loss: 1.56   conf_loss: 397.18   prob_loss: 1.19   total_loss: 399.93\n",
      "=> STEP 1085/6250   lr: 0.000955   giou_loss: 1.56   conf_loss: 396.84   prob_loss: 1.19   total_loss: 399.59\n",
      "=> STEP 1086/6250   lr: 0.000955   giou_loss: 1.61   conf_loss: 396.52   prob_loss: 1.17   total_loss: 399.30\n",
      "=> STEP 1087/6250   lr: 0.000955   giou_loss: 1.60   conf_loss: 396.18   prob_loss: 1.17   total_loss: 398.95\n",
      "=> STEP 1088/6250   lr: 0.000955   giou_loss: 1.59   conf_loss: 395.85   prob_loss: 1.18   total_loss: 398.61\n",
      "=> STEP 1089/6250   lr: 0.000955   giou_loss: 1.56   conf_loss: 395.52   prob_loss: 1.18   total_loss: 398.26\n",
      "=> STEP 1090/6250   lr: 0.000955   giou_loss: 1.56   conf_loss: 395.18   prob_loss: 1.18   total_loss: 397.92\n",
      "=> STEP 1091/6250   lr: 0.000955   giou_loss: 1.56   conf_loss: 394.85   prob_loss: 1.18   total_loss: 397.59\n",
      "=> STEP 1092/6250   lr: 0.000955   giou_loss: 1.56   conf_loss: 394.53   prob_loss: 1.17   total_loss: 397.26\n",
      "=> STEP 1093/6250   lr: 0.000954   giou_loss: 1.57   conf_loss: 394.20   prob_loss: 1.17   total_loss: 396.94\n",
      "=> STEP 1094/6250   lr: 0.000954   giou_loss: 1.67   conf_loss: 393.90   prob_loss: 1.17   total_loss: 396.73\n",
      "=> STEP 1095/6250   lr: 0.000954   giou_loss: 1.64   conf_loss: 393.55   prob_loss: 1.17   total_loss: 396.36\n",
      "=> STEP 1096/6250   lr: 0.000954   giou_loss: 1.63   conf_loss: 393.21   prob_loss: 1.18   total_loss: 396.01\n",
      "=> STEP 1097/6250   lr: 0.000954   giou_loss: 1.62   conf_loss: 392.89   prob_loss: 1.17   total_loss: 395.68\n",
      "=> STEP 1098/6250   lr: 0.000954   giou_loss: 1.63   conf_loss: 392.58   prob_loss: 1.16   total_loss: 395.37\n",
      "=> STEP 1099/6250   lr: 0.000954   giou_loss: 1.59   conf_loss: 392.26   prob_loss: 1.16   total_loss: 395.01\n",
      "=> STEP 1100/6250   lr: 0.000954   giou_loss: 1.72   conf_loss: 391.92   prob_loss: 1.17   total_loss: 394.81\n",
      "=> STEP 1101/6250   lr: 0.000954   giou_loss: 1.75   conf_loss: 391.60   prob_loss: 1.18   total_loss: 394.52\n",
      "=> STEP 1102/6250   lr: 0.000954   giou_loss: 1.62   conf_loss: 391.30   prob_loss: 1.16   total_loss: 394.08\n",
      "=> STEP 1103/6250   lr: 0.000953   giou_loss: 1.62   conf_loss: 390.98   prob_loss: 1.14   total_loss: 393.75\n",
      "=> STEP 1104/6250   lr: 0.000953   giou_loss: 1.68   conf_loss: 390.68   prob_loss: 1.14   total_loss: 393.51\n",
      "=> STEP 1105/6250   lr: 0.000953   giou_loss: 1.69   conf_loss: 390.32   prob_loss: 1.15   total_loss: 393.16\n",
      "=> STEP 1106/6250   lr: 0.000953   giou_loss: 1.57   conf_loss: 390.00   prob_loss: 1.16   total_loss: 392.73\n",
      "=> STEP 1107/6250   lr: 0.000953   giou_loss: 1.76   conf_loss: 389.66   prob_loss: 1.18   total_loss: 392.60\n",
      "=> STEP 1108/6250   lr: 0.000953   giou_loss: 1.71   conf_loss: 389.34   prob_loss: 1.17   total_loss: 392.23\n",
      "=> STEP 1109/6250   lr: 0.000953   giou_loss: 1.60   conf_loss: 389.04   prob_loss: 1.16   total_loss: 391.80\n",
      "=> STEP 1110/6250   lr: 0.000953   giou_loss: 1.64   conf_loss: 388.71   prob_loss: 1.15   total_loss: 391.51\n",
      "=> STEP 1111/6250   lr: 0.000953   giou_loss: 1.57   conf_loss: 388.39   prob_loss: 1.16   total_loss: 391.11\n",
      "=> STEP 1112/6250   lr: 0.000952   giou_loss: 1.57   conf_loss: 388.06   prob_loss: 1.16   total_loss: 390.79\n",
      "=> STEP 1113/6250   lr: 0.000952   giou_loss: 1.63   conf_loss: 387.72   prob_loss: 1.17   total_loss: 390.52\n",
      "=> STEP 1114/6250   lr: 0.000952   giou_loss: 1.58   conf_loss: 387.40   prob_loss: 1.17   total_loss: 390.15\n",
      "=> STEP 1115/6250   lr: 0.000952   giou_loss: 1.71   conf_loss: 387.10   prob_loss: 1.15   total_loss: 389.97\n",
      "=> STEP 1116/6250   lr: 0.000952   giou_loss: 1.80   conf_loss: 386.77   prob_loss: 1.15   total_loss: 389.72\n",
      "=> STEP 1117/6250   lr: 0.000952   giou_loss: 1.62   conf_loss: 386.47   prob_loss: 1.16   total_loss: 389.25\n",
      "=> STEP 1118/6250   lr: 0.000952   giou_loss: 1.61   conf_loss: 386.12   prob_loss: 1.17   total_loss: 388.91\n",
      "=> STEP 1119/6250   lr: 0.000952   giou_loss: 1.58   conf_loss: 385.82   prob_loss: 1.17   total_loss: 388.56\n",
      "=> STEP 1120/6250   lr: 0.000952   giou_loss: 1.58   conf_loss: 385.50   prob_loss: 1.16   total_loss: 388.24\n",
      "=> STEP 1121/6250   lr: 0.000951   giou_loss: 1.59   conf_loss: 385.21   prob_loss: 1.15   total_loss: 387.96\n",
      "=> STEP 1122/6250   lr: 0.000951   giou_loss: 1.61   conf_loss: 384.85   prob_loss: 1.16   total_loss: 387.62\n",
      "=> STEP 1123/6250   lr: 0.000951   giou_loss: 1.56   conf_loss: 384.56   prob_loss: 1.16   total_loss: 387.28\n",
      "=> STEP 1124/6250   lr: 0.000951   giou_loss: 1.56   conf_loss: 384.21   prob_loss: 1.16   total_loss: 386.93\n",
      "=> STEP 1125/6250   lr: 0.000951   giou_loss: 1.57   conf_loss: 383.90   prob_loss: 1.15   total_loss: 386.62\n",
      "=> STEP 1126/6250   lr: 0.000951   giou_loss: 1.64   conf_loss: 383.57   prob_loss: 1.16   total_loss: 386.37\n",
      "=> STEP 1127/6250   lr: 0.000951   giou_loss: 1.61   conf_loss: 383.25   prob_loss: 1.15   total_loss: 386.02\n",
      "=> STEP 1128/6250   lr: 0.000951   giou_loss: 1.66   conf_loss: 382.94   prob_loss: 1.15   total_loss: 385.74\n",
      "=> STEP 1129/6250   lr: 0.000951   giou_loss: 1.74   conf_loss: 382.63   prob_loss: 1.15   total_loss: 385.51\n",
      "=> STEP 1130/6250   lr: 0.000950   giou_loss: 1.66   conf_loss: 382.29   prob_loss: 1.15   total_loss: 385.11\n",
      "=> STEP 1131/6250   lr: 0.000950   giou_loss: 1.57   conf_loss: 381.97   prob_loss: 1.16   total_loss: 384.70\n",
      "=> STEP 1132/6250   lr: 0.000950   giou_loss: 1.58   conf_loss: 381.66   prob_loss: 1.16   total_loss: 384.40\n",
      "=> STEP 1133/6250   lr: 0.000950   giou_loss: 1.59   conf_loss: 381.35   prob_loss: 1.15   total_loss: 384.09\n",
      "=> STEP 1134/6250   lr: 0.000950   giou_loss: 1.57   conf_loss: 381.04   prob_loss: 1.15   total_loss: 383.75\n",
      "=> STEP 1135/6250   lr: 0.000950   giou_loss: 1.57   conf_loss: 380.71   prob_loss: 1.15   total_loss: 383.43\n",
      "=> STEP 1136/6250   lr: 0.000950   giou_loss: 1.56   conf_loss: 380.40   prob_loss: 1.15   total_loss: 383.11\n",
      "=> STEP 1137/6250   lr: 0.000950   giou_loss: 1.57   conf_loss: 380.08   prob_loss: 1.15   total_loss: 382.80\n",
      "=> STEP 1138/6250   lr: 0.000950   giou_loss: 1.68   conf_loss: 379.76   prob_loss: 1.16   total_loss: 382.60\n",
      "=> STEP 1139/6250   lr: 0.000949   giou_loss: 1.67   conf_loss: 379.45   prob_loss: 1.15   total_loss: 382.27\n",
      "=> STEP 1140/6250   lr: 0.000949   giou_loss: 1.57   conf_loss: 379.15   prob_loss: 1.14   total_loss: 381.86\n",
      "=> STEP 1141/6250   lr: 0.000949   giou_loss: 1.58   conf_loss: 378.84   prob_loss: 1.15   total_loss: 381.57\n",
      "=> STEP 1142/6250   lr: 0.000949   giou_loss: 1.61   conf_loss: 378.52   prob_loss: 1.14   total_loss: 381.27\n",
      "=> STEP 1143/6250   lr: 0.000949   giou_loss: 1.64   conf_loss: 378.22   prob_loss: 1.14   total_loss: 381.00\n",
      "=> STEP 1144/6250   lr: 0.000949   giou_loss: 1.58   conf_loss: 377.90   prob_loss: 1.15   total_loss: 380.63\n",
      "=> STEP 1145/6250   lr: 0.000949   giou_loss: 1.71   conf_loss: 377.57   prob_loss: 1.16   total_loss: 380.44\n",
      "=> STEP 1146/6250   lr: 0.000949   giou_loss: 1.74   conf_loss: 377.27   prob_loss: 1.15   total_loss: 380.16\n",
      "=> STEP 1147/6250   lr: 0.000949   giou_loss: 1.61   conf_loss: 376.96   prob_loss: 1.15   total_loss: 379.72\n",
      "=> STEP 1148/6250   lr: 0.000948   giou_loss: 1.78   conf_loss: 376.68   prob_loss: 1.14   total_loss: 379.60\n",
      "=> STEP 1149/6250   lr: 0.000948   giou_loss: 1.80   conf_loss: 376.38   prob_loss: 1.13   total_loss: 379.32\n",
      "=> STEP 1150/6250   lr: 0.000948   giou_loss: 1.77   conf_loss: 376.06   prob_loss: 1.13   total_loss: 378.96\n",
      "=> STEP 1151/6250   lr: 0.000948   giou_loss: 1.63   conf_loss: 375.75   prob_loss: 1.15   total_loss: 378.53\n",
      "=> STEP 1152/6250   lr: 0.000948   giou_loss: 1.68   conf_loss: 375.43   prob_loss: 1.16   total_loss: 378.26\n",
      "=> STEP 1153/6250   lr: 0.000948   giou_loss: 1.67   conf_loss: 375.15   prob_loss: 1.15   total_loss: 377.96\n",
      "=> STEP 1154/6250   lr: 0.000948   giou_loss: 1.56   conf_loss: 374.82   prob_loss: 1.14   total_loss: 377.53\n",
      "=> STEP 1155/6250   lr: 0.000948   giou_loss: 1.68   conf_loss: 374.54   prob_loss: 1.14   total_loss: 377.35\n",
      "=> STEP 1156/6250   lr: 0.000948   giou_loss: 1.56   conf_loss: 374.21   prob_loss: 1.13   total_loss: 376.91\n",
      "=> STEP 1157/6250   lr: 0.000947   giou_loss: 1.67   conf_loss: 373.93   prob_loss: 1.14   total_loss: 376.74\n",
      "=> STEP 1158/6250   lr: 0.000947   giou_loss: 1.59   conf_loss: 373.59   prob_loss: 1.14   total_loss: 376.33\n",
      "=> STEP 1159/6250   lr: 0.000947   giou_loss: 1.76   conf_loss: 373.29   prob_loss: 1.15   total_loss: 376.20\n",
      "=> STEP 1160/6250   lr: 0.000947   giou_loss: 1.80   conf_loss: 372.97   prob_loss: 1.15   total_loss: 375.92\n",
      "=> STEP 1161/6250   lr: 0.000947   giou_loss: 1.74   conf_loss: 372.68   prob_loss: 1.15   total_loss: 375.56\n",
      "=> STEP 1162/6250   lr: 0.000947   giou_loss: 1.67   conf_loss: 372.38   prob_loss: 1.14   total_loss: 375.19\n",
      "=> STEP 1163/6250   lr: 0.000947   giou_loss: 1.62   conf_loss: 372.08   prob_loss: 1.13   total_loss: 374.84\n",
      "=> STEP 1164/6250   lr: 0.000947   giou_loss: 1.74   conf_loss: 371.78   prob_loss: 1.13   total_loss: 374.65\n",
      "=> STEP 1165/6250   lr: 0.000947   giou_loss: 1.68   conf_loss: 371.47   prob_loss: 1.13   total_loss: 374.28\n",
      "=> STEP 1166/6250   lr: 0.000946   giou_loss: 1.68   conf_loss: 371.18   prob_loss: 1.13   total_loss: 373.99\n",
      "=> STEP 1167/6250   lr: 0.000946   giou_loss: 1.81   conf_loss: 370.87   prob_loss: 1.14   total_loss: 373.81\n",
      "=> STEP 1168/6250   lr: 0.000946   giou_loss: 1.68   conf_loss: 370.56   prob_loss: 1.14   total_loss: 373.38\n",
      "=> STEP 1169/6250   lr: 0.000946   giou_loss: 1.69   conf_loss: 370.26   prob_loss: 1.14   total_loss: 373.09\n",
      "=> STEP 1170/6250   lr: 0.000946   giou_loss: 1.74   conf_loss: 369.97   prob_loss: 1.13   total_loss: 372.84\n",
      "=> STEP 1171/6250   lr: 0.000946   giou_loss: 1.60   conf_loss: 369.69   prob_loss: 1.13   total_loss: 372.42\n",
      "=> STEP 1172/6250   lr: 0.000946   giou_loss: 1.62   conf_loss: 369.39   prob_loss: 1.14   total_loss: 372.14\n",
      "=> STEP 1173/6250   lr: 0.000946   giou_loss: 1.67   conf_loss: 369.08   prob_loss: 1.14   total_loss: 371.90\n",
      "=> STEP 1174/6250   lr: 0.000945   giou_loss: 1.71   conf_loss: 368.78   prob_loss: 1.13   total_loss: 371.62\n",
      "=> STEP 1175/6250   lr: 0.000945   giou_loss: 1.57   conf_loss: 368.51   prob_loss: 1.13   total_loss: 371.20\n",
      "=> STEP 1176/6250   lr: 0.000945   giou_loss: 1.67   conf_loss: 368.20   prob_loss: 1.12   total_loss: 370.99\n",
      "=> STEP 1177/6250   lr: 0.000945   giou_loss: 1.61   conf_loss: 367.91   prob_loss: 1.13   total_loss: 370.65\n",
      "=> STEP 1178/6250   lr: 0.000945   giou_loss: 1.68   conf_loss: 367.58   prob_loss: 1.14   total_loss: 370.40\n",
      "=> STEP 1179/6250   lr: 0.000945   giou_loss: 1.71   conf_loss: 367.28   prob_loss: 1.14   total_loss: 370.14\n",
      "=> STEP 1180/6250   lr: 0.000945   giou_loss: 1.58   conf_loss: 366.99   prob_loss: 1.13   total_loss: 369.71\n",
      "=> STEP 1181/6250   lr: 0.000945   giou_loss: 1.76   conf_loss: 366.72   prob_loss: 1.12   total_loss: 369.60\n",
      "=> STEP 1182/6250   lr: 0.000945   giou_loss: 1.84   conf_loss: 366.44   prob_loss: 1.12   total_loss: 369.40\n",
      "=> STEP 1183/6250   lr: 0.000944   giou_loss: 1.76   conf_loss: 366.12   prob_loss: 1.12   total_loss: 369.01\n",
      "=> STEP 1184/6250   lr: 0.000944   giou_loss: 1.68   conf_loss: 365.82   prob_loss: 1.13   total_loss: 368.63\n",
      "=> STEP 1185/6250   lr: 0.000944   giou_loss: 1.78   conf_loss: 365.51   prob_loss: 1.14   total_loss: 368.43\n",
      "=> STEP 1186/6250   lr: 0.000944   giou_loss: 1.74   conf_loss: 365.21   prob_loss: 1.14   total_loss: 368.08\n",
      "=> STEP 1187/6250   lr: 0.000944   giou_loss: 1.62   conf_loss: 364.94   prob_loss: 1.12   total_loss: 367.68\n",
      "=> STEP 1188/6250   lr: 0.000944   giou_loss: 1.75   conf_loss: 364.68   prob_loss: 1.11   total_loss: 367.53\n",
      "=> STEP 1189/6250   lr: 0.000944   giou_loss: 1.88   conf_loss: 364.39   prob_loss: 1.10   total_loss: 367.37\n",
      "=> STEP 1190/6250   lr: 0.000944   giou_loss: 1.77   conf_loss: 364.08   prob_loss: 1.11   total_loss: 366.96\n",
      "=> STEP 1191/6250   lr: 0.000944   giou_loss: 1.56   conf_loss: 363.77   prob_loss: 1.14   total_loss: 366.47\n",
      "=> STEP 1192/6250   lr: 0.000943   giou_loss: 1.83   conf_loss: 363.45   prob_loss: 1.15   total_loss: 366.43\n",
      "=> STEP 1193/6250   lr: 0.000943   giou_loss: 1.84   conf_loss: 363.16   prob_loss: 1.14   total_loss: 366.14\n",
      "=> STEP 1194/6250   lr: 0.000943   giou_loss: 1.62   conf_loss: 362.89   prob_loss: 1.12   total_loss: 365.64\n",
      "=> STEP 1195/6250   lr: 0.000943   giou_loss: 1.71   conf_loss: 362.61   prob_loss: 1.11   total_loss: 365.43\n",
      "=> STEP 1196/6250   lr: 0.000943   giou_loss: 1.82   conf_loss: 362.32   prob_loss: 1.11   total_loss: 365.25\n",
      "=> STEP 1197/6250   lr: 0.000943   giou_loss: 1.68   conf_loss: 362.02   prob_loss: 1.12   total_loss: 364.82\n",
      "=> STEP 1198/6250   lr: 0.000943   giou_loss: 1.66   conf_loss: 361.70   prob_loss: 1.14   total_loss: 364.50\n",
      "=> STEP 1199/6250   lr: 0.000943   giou_loss: 1.70   conf_loss: 361.42   prob_loss: 1.13   total_loss: 364.26\n",
      "=> STEP 1200/6250   lr: 0.000942   giou_loss: 1.57   conf_loss: 361.13   prob_loss: 1.12   total_loss: 363.83\n",
      "=> STEP 1201/6250   lr: 0.000942   giou_loss: 1.77   conf_loss: 360.87   prob_loss: 1.11   total_loss: 363.76\n",
      "=> STEP 1202/6250   lr: 0.000942   giou_loss: 1.82   conf_loss: 360.58   prob_loss: 1.11   total_loss: 363.51\n",
      "=> STEP 1203/6250   lr: 0.000942   giou_loss: 1.73   conf_loss: 360.29   prob_loss: 1.12   total_loss: 363.14\n",
      "=> STEP 1204/6250   lr: 0.000942   giou_loss: 1.56   conf_loss: 359.97   prob_loss: 1.13   total_loss: 362.66\n",
      "=> STEP 1205/6250   lr: 0.000942   giou_loss: 1.77   conf_loss: 359.68   prob_loss: 1.13   total_loss: 362.58\n",
      "=> STEP 1206/6250   lr: 0.000942   giou_loss: 1.80   conf_loss: 359.39   prob_loss: 1.12   total_loss: 362.31\n",
      "=> STEP 1207/6250   lr: 0.000942   giou_loss: 1.75   conf_loss: 359.11   prob_loss: 1.11   total_loss: 361.98\n",
      "=> STEP 1208/6250   lr: 0.000941   giou_loss: 1.65   conf_loss: 358.83   prob_loss: 1.12   total_loss: 361.60\n",
      "=> STEP 1209/6250   lr: 0.000941   giou_loss: 1.70   conf_loss: 358.53   prob_loss: 1.12   total_loss: 361.36\n",
      "=> STEP 1210/6250   lr: 0.000941   giou_loss: 1.60   conf_loss: 358.23   prob_loss: 1.12   total_loss: 360.96\n",
      "=> STEP 1211/6250   lr: 0.000941   giou_loss: 1.77   conf_loss: 357.95   prob_loss: 1.12   total_loss: 360.84\n",
      "=> STEP 1212/6250   lr: 0.000941   giou_loss: 1.84   conf_loss: 357.66   prob_loss: 1.12   total_loss: 360.63\n",
      "=> STEP 1213/6250   lr: 0.000941   giou_loss: 1.79   conf_loss: 357.35   prob_loss: 1.12   total_loss: 360.26\n",
      "=> STEP 1214/6250   lr: 0.000941   giou_loss: 1.58   conf_loss: 357.09   prob_loss: 1.12   total_loss: 359.79\n",
      "=> STEP 1215/6250   lr: 0.000941   giou_loss: 1.85   conf_loss: 356.80   prob_loss: 1.12   total_loss: 359.77\n",
      "=> STEP 1216/6250   lr: 0.000941   giou_loss: 1.95   conf_loss: 356.53   prob_loss: 1.11   total_loss: 359.59\n",
      "=> STEP 1217/6250   lr: 0.000940   giou_loss: 1.95   conf_loss: 356.23   prob_loss: 1.11   total_loss: 359.29\n",
      "=> STEP 1218/6250   lr: 0.000940   giou_loss: 1.80   conf_loss: 355.94   prob_loss: 1.11   total_loss: 358.85\n",
      "=> STEP 1219/6250   lr: 0.000940   giou_loss: 1.56   conf_loss: 355.64   prob_loss: 1.13   total_loss: 358.33\n",
      "=> STEP 1220/6250   lr: 0.000940   giou_loss: 1.75   conf_loss: 355.36   prob_loss: 1.13   total_loss: 358.25\n",
      "=> STEP 1221/6250   lr: 0.000940   giou_loss: 1.81   conf_loss: 355.08   prob_loss: 1.13   total_loss: 358.02\n",
      "=> STEP 1222/6250   lr: 0.000940   giou_loss: 1.68   conf_loss: 354.81   prob_loss: 1.13   total_loss: 357.62\n",
      "=> STEP 1223/6250   lr: 0.000940   giou_loss: 1.75   conf_loss: 354.54   prob_loss: 1.14   total_loss: 357.42\n",
      "=> STEP 1224/6250   lr: 0.000940   giou_loss: 1.73   conf_loss: 354.28   prob_loss: 1.13   total_loss: 357.14\n",
      "=> STEP 1225/6250   lr: 0.000939   giou_loss: 1.59   conf_loss: 353.98   prob_loss: 1.13   total_loss: 356.70\n",
      "=> STEP 1226/6250   lr: 0.000939   giou_loss: 1.77   conf_loss: 353.68   prob_loss: 1.12   total_loss: 356.58\n",
      "=> STEP 1227/6250   lr: 0.000939   giou_loss: 1.88   conf_loss: 353.39   prob_loss: 1.12   total_loss: 356.38\n",
      "=> STEP 1228/6250   lr: 0.000939   giou_loss: 1.88   conf_loss: 353.11   prob_loss: 1.12   total_loss: 356.10\n",
      "=> STEP 1229/6250   lr: 0.000939   giou_loss: 1.67   conf_loss: 352.81   prob_loss: 1.12   total_loss: 355.60\n",
      "=> STEP 1230/6250   lr: 0.000939   giou_loss: 1.71   conf_loss: 352.55   prob_loss: 1.12   total_loss: 355.38\n",
      "=> STEP 1231/6250   lr: 0.000939   giou_loss: 1.80   conf_loss: 352.27   prob_loss: 1.11   total_loss: 355.18\n",
      "=> STEP 1232/6250   lr: 0.000939   giou_loss: 1.78   conf_loss: 352.00   prob_loss: 1.11   total_loss: 354.89\n",
      "=> STEP 1233/6250   lr: 0.000938   giou_loss: 1.57   conf_loss: 351.70   prob_loss: 1.11   total_loss: 354.38\n",
      "=> STEP 1234/6250   lr: 0.000938   giou_loss: 1.73   conf_loss: 351.41   prob_loss: 1.12   total_loss: 354.26\n",
      "=> STEP 1235/6250   lr: 0.000938   giou_loss: 1.73   conf_loss: 351.14   prob_loss: 1.12   total_loss: 354.00\n",
      "=> STEP 1236/6250   lr: 0.000938   giou_loss: 1.56   conf_loss: 350.86   prob_loss: 1.11   total_loss: 353.54\n",
      "=> STEP 1237/6250   lr: 0.000938   giou_loss: 1.69   conf_loss: 350.60   prob_loss: 1.10   total_loss: 353.39\n",
      "=> STEP 1238/6250   lr: 0.000938   giou_loss: 1.67   conf_loss: 350.30   prob_loss: 1.10   total_loss: 353.07\n",
      "=> STEP 1239/6250   lr: 0.000938   giou_loss: 1.68   conf_loss: 350.04   prob_loss: 1.11   total_loss: 352.82\n",
      "=> STEP 1240/6250   lr: 0.000938   giou_loss: 1.71   conf_loss: 349.74   prob_loss: 1.11   total_loss: 352.56\n",
      "=> STEP 1241/6250   lr: 0.000937   giou_loss: 1.65   conf_loss: 349.47   prob_loss: 1.11   total_loss: 352.22\n",
      "=> STEP 1242/6250   lr: 0.000937   giou_loss: 1.61   conf_loss: 349.21   prob_loss: 1.10   total_loss: 351.92\n",
      "=> STEP 1243/6250   lr: 0.000937   giou_loss: 1.57   conf_loss: 348.92   prob_loss: 1.09   total_loss: 351.58\n",
      "=> STEP 1244/6250   lr: 0.000937   giou_loss: 1.67   conf_loss: 348.64   prob_loss: 1.10   total_loss: 351.40\n",
      "=> STEP 1245/6250   lr: 0.000937   giou_loss: 1.56   conf_loss: 348.36   prob_loss: 1.10   total_loss: 351.02\n",
      "=> STEP 1246/6250   lr: 0.000937   giou_loss: 1.69   conf_loss: 348.07   prob_loss: 1.10   total_loss: 350.87\n",
      "=> STEP 1247/6250   lr: 0.000937   giou_loss: 1.56   conf_loss: 347.79   prob_loss: 1.10   total_loss: 350.46\n",
      "=> STEP 1248/6250   lr: 0.000937   giou_loss: 1.59   conf_loss: 347.51   prob_loss: 1.10   total_loss: 350.20\n",
      "=> STEP 1249/6250   lr: 0.000936   giou_loss: 1.61   conf_loss: 347.24   prob_loss: 1.09   total_loss: 349.94\n",
      "=> STEP 1250/6250   lr: 0.000936   giou_loss: 1.56   conf_loss: 346.96   prob_loss: 1.10   total_loss: 349.63\n",
      "=> STEP 1251/6250   lr: 0.000936   giou_loss: 1.66   conf_loss: 346.67   prob_loss: 1.11   total_loss: 349.44\n",
      "=> STEP 1252/6250   lr: 0.000936   giou_loss: 1.56   conf_loss: 346.41   prob_loss: 1.10   total_loss: 349.07\n",
      "=> STEP 1253/6250   lr: 0.000936   giou_loss: 1.63   conf_loss: 346.14   prob_loss: 1.09   total_loss: 348.86\n",
      "=> STEP 1254/6250   lr: 0.000936   giou_loss: 1.57   conf_loss: 345.87   prob_loss: 1.09   total_loss: 348.52\n",
      "=> STEP 1255/6250   lr: 0.000936   giou_loss: 1.66   conf_loss: 345.58   prob_loss: 1.09   total_loss: 348.33\n",
      "=> STEP 1256/6250   lr: 0.000936   giou_loss: 1.56   conf_loss: 345.30   prob_loss: 1.10   total_loss: 347.96\n",
      "=> STEP 1257/6250   lr: 0.000935   giou_loss: 1.73   conf_loss: 345.03   prob_loss: 1.10   total_loss: 347.86\n",
      "=> STEP 1258/6250   lr: 0.000935   giou_loss: 1.58   conf_loss: 344.77   prob_loss: 1.09   total_loss: 347.44\n",
      "=> STEP 1259/6250   lr: 0.000935   giou_loss: 1.73   conf_loss: 344.52   prob_loss: 1.08   total_loss: 347.33\n",
      "=> STEP 1260/6250   lr: 0.000935   giou_loss: 1.68   conf_loss: 344.24   prob_loss: 1.08   total_loss: 347.00\n",
      "=> STEP 1261/6250   lr: 0.000935   giou_loss: 1.58   conf_loss: 344.00   prob_loss: 1.08   total_loss: 346.66\n",
      "=> STEP 1262/6250   lr: 0.000935   giou_loss: 1.58   conf_loss: 343.74   prob_loss: 1.08   total_loss: 346.41\n",
      "=> STEP 1263/6250   lr: 0.000935   giou_loss: 1.62   conf_loss: 343.49   prob_loss: 1.09   total_loss: 346.20\n",
      "=> STEP 1264/6250   lr: 0.000935   giou_loss: 1.65   conf_loss: 343.21   prob_loss: 1.08   total_loss: 345.94\n",
      "=> STEP 1265/6250   lr: 0.000934   giou_loss: 1.58   conf_loss: 342.97   prob_loss: 1.08   total_loss: 345.63\n",
      "=> STEP 1266/6250   lr: 0.000934   giou_loss: 1.61   conf_loss: 342.68   prob_loss: 1.07   total_loss: 345.36\n",
      "=> STEP 1267/6250   lr: 0.000934   giou_loss: 1.60   conf_loss: 342.39   prob_loss: 1.07   total_loss: 345.07\n",
      "=> STEP 1268/6250   lr: 0.000934   giou_loss: 1.67   conf_loss: 342.14   prob_loss: 1.07   total_loss: 344.88\n",
      "=> STEP 1269/6250   lr: 0.000934   giou_loss: 1.63   conf_loss: 341.87   prob_loss: 1.07   total_loss: 344.57\n",
      "=> STEP 1270/6250   lr: 0.000934   giou_loss: 1.68   conf_loss: 341.60   prob_loss: 1.07   total_loss: 344.34\n",
      "=> STEP 1271/6250   lr: 0.000934   giou_loss: 1.65   conf_loss: 341.33   prob_loss: 1.06   total_loss: 344.05\n",
      "=> STEP 1272/6250   lr: 0.000934   giou_loss: 1.56   conf_loss: 341.06   prob_loss: 1.07   total_loss: 343.69\n",
      "=> STEP 1273/6250   lr: 0.000933   giou_loss: 1.64   conf_loss: 340.77   prob_loss: 1.08   total_loss: 343.49\n",
      "=> STEP 1274/6250   lr: 0.000933   giou_loss: 1.59   conf_loss: 340.50   prob_loss: 1.08   total_loss: 343.18\n",
      "=> STEP 1275/6250   lr: 0.000933   giou_loss: 1.57   conf_loss: 340.22   prob_loss: 1.08   total_loss: 342.87\n",
      "=> STEP 1276/6250   lr: 0.000933   giou_loss: 1.57   conf_loss: 339.97   prob_loss: 1.07   total_loss: 342.60\n",
      "=> STEP 1277/6250   lr: 0.000933   giou_loss: 1.63   conf_loss: 339.70   prob_loss: 1.07   total_loss: 342.39\n",
      "=> STEP 1278/6250   lr: 0.000933   giou_loss: 1.60   conf_loss: 339.40   prob_loss: 1.08   total_loss: 342.09\n",
      "=> STEP 1279/6250   lr: 0.000933   giou_loss: 1.65   conf_loss: 339.13   prob_loss: 1.09   total_loss: 341.87\n",
      "=> STEP 1280/6250   lr: 0.000933   giou_loss: 1.59   conf_loss: 338.87   prob_loss: 1.08   total_loss: 341.54\n",
      "=> STEP 1281/6250   lr: 0.000932   giou_loss: 1.64   conf_loss: 338.61   prob_loss: 1.07   total_loss: 341.32\n",
      "=> STEP 1282/6250   lr: 0.000932   giou_loss: 1.73   conf_loss: 338.35   prob_loss: 1.07   total_loss: 341.15\n",
      "=> STEP 1283/6250   lr: 0.000932   giou_loss: 1.59   conf_loss: 338.07   prob_loss: 1.08   total_loss: 340.74\n",
      "=> STEP 1284/6250   lr: 0.000932   giou_loss: 1.73   conf_loss: 337.82   prob_loss: 1.07   total_loss: 340.62\n",
      "=> STEP 1285/6250   lr: 0.000932   giou_loss: 1.69   conf_loss: 337.55   prob_loss: 1.08   total_loss: 340.31\n",
      "=> STEP 1286/6250   lr: 0.000932   giou_loss: 1.56   conf_loss: 337.27   prob_loss: 1.08   total_loss: 339.92\n",
      "=> STEP 1287/6250   lr: 0.000932   giou_loss: 1.68   conf_loss: 336.99   prob_loss: 1.08   total_loss: 339.75\n",
      "=> STEP 1288/6250   lr: 0.000932   giou_loss: 1.66   conf_loss: 336.75   prob_loss: 1.07   total_loss: 339.48\n",
      "=> STEP 1289/6250   lr: 0.000931   giou_loss: 1.66   conf_loss: 336.47   prob_loss: 1.07   total_loss: 339.20\n",
      "=> STEP 1290/6250   lr: 0.000931   giou_loss: 1.64   conf_loss: 336.21   prob_loss: 1.07   total_loss: 338.92\n",
      "=> STEP 1291/6250   lr: 0.000931   giou_loss: 1.59   conf_loss: 335.93   prob_loss: 1.08   total_loss: 338.60\n",
      "=> STEP 1292/6250   lr: 0.000931   giou_loss: 1.57   conf_loss: 335.67   prob_loss: 1.07   total_loss: 338.30\n",
      "=> STEP 1293/6250   lr: 0.000931   giou_loss: 1.69   conf_loss: 335.43   prob_loss: 1.06   total_loss: 338.18\n",
      "=> STEP 1294/6250   lr: 0.000931   giou_loss: 1.69   conf_loss: 335.16   prob_loss: 1.06   total_loss: 337.92\n",
      "=> STEP 1295/6250   lr: 0.000931   giou_loss: 1.58   conf_loss: 334.87   prob_loss: 1.07   total_loss: 337.52\n",
      "=> STEP 1296/6250   lr: 0.000931   giou_loss: 1.67   conf_loss: 334.61   prob_loss: 1.08   total_loss: 337.37\n",
      "=> STEP 1297/6250   lr: 0.000930   giou_loss: 1.62   conf_loss: 334.33   prob_loss: 1.07   total_loss: 337.02\n",
      "=> STEP 1298/6250   lr: 0.000930   giou_loss: 1.65   conf_loss: 334.04   prob_loss: 1.05   total_loss: 336.75\n",
      "=> STEP 1299/6250   lr: 0.000930   giou_loss: 1.66   conf_loss: 333.74   prob_loss: 1.05   total_loss: 336.45\n",
      "=> STEP 1300/6250   lr: 0.000930   giou_loss: 1.57   conf_loss: 333.43   prob_loss: 1.06   total_loss: 336.06\n",
      "=> STEP 1301/6250   lr: 0.000930   giou_loss: 1.56   conf_loss: 333.16   prob_loss: 1.06   total_loss: 335.78\n",
      "=> STEP 1302/6250   lr: 0.000930   giou_loss: 1.56   conf_loss: 332.87   prob_loss: 1.06   total_loss: 335.50\n",
      "=> STEP 1303/6250   lr: 0.000930   giou_loss: 1.57   conf_loss: 332.60   prob_loss: 1.06   total_loss: 335.23\n",
      "=> STEP 1304/6250   lr: 0.000929   giou_loss: 1.66   conf_loss: 332.32   prob_loss: 1.06   total_loss: 335.04\n",
      "=> STEP 1305/6250   lr: 0.000929   giou_loss: 1.63   conf_loss: 332.05   prob_loss: 1.06   total_loss: 334.75\n",
      "=> STEP 1306/6250   lr: 0.000929   giou_loss: 1.62   conf_loss: 331.79   prob_loss: 1.06   total_loss: 334.46\n",
      "=> STEP 1307/6250   lr: 0.000929   giou_loss: 1.61   conf_loss: 331.53   prob_loss: 1.06   total_loss: 334.19\n",
      "=> STEP 1308/6250   lr: 0.000929   giou_loss: 1.64   conf_loss: 331.24   prob_loss: 1.07   total_loss: 333.95\n",
      "=> STEP 1309/6250   lr: 0.000929   giou_loss: 1.63   conf_loss: 330.98   prob_loss: 1.07   total_loss: 333.68\n",
      "=> STEP 1310/6250   lr: 0.000929   giou_loss: 1.64   conf_loss: 330.71   prob_loss: 1.06   total_loss: 333.42\n",
      "=> STEP 1311/6250   lr: 0.000929   giou_loss: 1.57   conf_loss: 330.46   prob_loss: 1.06   total_loss: 333.09\n",
      "=> STEP 1312/6250   lr: 0.000928   giou_loss: 1.70   conf_loss: 330.19   prob_loss: 1.06   total_loss: 332.95\n",
      "=> STEP 1313/6250   lr: 0.000928   giou_loss: 1.65   conf_loss: 329.92   prob_loss: 1.06   total_loss: 332.63\n",
      "=> STEP 1314/6250   lr: 0.000928   giou_loss: 1.57   conf_loss: 329.67   prob_loss: 1.07   total_loss: 332.32\n",
      "=> STEP 1315/6250   lr: 0.000928   giou_loss: 1.57   conf_loss: 329.40   prob_loss: 1.07   total_loss: 332.04\n",
      "=> STEP 1316/6250   lr: 0.000928   giou_loss: 1.67   conf_loss: 329.17   prob_loss: 1.06   total_loss: 331.89\n",
      "=> STEP 1317/6250   lr: 0.000928   giou_loss: 1.62   conf_loss: 328.92   prob_loss: 1.05   total_loss: 331.58\n",
      "=> STEP 1318/6250   lr: 0.000928   giou_loss: 1.70   conf_loss: 328.65   prob_loss: 1.06   total_loss: 331.40\n",
      "=> STEP 1319/6250   lr: 0.000928   giou_loss: 1.71   conf_loss: 328.39   prob_loss: 1.06   total_loss: 331.17\n",
      "=> STEP 1320/6250   lr: 0.000927   giou_loss: 1.60   conf_loss: 328.15   prob_loss: 1.06   total_loss: 330.82\n",
      "=> STEP 1321/6250   lr: 0.000927   giou_loss: 1.62   conf_loss: 327.88   prob_loss: 1.05   total_loss: 330.56\n",
      "=> STEP 1322/6250   lr: 0.000927   giou_loss: 1.71   conf_loss: 327.66   prob_loss: 1.05   total_loss: 330.42\n",
      "=> STEP 1323/6250   lr: 0.000927   giou_loss: 1.72   conf_loss: 327.37   prob_loss: 1.06   total_loss: 330.15\n",
      "=> STEP 1324/6250   lr: 0.000927   giou_loss: 1.58   conf_loss: 327.14   prob_loss: 1.06   total_loss: 329.78\n",
      "=> STEP 1325/6250   lr: 0.000927   giou_loss: 1.66   conf_loss: 326.86   prob_loss: 1.07   total_loss: 329.60\n",
      "=> STEP 1326/6250   lr: 0.000927   giou_loss: 1.69   conf_loss: 326.60   prob_loss: 1.07   total_loss: 329.36\n",
      "=> STEP 1327/6250   lr: 0.000926   giou_loss: 1.57   conf_loss: 326.38   prob_loss: 1.05   total_loss: 329.00\n",
      "=> STEP 1328/6250   lr: 0.000926   giou_loss: 1.58   conf_loss: 326.12   prob_loss: 1.04   total_loss: 328.75\n",
      "=> STEP 1329/6250   lr: 0.000926   giou_loss: 1.59   conf_loss: 325.87   prob_loss: 1.05   total_loss: 328.51\n",
      "=> STEP 1330/6250   lr: 0.000926   giou_loss: 1.56   conf_loss: 325.61   prob_loss: 1.05   total_loss: 328.23\n",
      "=> STEP 1331/6250   lr: 0.000926   giou_loss: 1.61   conf_loss: 325.35   prob_loss: 1.05   total_loss: 328.02\n",
      "=> STEP 1332/6250   lr: 0.000926   giou_loss: 1.58   conf_loss: 325.10   prob_loss: 1.06   total_loss: 327.73\n",
      "=> STEP 1333/6250   lr: 0.000926   giou_loss: 1.56   conf_loss: 324.84   prob_loss: 1.05   total_loss: 327.46\n",
      "=> STEP 1334/6250   lr: 0.000926   giou_loss: 1.62   conf_loss: 324.60   prob_loss: 1.05   total_loss: 327.27\n",
      "=> STEP 1335/6250   lr: 0.000925   giou_loss: 1.57   conf_loss: 324.33   prob_loss: 1.06   total_loss: 326.96\n",
      "=> STEP 1336/6250   lr: 0.000925   giou_loss: 1.73   conf_loss: 324.08   prob_loss: 1.06   total_loss: 326.86\n",
      "=> STEP 1337/6250   lr: 0.000925   giou_loss: 1.64   conf_loss: 323.83   prob_loss: 1.05   total_loss: 326.52\n",
      "=> STEP 1338/6250   lr: 0.000925   giou_loss: 1.75   conf_loss: 323.59   prob_loss: 1.05   total_loss: 326.39\n",
      "=> STEP 1339/6250   lr: 0.000925   giou_loss: 1.78   conf_loss: 323.34   prob_loss: 1.05   total_loss: 326.17\n",
      "=> STEP 1340/6250   lr: 0.000925   giou_loss: 1.58   conf_loss: 323.09   prob_loss: 1.05   total_loss: 325.72\n",
      "=> STEP 1341/6250   lr: 0.000925   giou_loss: 1.89   conf_loss: 322.83   prob_loss: 1.06   total_loss: 325.77\n",
      "=> STEP 1342/6250   lr: 0.000924   giou_loss: 1.97   conf_loss: 322.58   prob_loss: 1.06   total_loss: 325.61\n",
      "=> STEP 1343/6250   lr: 0.000924   giou_loss: 1.86   conf_loss: 322.33   prob_loss: 1.05   total_loss: 325.25\n",
      "=> STEP 1344/6250   lr: 0.000924   giou_loss: 1.74   conf_loss: 322.09   prob_loss: 1.05   total_loss: 324.87\n",
      "=> STEP 1345/6250   lr: 0.000924   giou_loss: 1.65   conf_loss: 321.86   prob_loss: 1.04   total_loss: 324.56\n",
      "=> STEP 1346/6250   lr: 0.000924   giou_loss: 1.74   conf_loss: 321.61   prob_loss: 1.04   total_loss: 324.39\n",
      "=> STEP 1347/6250   lr: 0.000924   giou_loss: 1.65   conf_loss: 321.35   prob_loss: 1.05   total_loss: 324.05\n",
      "=> STEP 1348/6250   lr: 0.000924   giou_loss: 1.66   conf_loss: 321.10   prob_loss: 1.06   total_loss: 323.82\n",
      "=> STEP 1349/6250   lr: 0.000924   giou_loss: 1.73   conf_loss: 320.84   prob_loss: 1.06   total_loss: 323.63\n",
      "=> STEP 1350/6250   lr: 0.000923   giou_loss: 1.60   conf_loss: 320.60   prob_loss: 1.06   total_loss: 323.26\n",
      "=> STEP 1351/6250   lr: 0.000923   giou_loss: 1.72   conf_loss: 320.37   prob_loss: 1.05   total_loss: 323.13\n",
      "=> STEP 1352/6250   lr: 0.000923   giou_loss: 1.78   conf_loss: 320.12   prob_loss: 1.04   total_loss: 322.95\n",
      "=> STEP 1353/6250   lr: 0.000923   giou_loss: 1.69   conf_loss: 319.87   prob_loss: 1.05   total_loss: 322.60\n",
      "=> STEP 1354/6250   lr: 0.000923   giou_loss: 1.65   conf_loss: 319.61   prob_loss: 1.06   total_loss: 322.32\n",
      "=> STEP 1355/6250   lr: 0.000923   giou_loss: 1.66   conf_loss: 319.36   prob_loss: 1.06   total_loss: 322.08\n",
      "=> STEP 1356/6250   lr: 0.000923   giou_loss: 1.67   conf_loss: 319.13   prob_loss: 1.06   total_loss: 321.85\n",
      "=> STEP 1357/6250   lr: 0.000922   giou_loss: 1.67   conf_loss: 318.89   prob_loss: 1.05   total_loss: 321.62\n",
      "=> STEP 1358/6250   lr: 0.000922   giou_loss: 1.59   conf_loss: 318.65   prob_loss: 1.04   total_loss: 321.28\n",
      "=> STEP 1359/6250   lr: 0.000922   giou_loss: 1.65   conf_loss: 318.41   prob_loss: 1.04   total_loss: 321.10\n",
      "=> STEP 1360/6250   lr: 0.000922   giou_loss: 1.56   conf_loss: 318.16   prob_loss: 1.05   total_loss: 320.77\n",
      "=> STEP 1361/6250   lr: 0.000922   giou_loss: 1.79   conf_loss: 317.93   prob_loss: 1.05   total_loss: 320.77\n",
      "=> STEP 1362/6250   lr: 0.000922   giou_loss: 1.69   conf_loss: 317.67   prob_loss: 1.05   total_loss: 320.41\n",
      "=> STEP 1363/6250   lr: 0.000922   giou_loss: 1.71   conf_loss: 317.44   prob_loss: 1.05   total_loss: 320.19\n",
      "=> STEP 1364/6250   lr: 0.000921   giou_loss: 1.87   conf_loss: 317.19   prob_loss: 1.04   total_loss: 320.10\n",
      "=> STEP 1365/6250   lr: 0.000921   giou_loss: 1.78   conf_loss: 316.95   prob_loss: 1.04   total_loss: 319.76\n",
      "=> STEP 1366/6250   lr: 0.000921   giou_loss: 1.63   conf_loss: 316.71   prob_loss: 1.04   total_loss: 319.39\n",
      "=> STEP 1367/6250   lr: 0.000921   giou_loss: 1.72   conf_loss: 316.46   prob_loss: 1.06   total_loss: 319.24\n",
      "=> STEP 1368/6250   lr: 0.000921   giou_loss: 1.56   conf_loss: 316.21   prob_loss: 1.06   total_loss: 318.83\n",
      "=> STEP 1369/6250   lr: 0.000921   giou_loss: 1.62   conf_loss: 315.96   prob_loss: 1.05   total_loss: 318.64\n",
      "=> STEP 1370/6250   lr: 0.000921   giou_loss: 1.56   conf_loss: 315.73   prob_loss: 1.04   total_loss: 318.34\n",
      "=> STEP 1371/6250   lr: 0.000920   giou_loss: 1.56   conf_loss: 315.49   prob_loss: 1.04   total_loss: 318.09\n",
      "=> STEP 1372/6250   lr: 0.000920   giou_loss: 1.60   conf_loss: 315.25   prob_loss: 1.04   total_loss: 317.89\n",
      "=> STEP 1373/6250   lr: 0.000920   giou_loss: 1.59   conf_loss: 315.00   prob_loss: 1.04   total_loss: 317.64\n",
      "=> STEP 1374/6250   lr: 0.000920   giou_loss: 1.57   conf_loss: 314.76   prob_loss: 1.04   total_loss: 317.36\n",
      "=> STEP 1375/6250   lr: 0.000920   giou_loss: 1.57   conf_loss: 314.52   prob_loss: 1.04   total_loss: 317.12\n",
      "=> STEP 1376/6250   lr: 0.000920   giou_loss: 1.60   conf_loss: 314.28   prob_loss: 1.04   total_loss: 316.92\n",
      "=> STEP 1377/6250   lr: 0.000920   giou_loss: 1.60   conf_loss: 314.03   prob_loss: 1.04   total_loss: 316.68\n",
      "=> STEP 1378/6250   lr: 0.000920   giou_loss: 1.57   conf_loss: 313.80   prob_loss: 1.04   total_loss: 316.40\n",
      "=> STEP 1379/6250   lr: 0.000919   giou_loss: 1.56   conf_loss: 313.56   prob_loss: 1.04   total_loss: 316.16\n",
      "=> STEP 1380/6250   lr: 0.000919   giou_loss: 1.58   conf_loss: 313.32   prob_loss: 1.04   total_loss: 315.93\n",
      "=> STEP 1381/6250   lr: 0.000919   giou_loss: 1.63   conf_loss: 313.07   prob_loss: 1.04   total_loss: 315.74\n",
      "=> STEP 1382/6250   lr: 0.000919   giou_loss: 1.59   conf_loss: 312.84   prob_loss: 1.04   total_loss: 315.47\n",
      "=> STEP 1383/6250   lr: 0.000919   giou_loss: 1.68   conf_loss: 312.62   prob_loss: 1.03   total_loss: 315.32\n",
      "=> STEP 1384/6250   lr: 0.000919   giou_loss: 1.68   conf_loss: 312.38   prob_loss: 1.03   total_loss: 315.09\n",
      "=> STEP 1385/6250   lr: 0.000919   giou_loss: 1.56   conf_loss: 312.14   prob_loss: 1.04   total_loss: 314.74\n",
      "=> STEP 1386/6250   lr: 0.000918   giou_loss: 1.67   conf_loss: 311.89   prob_loss: 1.04   total_loss: 314.60\n",
      "=> STEP 1387/6250   lr: 0.000918   giou_loss: 1.63   conf_loss: 311.67   prob_loss: 1.03   total_loss: 314.32\n",
      "=> STEP 1388/6250   lr: 0.000918   giou_loss: 1.68   conf_loss: 311.45   prob_loss: 1.02   total_loss: 314.15\n",
      "=> STEP 1389/6250   lr: 0.000918   giou_loss: 1.64   conf_loss: 311.21   prob_loss: 1.02   total_loss: 313.87\n",
      "=> STEP 1390/6250   lr: 0.000918   giou_loss: 1.72   conf_loss: 310.97   prob_loss: 1.02   total_loss: 313.71\n",
      "=> STEP 1391/6250   lr: 0.000918   giou_loss: 1.65   conf_loss: 310.73   prob_loss: 1.03   total_loss: 313.41\n",
      "=> STEP 1392/6250   lr: 0.000918   giou_loss: 1.70   conf_loss: 310.50   prob_loss: 1.03   total_loss: 313.23\n",
      "=> STEP 1393/6250   lr: 0.000917   giou_loss: 1.71   conf_loss: 310.26   prob_loss: 1.03   total_loss: 313.01\n",
      "=> STEP 1394/6250   lr: 0.000917   giou_loss: 1.58   conf_loss: 310.02   prob_loss: 1.03   total_loss: 312.64\n",
      "=> STEP 1395/6250   lr: 0.000917   giou_loss: 1.66   conf_loss: 309.80   prob_loss: 1.02   total_loss: 312.48\n",
      "=> STEP 1396/6250   lr: 0.000917   giou_loss: 1.67   conf_loss: 309.57   prob_loss: 1.02   total_loss: 312.26\n",
      "=> STEP 1397/6250   lr: 0.000917   giou_loss: 1.66   conf_loss: 309.33   prob_loss: 1.03   total_loss: 312.02\n",
      "=> STEP 1398/6250   lr: 0.000917   giou_loss: 1.57   conf_loss: 309.07   prob_loss: 1.04   total_loss: 311.68\n",
      "=> STEP 1399/6250   lr: 0.000917   giou_loss: 1.57   conf_loss: 308.85   prob_loss: 1.04   total_loss: 311.46\n",
      "=> STEP 1400/6250   lr: 0.000916   giou_loss: 1.57   conf_loss: 308.62   prob_loss: 1.03   total_loss: 311.21\n",
      "=> STEP 1401/6250   lr: 0.000916   giou_loss: 1.67   conf_loss: 308.40   prob_loss: 1.02   total_loss: 311.08\n",
      "=> STEP 1402/6250   lr: 0.000916   giou_loss: 1.59   conf_loss: 308.16   prob_loss: 1.02   total_loss: 310.77\n",
      "=> STEP 1403/6250   lr: 0.000916   giou_loss: 1.63   conf_loss: 307.92   prob_loss: 1.03   total_loss: 310.59\n",
      "=> STEP 1404/6250   lr: 0.000916   giou_loss: 1.72   conf_loss: 307.69   prob_loss: 1.04   total_loss: 310.44\n",
      "=> STEP 1405/6250   lr: 0.000916   giou_loss: 1.65   conf_loss: 307.47   prob_loss: 1.02   total_loss: 310.14\n",
      "=> STEP 1406/6250   lr: 0.000916   giou_loss: 1.61   conf_loss: 307.26   prob_loss: 1.01   total_loss: 309.88\n",
      "=> STEP 1407/6250   lr: 0.000915   giou_loss: 1.56   conf_loss: 307.03   prob_loss: 1.01   total_loss: 309.61\n",
      "=> STEP 1408/6250   lr: 0.000915   giou_loss: 1.56   conf_loss: 306.78   prob_loss: 1.02   total_loss: 309.36\n",
      "=> STEP 1409/6250   lr: 0.000915   giou_loss: 1.58   conf_loss: 306.54   prob_loss: 1.03   total_loss: 309.15\n",
      "=> STEP 1410/6250   lr: 0.000915   giou_loss: 1.57   conf_loss: 306.30   prob_loss: 1.03   total_loss: 308.89\n",
      "=> STEP 1411/6250   lr: 0.000915   giou_loss: 1.66   conf_loss: 306.08   prob_loss: 1.02   total_loss: 308.77\n",
      "=> STEP 1412/6250   lr: 0.000915   giou_loss: 1.63   conf_loss: 305.85   prob_loss: 1.01   total_loss: 308.49\n",
      "=> STEP 1413/6250   lr: 0.000915   giou_loss: 1.59   conf_loss: 305.62   prob_loss: 1.02   total_loss: 308.23\n",
      "=> STEP 1414/6250   lr: 0.000914   giou_loss: 1.68   conf_loss: 305.38   prob_loss: 1.03   total_loss: 308.09\n",
      "=> STEP 1415/6250   lr: 0.000914   giou_loss: 1.73   conf_loss: 305.16   prob_loss: 1.03   total_loss: 307.92\n",
      "=> STEP 1416/6250   lr: 0.000914   giou_loss: 1.56   conf_loss: 304.95   prob_loss: 1.02   total_loss: 307.53\n",
      "=> STEP 1417/6250   lr: 0.000914   giou_loss: 1.63   conf_loss: 304.72   prob_loss: 1.01   total_loss: 307.36\n",
      "=> STEP 1418/6250   lr: 0.000914   giou_loss: 1.56   conf_loss: 304.52   prob_loss: 1.02   total_loss: 307.10\n",
      "=> STEP 1419/6250   lr: 0.000914   giou_loss: 1.57   conf_loss: 304.27   prob_loss: 1.02   total_loss: 306.85\n",
      "=> STEP 1420/6250   lr: 0.000914   giou_loss: 1.61   conf_loss: 304.13   prob_loss: 1.01   total_loss: 306.74\n",
      "=> STEP 1421/6250   lr: 0.000913   giou_loss: 1.64   conf_loss: 303.85   prob_loss: 1.01   total_loss: 306.50\n",
      "=> STEP 1422/6250   lr: 0.000913   giou_loss: 1.61   conf_loss: 303.66   prob_loss: 1.01   total_loss: 306.28\n",
      "=> STEP 1423/6250   lr: 0.000913   giou_loss: 1.58   conf_loss: 303.39   prob_loss: 1.01   total_loss: 305.97\n",
      "=> STEP 1424/6250   lr: 0.000913   giou_loss: 1.60   conf_loss: 303.18   prob_loss: 1.01   total_loss: 305.80\n",
      "=> STEP 1425/6250   lr: 0.000913   giou_loss: 1.64   conf_loss: 302.92   prob_loss: 1.01   total_loss: 305.57\n",
      "=> STEP 1426/6250   lr: 0.000913   giou_loss: 1.58   conf_loss: 302.72   prob_loss: 1.00   total_loss: 305.31\n",
      "=> STEP 1427/6250   lr: 0.000913   giou_loss: 1.63   conf_loss: 302.48   prob_loss: 1.00   total_loss: 305.10\n",
      "=> STEP 1428/6250   lr: 0.000912   giou_loss: 1.58   conf_loss: 302.25   prob_loss: 1.01   total_loss: 304.85\n",
      "=> STEP 1429/6250   lr: 0.000912   giou_loss: 1.63   conf_loss: 302.01   prob_loss: 1.01   total_loss: 304.65\n",
      "=> STEP 1430/6250   lr: 0.000912   giou_loss: 1.60   conf_loss: 301.79   prob_loss: 1.01   total_loss: 304.40\n",
      "=> STEP 1431/6250   lr: 0.000912   giou_loss: 1.60   conf_loss: 301.57   prob_loss: 1.01   total_loss: 304.18\n",
      "=> STEP 1432/6250   lr: 0.000912   giou_loss: 1.58   conf_loss: 301.32   prob_loss: 1.01   total_loss: 303.91\n",
      "=> STEP 1433/6250   lr: 0.000912   giou_loss: 1.65   conf_loss: 301.12   prob_loss: 1.01   total_loss: 303.79\n",
      "=> STEP 1434/6250   lr: 0.000912   giou_loss: 1.63   conf_loss: 300.87   prob_loss: 1.01   total_loss: 303.51\n",
      "=> STEP 1435/6250   lr: 0.000911   giou_loss: 1.72   conf_loss: 300.69   prob_loss: 1.01   total_loss: 303.42\n",
      "=> STEP 1436/6250   lr: 0.000911   giou_loss: 1.81   conf_loss: 300.42   prob_loss: 1.01   total_loss: 303.24\n",
      "=> STEP 1437/6250   lr: 0.000911   giou_loss: 1.66   conf_loss: 300.23   prob_loss: 1.01   total_loss: 302.90\n",
      "=> STEP 1438/6250   lr: 0.000911   giou_loss: 1.68   conf_loss: 299.96   prob_loss: 1.02   total_loss: 302.65\n",
      "=> STEP 1439/6250   lr: 0.000911   giou_loss: 1.78   conf_loss: 299.76   prob_loss: 1.02   total_loss: 302.56\n",
      "=> STEP 1440/6250   lr: 0.000911   giou_loss: 1.65   conf_loss: 299.51   prob_loss: 1.01   total_loss: 302.18\n",
      "=> STEP 1441/6250   lr: 0.000911   giou_loss: 1.70   conf_loss: 299.33   prob_loss: 1.00   total_loss: 302.04\n",
      "=> STEP 1442/6250   lr: 0.000910   giou_loss: 1.70   conf_loss: 299.10   prob_loss: 1.00   total_loss: 301.80\n",
      "=> STEP 1443/6250   lr: 0.000910   giou_loss: 1.56   conf_loss: 298.89   prob_loss: 1.01   total_loss: 301.46\n",
      "=> STEP 1444/6250   lr: 0.000910   giou_loss: 1.63   conf_loss: 298.63   prob_loss: 1.01   total_loss: 301.27\n",
      "=> STEP 1445/6250   lr: 0.000910   giou_loss: 1.56   conf_loss: 298.41   prob_loss: 1.01   total_loss: 300.99\n",
      "=> STEP 1446/6250   lr: 0.000910   giou_loss: 1.69   conf_loss: 298.19   prob_loss: 1.01   total_loss: 300.90\n",
      "=> STEP 1447/6250   lr: 0.000910   giou_loss: 1.58   conf_loss: 297.96   prob_loss: 1.01   total_loss: 300.55\n",
      "=> STEP 1448/6250   lr: 0.000910   giou_loss: 1.65   conf_loss: 297.76   prob_loss: 1.00   total_loss: 300.41\n",
      "=> STEP 1449/6250   lr: 0.000909   giou_loss: 1.71   conf_loss: 297.53   prob_loss: 1.00   total_loss: 300.24\n",
      "=> STEP 1450/6250   lr: 0.000909   giou_loss: 1.57   conf_loss: 297.31   prob_loss: 1.01   total_loss: 299.88\n",
      "=> STEP 1451/6250   lr: 0.000909   giou_loss: 1.64   conf_loss: 297.07   prob_loss: 1.02   total_loss: 299.73\n",
      "=> STEP 1452/6250   lr: 0.000909   giou_loss: 1.66   conf_loss: 296.85   prob_loss: 1.02   total_loss: 299.53\n",
      "=> STEP 1453/6250   lr: 0.000909   giou_loss: 1.59   conf_loss: 296.64   prob_loss: 1.01   total_loss: 299.24\n",
      "=> STEP 1454/6250   lr: 0.000909   giou_loss: 1.73   conf_loss: 296.43   prob_loss: 1.00   total_loss: 299.16\n",
      "=> STEP 1455/6250   lr: 0.000909   giou_loss: 1.73   conf_loss: 296.21   prob_loss: 0.99   total_loss: 298.93\n",
      "=> STEP 1456/6250   lr: 0.000908   giou_loss: 1.71   conf_loss: 296.00   prob_loss: 1.00   total_loss: 298.70\n",
      "=> STEP 1457/6250   lr: 0.000908   giou_loss: 1.80   conf_loss: 295.77   prob_loss: 1.01   total_loss: 298.58\n",
      "=> STEP 1458/6250   lr: 0.000908   giou_loss: 1.72   conf_loss: 295.54   prob_loss: 1.02   total_loss: 298.27\n",
      "=> STEP 1459/6250   lr: 0.000908   giou_loss: 1.79   conf_loss: 295.32   prob_loss: 1.01   total_loss: 298.13\n",
      "=> STEP 1460/6250   lr: 0.000908   giou_loss: 1.95   conf_loss: 295.12   prob_loss: 1.00   total_loss: 298.07\n",
      "=> STEP 1461/6250   lr: 0.000908   giou_loss: 1.99   conf_loss: 294.90   prob_loss: 0.99   total_loss: 297.88\n",
      "=> STEP 1462/6250   lr: 0.000908   giou_loss: 1.82   conf_loss: 294.69   prob_loss: 0.99   total_loss: 297.50\n",
      "=> STEP 1463/6250   lr: 0.000907   giou_loss: 1.64   conf_loss: 294.47   prob_loss: 0.99   total_loss: 297.11\n",
      "=> STEP 1464/6250   lr: 0.000907   giou_loss: 1.81   conf_loss: 294.25   prob_loss: 1.01   total_loss: 297.07\n",
      "=> STEP 1465/6250   lr: 0.000907   giou_loss: 1.72   conf_loss: 294.02   prob_loss: 1.02   total_loss: 296.76\n",
      "=> STEP 1466/6250   lr: 0.000907   giou_loss: 1.67   conf_loss: 293.79   prob_loss: 1.02   total_loss: 296.48\n",
      "=> STEP 1467/6250   lr: 0.000907   giou_loss: 1.82   conf_loss: 293.58   prob_loss: 1.01   total_loss: 296.41\n",
      "=> STEP 1468/6250   lr: 0.000907   giou_loss: 1.69   conf_loss: 293.38   prob_loss: 1.00   total_loss: 296.07\n",
      "=> STEP 1469/6250   lr: 0.000906   giou_loss: 1.71   conf_loss: 293.18   prob_loss: 0.99   total_loss: 295.88\n",
      "=> STEP 1470/6250   lr: 0.000906   giou_loss: 1.76   conf_loss: 292.97   prob_loss: 0.99   total_loss: 295.72\n",
      "=> STEP 1471/6250   lr: 0.000906   giou_loss: 1.68   conf_loss: 292.75   prob_loss: 1.00   total_loss: 295.43\n",
      "=> STEP 1472/6250   lr: 0.000906   giou_loss: 1.69   conf_loss: 292.52   prob_loss: 1.02   total_loss: 295.23\n",
      "=> STEP 1473/6250   lr: 0.000906   giou_loss: 1.75   conf_loss: 292.30   prob_loss: 1.02   total_loss: 295.07\n",
      "=> STEP 1474/6250   lr: 0.000906   giou_loss: 1.64   conf_loss: 292.10   prob_loss: 1.01   total_loss: 294.74\n",
      "=> STEP 1475/6250   lr: 0.000906   giou_loss: 1.68   conf_loss: 291.91   prob_loss: 0.99   total_loss: 294.58\n",
      "=> STEP 1476/6250   lr: 0.000905   giou_loss: 1.70   conf_loss: 291.71   prob_loss: 0.99   total_loss: 294.39\n",
      "=> STEP 1477/6250   lr: 0.000905   giou_loss: 1.57   conf_loss: 291.47   prob_loss: 1.00   total_loss: 294.03\n",
      "=> STEP 1478/6250   lr: 0.000905   giou_loss: 1.76   conf_loss: 291.23   prob_loss: 1.01   total_loss: 294.00\n",
      "=> STEP 1479/6250   lr: 0.000905   giou_loss: 1.85   conf_loss: 291.03   prob_loss: 1.00   total_loss: 293.88\n",
      "=> STEP 1480/6250   lr: 0.000905   giou_loss: 1.81   conf_loss: 290.79   prob_loss: 1.00   total_loss: 293.61\n",
      "=> STEP 1481/6250   lr: 0.000905   giou_loss: 1.65   conf_loss: 290.60   prob_loss: 1.00   total_loss: 293.25\n",
      "=> STEP 1482/6250   lr: 0.000905   giou_loss: 1.72   conf_loss: 290.39   prob_loss: 0.99   total_loss: 293.09\n",
      "=> STEP 1483/6250   lr: 0.000904   giou_loss: 1.82   conf_loss: 290.18   prob_loss: 0.99   total_loss: 292.99\n",
      "=> STEP 1484/6250   lr: 0.000904   giou_loss: 1.81   conf_loss: 289.95   prob_loss: 1.00   total_loss: 292.75\n",
      "=> STEP 1485/6250   lr: 0.000904   giou_loss: 1.60   conf_loss: 289.72   prob_loss: 1.00   total_loss: 292.33\n",
      "=> STEP 1486/6250   lr: 0.000904   giou_loss: 1.88   conf_loss: 289.50   prob_loss: 1.01   total_loss: 292.39\n",
      "=> STEP 1487/6250   lr: 0.000904   giou_loss: 1.98   conf_loss: 289.28   prob_loss: 1.01   total_loss: 292.27\n",
      "=> STEP 1488/6250   lr: 0.000904   giou_loss: 1.91   conf_loss: 289.07   prob_loss: 1.01   total_loss: 291.99\n",
      "=> STEP 1489/6250   lr: 0.000904   giou_loss: 1.86   conf_loss: 288.86   prob_loss: 1.00   total_loss: 291.73\n",
      "=> STEP 1490/6250   lr: 0.000903   giou_loss: 1.64   conf_loss: 288.67   prob_loss: 0.99   total_loss: 291.30\n",
      "=> STEP 1491/6250   lr: 0.000903   giou_loss: 1.78   conf_loss: 288.49   prob_loss: 0.98   total_loss: 291.25\n",
      "=> STEP 1492/6250   lr: 0.000903   giou_loss: 2.02   conf_loss: 288.28   prob_loss: 0.98   total_loss: 291.28\n",
      "=> STEP 1493/6250   lr: 0.000903   giou_loss: 2.01   conf_loss: 288.07   prob_loss: 0.99   total_loss: 291.06\n",
      "=> STEP 1494/6250   lr: 0.000903   giou_loss: 1.81   conf_loss: 287.82   prob_loss: 1.01   total_loss: 290.64\n",
      "=> STEP 1495/6250   lr: 0.000903   giou_loss: 1.59   conf_loss: 287.61   prob_loss: 1.01   total_loss: 290.21\n",
      "=> STEP 1496/6250   lr: 0.000902   giou_loss: 1.85   conf_loss: 287.39   prob_loss: 1.02   total_loss: 290.26\n",
      "=> STEP 1497/6250   lr: 0.000902   giou_loss: 1.98   conf_loss: 287.19   prob_loss: 1.01   total_loss: 290.17\n",
      "=> STEP 1498/6250   lr: 0.000902   giou_loss: 2.02   conf_loss: 286.99   prob_loss: 0.99   total_loss: 290.00\n",
      "=> STEP 1499/6250   lr: 0.000902   giou_loss: 1.99   conf_loss: 286.79   prob_loss: 0.99   total_loss: 289.77\n",
      "=> STEP 1500/6250   lr: 0.000902   giou_loss: 1.85   conf_loss: 286.60   prob_loss: 0.99   total_loss: 289.43\n",
      "=> STEP 1501/6250   lr: 0.000902   giou_loss: 1.61   conf_loss: 286.41   prob_loss: 1.00   total_loss: 289.01\n",
      "=> STEP 1502/6250   lr: 0.000902   giou_loss: 1.84   conf_loss: 286.23   prob_loss: 0.99   total_loss: 289.06\n",
      "=> STEP 1503/6250   lr: 0.000901   giou_loss: 2.00   conf_loss: 286.02   prob_loss: 0.98   total_loss: 289.00\n",
      "=> STEP 1504/6250   lr: 0.000901   giou_loss: 2.05   conf_loss: 285.81   prob_loss: 0.99   total_loss: 288.85\n",
      "=> STEP 1505/6250   lr: 0.000901   giou_loss: 1.98   conf_loss: 285.59   prob_loss: 1.00   total_loss: 288.56\n",
      "=> STEP 1506/6250   lr: 0.000901   giou_loss: 1.84   conf_loss: 285.35   prob_loss: 1.00   total_loss: 288.19\n",
      "=> STEP 1507/6250   lr: 0.000901   giou_loss: 1.63   conf_loss: 285.14   prob_loss: 1.01   total_loss: 287.78\n",
      "=> STEP 1508/6250   lr: 0.000901   giou_loss: 1.87   conf_loss: 284.93   prob_loss: 1.01   total_loss: 287.81\n",
      "=> STEP 1509/6250   lr: 0.000900   giou_loss: 2.08   conf_loss: 284.73   prob_loss: 1.00   total_loss: 287.81\n",
      "=> STEP 1510/6250   lr: 0.000900   giou_loss: 2.13   conf_loss: 284.52   prob_loss: 0.99   total_loss: 287.63\n",
      "=> STEP 1511/6250   lr: 0.000900   giou_loss: 2.08   conf_loss: 284.30   prob_loss: 0.99   total_loss: 287.38\n",
      "=> STEP 1512/6250   lr: 0.000900   giou_loss: 2.02   conf_loss: 284.09   prob_loss: 0.99   total_loss: 287.11\n",
      "=> STEP 1513/6250   lr: 0.000900   giou_loss: 1.83   conf_loss: 283.89   prob_loss: 0.98   total_loss: 286.71\n",
      "=> STEP 1514/6250   lr: 0.000900   giou_loss: 1.56   conf_loss: 283.70   prob_loss: 0.98   total_loss: 286.24\n",
      "=> STEP 1515/6250   lr: 0.000900   giou_loss: 1.75   conf_loss: 283.50   prob_loss: 0.97   total_loss: 286.22\n",
      "=> STEP 1516/6250   lr: 0.000899   giou_loss: 1.81   conf_loss: 283.29   prob_loss: 0.97   total_loss: 286.08\n",
      "=> STEP 1517/6250   lr: 0.000899   giou_loss: 1.73   conf_loss: 283.08   prob_loss: 0.98   total_loss: 285.79\n",
      "=> STEP 1518/6250   lr: 0.000899   giou_loss: 1.58   conf_loss: 282.87   prob_loss: 0.99   total_loss: 285.45\n",
      "=> STEP 1519/6250   lr: 0.000899   giou_loss: 1.59   conf_loss: 282.65   prob_loss: 0.98   total_loss: 285.23\n",
      "=> STEP 1520/6250   lr: 0.000899   giou_loss: 1.65   conf_loss: 282.45   prob_loss: 0.97   total_loss: 285.08\n",
      "=> STEP 1521/6250   lr: 0.000899   giou_loss: 1.62   conf_loss: 282.24   prob_loss: 0.97   total_loss: 284.83\n",
      "=> STEP 1522/6250   lr: 0.000898   giou_loss: 1.67   conf_loss: 282.02   prob_loss: 0.98   total_loss: 284.66\n",
      "=> STEP 1523/6250   lr: 0.000898   giou_loss: 1.69   conf_loss: 281.81   prob_loss: 0.98   total_loss: 284.48\n",
      "=> STEP 1524/6250   lr: 0.000898   giou_loss: 1.57   conf_loss: 281.61   prob_loss: 0.97   total_loss: 284.15\n",
      "=> STEP 1525/6250   lr: 0.000898   giou_loss: 1.64   conf_loss: 281.41   prob_loss: 0.97   total_loss: 284.01\n",
      "=> STEP 1526/6250   lr: 0.000898   giou_loss: 1.56   conf_loss: 281.20   prob_loss: 0.97   total_loss: 283.73\n",
      "=> STEP 1527/6250   lr: 0.000898   giou_loss: 1.56   conf_loss: 280.99   prob_loss: 0.98   total_loss: 283.53\n",
      "=> STEP 1528/6250   lr: 0.000898   giou_loss: 1.63   conf_loss: 280.78   prob_loss: 0.98   total_loss: 283.38\n",
      "=> STEP 1529/6250   lr: 0.000897   giou_loss: 1.59   conf_loss: 280.57   prob_loss: 0.97   total_loss: 283.14\n",
      "=> STEP 1530/6250   lr: 0.000897   giou_loss: 1.67   conf_loss: 280.39   prob_loss: 0.96   total_loss: 283.02\n",
      "=> STEP 1531/6250   lr: 0.000897   giou_loss: 1.58   conf_loss: 280.17   prob_loss: 0.97   total_loss: 282.72\n",
      "=> STEP 1532/6250   lr: 0.000897   giou_loss: 1.75   conf_loss: 279.96   prob_loss: 0.98   total_loss: 282.69\n",
      "=> STEP 1533/6250   lr: 0.000897   giou_loss: 1.80   conf_loss: 279.75   prob_loss: 0.98   total_loss: 282.54\n",
      "=> STEP 1534/6250   lr: 0.000897   giou_loss: 1.70   conf_loss: 279.56   prob_loss: 0.97   total_loss: 282.23\n",
      "=> STEP 1535/6250   lr: 0.000896   giou_loss: 1.62   conf_loss: 279.37   prob_loss: 0.96   total_loss: 281.96\n",
      "=> STEP 1536/6250   lr: 0.000896   giou_loss: 1.67   conf_loss: 279.17   prob_loss: 0.96   total_loss: 281.81\n",
      "=> STEP 1537/6250   lr: 0.000896   giou_loss: 1.61   conf_loss: 278.97   prob_loss: 0.96   total_loss: 281.53\n",
      "=> STEP 1538/6250   lr: 0.000896   giou_loss: 1.66   conf_loss: 278.75   prob_loss: 0.97   total_loss: 281.38\n",
      "=> STEP 1539/6250   lr: 0.000896   giou_loss: 1.65   conf_loss: 278.56   prob_loss: 0.98   total_loss: 281.19\n",
      "=> STEP 1540/6250   lr: 0.000896   giou_loss: 1.79   conf_loss: 278.35   prob_loss: 0.97   total_loss: 281.12\n",
      "=> STEP 1541/6250   lr: 0.000896   giou_loss: 1.73   conf_loss: 278.19   prob_loss: 0.96   total_loss: 280.88\n",
      "=> STEP 1542/6250   lr: 0.000895   giou_loss: 1.63   conf_loss: 277.96   prob_loss: 0.96   total_loss: 280.56\n",
      "=> STEP 1543/6250   lr: 0.000895   giou_loss: 1.73   conf_loss: 277.79   prob_loss: 0.97   total_loss: 280.48\n",
      "=> STEP 1544/6250   lr: 0.000895   giou_loss: 1.80   conf_loss: 277.55   prob_loss: 0.97   total_loss: 280.32\n",
      "=> STEP 1545/6250   lr: 0.000895   giou_loss: 1.74   conf_loss: 277.38   prob_loss: 0.97   total_loss: 280.09\n",
      "=> STEP 1546/6250   lr: 0.000895   giou_loss: 1.60   conf_loss: 277.15   prob_loss: 0.97   total_loss: 279.72\n",
      "=> STEP 1547/6250   lr: 0.000895   giou_loss: 1.68   conf_loss: 276.98   prob_loss: 0.96   total_loss: 279.62\n",
      "=> STEP 1548/6250   lr: 0.000894   giou_loss: 1.72   conf_loss: 276.77   prob_loss: 0.96   total_loss: 279.45\n",
      "=> STEP 1549/6250   lr: 0.000894   giou_loss: 1.58   conf_loss: 276.56   prob_loss: 0.97   total_loss: 279.10\n",
      "=> STEP 1550/6250   lr: 0.000894   giou_loss: 1.73   conf_loss: 276.36   prob_loss: 0.98   total_loss: 279.06\n",
      "=> STEP 1551/6250   lr: 0.000894   giou_loss: 1.73   conf_loss: 276.15   prob_loss: 0.98   total_loss: 278.86\n",
      "=> STEP 1552/6250   lr: 0.000894   giou_loss: 1.57   conf_loss: 275.97   prob_loss: 0.97   total_loss: 278.51\n",
      "=> STEP 1553/6250   lr: 0.000894   giou_loss: 1.57   conf_loss: 275.78   prob_loss: 0.96   total_loss: 278.31\n",
      "=> STEP 1554/6250   lr: 0.000894   giou_loss: 1.63   conf_loss: 275.58   prob_loss: 0.96   total_loss: 278.17\n",
      "=> STEP 1555/6250   lr: 0.000893   giou_loss: 1.57   conf_loss: 275.38   prob_loss: 0.96   total_loss: 277.91\n",
      "=> STEP 1556/6250   lr: 0.000893   giou_loss: 1.56   conf_loss: 275.17   prob_loss: 0.97   total_loss: 277.70\n",
      "=> STEP 1557/6250   lr: 0.000893   giou_loss: 1.69   conf_loss: 274.97   prob_loss: 0.97   total_loss: 277.64\n",
      "=> STEP 1558/6250   lr: 0.000893   giou_loss: 1.60   conf_loss: 274.77   prob_loss: 0.97   total_loss: 277.34\n",
      "=> STEP 1559/6250   lr: 0.000893   giou_loss: 1.65   conf_loss: 274.58   prob_loss: 0.96   total_loss: 277.19\n",
      "=> STEP 1560/6250   lr: 0.000893   giou_loss: 1.64   conf_loss: 274.38   prob_loss: 0.96   total_loss: 276.98\n",
      "=> STEP 1561/6250   lr: 0.000892   giou_loss: 1.62   conf_loss: 274.18   prob_loss: 0.96   total_loss: 276.77\n",
      "=> STEP 1562/6250   lr: 0.000892   giou_loss: 1.56   conf_loss: 273.99   prob_loss: 0.96   total_loss: 276.52\n",
      "=> STEP 1563/6250   lr: 0.000892   giou_loss: 1.57   conf_loss: 273.78   prob_loss: 0.96   total_loss: 276.31\n",
      "=> STEP 1564/6250   lr: 0.000892   giou_loss: 1.63   conf_loss: 273.59   prob_loss: 0.96   total_loss: 276.19\n",
      "=> STEP 1565/6250   lr: 0.000892   giou_loss: 1.61   conf_loss: 273.39   prob_loss: 0.96   total_loss: 275.97\n",
      "=> STEP 1566/6250   lr: 0.000892   giou_loss: 1.61   conf_loss: 273.20   prob_loss: 0.97   total_loss: 275.78\n",
      "=> STEP 1567/6250   lr: 0.000891   giou_loss: 1.67   conf_loss: 273.00   prob_loss: 0.97   total_loss: 275.64\n",
      "=> STEP 1568/6250   lr: 0.000891   giou_loss: 1.71   conf_loss: 272.81   prob_loss: 0.97   total_loss: 275.48\n",
      "=> STEP 1569/6250   lr: 0.000891   giou_loss: 1.57   conf_loss: 272.62   prob_loss: 0.97   total_loss: 275.16\n",
      "=> STEP 1570/6250   lr: 0.000891   giou_loss: 1.81   conf_loss: 272.42   prob_loss: 0.96   total_loss: 275.19\n",
      "=> STEP 1571/6250   lr: 0.000891   giou_loss: 1.83   conf_loss: 272.26   prob_loss: 0.96   total_loss: 275.04\n",
      "=> STEP 1572/6250   lr: 0.000891   giou_loss: 1.78   conf_loss: 272.03   prob_loss: 0.96   total_loss: 274.78\n",
      "=> STEP 1573/6250   lr: 0.000891   giou_loss: 1.60   conf_loss: 271.85   prob_loss: 0.97   total_loss: 274.42\n",
      "=> STEP 1574/6250   lr: 0.000890   giou_loss: 1.80   conf_loss: 271.64   prob_loss: 0.97   total_loss: 274.41\n",
      "=> STEP 1575/6250   lr: 0.000890   giou_loss: 1.88   conf_loss: 271.45   prob_loss: 0.97   total_loss: 274.30\n",
      "=> STEP 1576/6250   lr: 0.000890   giou_loss: 1.88   conf_loss: 271.25   prob_loss: 0.97   total_loss: 274.10\n",
      "=> STEP 1577/6250   lr: 0.000890   giou_loss: 1.77   conf_loss: 271.07   prob_loss: 0.96   total_loss: 273.80\n",
      "=> STEP 1578/6250   lr: 0.000890   giou_loss: 1.56   conf_loss: 270.89   prob_loss: 0.95   total_loss: 273.41\n",
      "=> STEP 1579/6250   lr: 0.000890   giou_loss: 1.72   conf_loss: 270.70   prob_loss: 0.95   total_loss: 273.37\n",
      "=> STEP 1580/6250   lr: 0.000889   giou_loss: 1.75   conf_loss: 270.50   prob_loss: 0.95   total_loss: 273.20\n",
      "=> STEP 1581/6250   lr: 0.000889   giou_loss: 1.64   conf_loss: 270.31   prob_loss: 0.96   total_loss: 272.92\n",
      "=> STEP 1582/6250   lr: 0.000889   giou_loss: 1.71   conf_loss: 270.11   prob_loss: 0.97   total_loss: 272.79\n",
      "=> STEP 1583/6250   lr: 0.000889   giou_loss: 1.72   conf_loss: 269.94   prob_loss: 0.97   total_loss: 272.63\n",
      "=> STEP 1584/6250   lr: 0.000889   giou_loss: 1.66   conf_loss: 269.76   prob_loss: 0.96   total_loss: 272.37\n",
      "=> STEP 1585/6250   lr: 0.000889   giou_loss: 1.60   conf_loss: 269.58   prob_loss: 0.95   total_loss: 272.13\n",
      "=> STEP 1586/6250   lr: 0.000888   giou_loss: 1.62   conf_loss: 269.38   prob_loss: 0.95   total_loss: 271.95\n",
      "=> STEP 1587/6250   lr: 0.000888   giou_loss: 1.58   conf_loss: 269.18   prob_loss: 0.95   total_loss: 271.71\n",
      "=> STEP 1588/6250   lr: 0.000888   giou_loss: 1.57   conf_loss: 268.98   prob_loss: 0.96   total_loss: 271.51\n",
      "=> STEP 1589/6250   lr: 0.000888   giou_loss: 1.58   conf_loss: 268.79   prob_loss: 0.95   total_loss: 271.32\n",
      "=> STEP 1590/6250   lr: 0.000888   giou_loss: 1.61   conf_loss: 268.60   prob_loss: 0.95   total_loss: 271.16\n",
      "=> STEP 1591/6250   lr: 0.000888   giou_loss: 1.57   conf_loss: 268.41   prob_loss: 0.95   total_loss: 270.92\n",
      "=> STEP 1592/6250   lr: 0.000887   giou_loss: 1.57   conf_loss: 268.22   prob_loss: 0.95   total_loss: 270.73\n",
      "=> STEP 1593/6250   lr: 0.000887   giou_loss: 1.57   conf_loss: 268.02   prob_loss: 0.95   total_loss: 270.54\n",
      "=> STEP 1594/6250   lr: 0.000887   giou_loss: 1.62   conf_loss: 267.83   prob_loss: 0.96   total_loss: 270.41\n",
      "=> STEP 1595/6250   lr: 0.000887   giou_loss: 1.59   conf_loss: 267.63   prob_loss: 0.96   total_loss: 270.18\n",
      "=> STEP 1596/6250   lr: 0.000887   giou_loss: 1.56   conf_loss: 267.45   prob_loss: 0.95   total_loss: 269.96\n",
      "=> STEP 1597/6250   lr: 0.000887   giou_loss: 1.56   conf_loss: 267.25   prob_loss: 0.95   total_loss: 269.76\n",
      "=> STEP 1598/6250   lr: 0.000887   giou_loss: 1.56   conf_loss: 267.07   prob_loss: 0.95   total_loss: 269.58\n",
      "=> STEP 1599/6250   lr: 0.000886   giou_loss: 1.63   conf_loss: 266.87   prob_loss: 0.95   total_loss: 269.45\n",
      "=> STEP 1600/6250   lr: 0.000886   giou_loss: 1.60   conf_loss: 266.68   prob_loss: 0.95   total_loss: 269.23\n",
      "=> STEP 1601/6250   lr: 0.000886   giou_loss: 1.65   conf_loss: 266.49   prob_loss: 0.95   total_loss: 269.09\n",
      "=> STEP 1602/6250   lr: 0.000886   giou_loss: 1.65   conf_loss: 266.31   prob_loss: 0.95   total_loss: 268.91\n",
      "=> STEP 1603/6250   lr: 0.000886   giou_loss: 1.57   conf_loss: 266.11   prob_loss: 0.95   total_loss: 268.63\n",
      "=> STEP 1604/6250   lr: 0.000886   giou_loss: 1.56   conf_loss: 265.94   prob_loss: 0.95   total_loss: 268.45\n",
      "=> STEP 1605/6250   lr: 0.000885   giou_loss: 1.56   conf_loss: 265.74   prob_loss: 0.95   total_loss: 268.25\n",
      "=> STEP 1606/6250   lr: 0.000885   giou_loss: 1.56   conf_loss: 265.56   prob_loss: 0.95   total_loss: 268.07\n",
      "=> STEP 1607/6250   lr: 0.000885   giou_loss: 1.58   conf_loss: 265.36   prob_loss: 0.95   total_loss: 267.89\n",
      "=> STEP 1608/6250   lr: 0.000885   giou_loss: 1.62   conf_loss: 265.17   prob_loss: 0.95   total_loss: 267.74\n",
      "=> STEP 1609/6250   lr: 0.000885   giou_loss: 1.60   conf_loss: 264.98   prob_loss: 0.95   total_loss: 267.53\n",
      "=> STEP 1610/6250   lr: 0.000885   giou_loss: 1.65   conf_loss: 264.81   prob_loss: 0.94   total_loss: 267.40\n",
      "=> STEP 1611/6250   lr: 0.000884   giou_loss: 1.76   conf_loss: 264.63   prob_loss: 0.93   total_loss: 267.32\n",
      "=> STEP 1612/6250   lr: 0.000884   giou_loss: 1.66   conf_loss: 264.44   prob_loss: 0.94   total_loss: 267.04\n",
      "=> STEP 1613/6250   lr: 0.000884   giou_loss: 1.56   conf_loss: 264.25   prob_loss: 0.95   total_loss: 266.76\n",
      "=> STEP 1614/6250   lr: 0.000884   giou_loss: 1.73   conf_loss: 264.06   prob_loss: 0.95   total_loss: 266.74\n",
      "=> STEP 1615/6250   lr: 0.000884   giou_loss: 1.73   conf_loss: 263.89   prob_loss: 0.95   total_loss: 266.57\n",
      "=> STEP 1616/6250   lr: 0.000884   giou_loss: 1.64   conf_loss: 263.70   prob_loss: 0.94   total_loss: 266.27\n",
      "=> STEP 1617/6250   lr: 0.000883   giou_loss: 1.77   conf_loss: 263.54   prob_loss: 0.92   total_loss: 266.24\n",
      "=> STEP 1618/6250   lr: 0.000883   giou_loss: 1.82   conf_loss: 263.36   prob_loss: 0.92   total_loss: 266.10\n",
      "=> STEP 1619/6250   lr: 0.000883   giou_loss: 1.77   conf_loss: 263.17   prob_loss: 0.93   total_loss: 265.87\n",
      "=> STEP 1620/6250   lr: 0.000883   giou_loss: 1.62   conf_loss: 262.97   prob_loss: 0.95   total_loss: 265.54\n",
      "=> STEP 1621/6250   lr: 0.000883   giou_loss: 1.73   conf_loss: 262.79   prob_loss: 0.96   total_loss: 265.48\n",
      "=> STEP 1622/6250   lr: 0.000883   giou_loss: 1.83   conf_loss: 262.61   prob_loss: 0.96   total_loss: 265.40\n",
      "=> STEP 1623/6250   lr: 0.000882   giou_loss: 1.83   conf_loss: 262.44   prob_loss: 0.95   total_loss: 265.22\n",
      "=> STEP 1624/6250   lr: 0.000882   giou_loss: 1.66   conf_loss: 262.25   prob_loss: 0.93   total_loss: 264.85\n",
      "=> STEP 1625/6250   lr: 0.000882   giou_loss: 1.71   conf_loss: 262.11   prob_loss: 0.92   total_loss: 264.73\n",
      "=> STEP 1626/6250   lr: 0.000882   giou_loss: 1.79   conf_loss: 261.92   prob_loss: 0.92   total_loss: 264.63\n",
      "=> STEP 1627/6250   lr: 0.000882   giou_loss: 1.69   conf_loss: 261.74   prob_loss: 0.93   total_loss: 264.36\n",
      "=> STEP 1628/6250   lr: 0.000882   giou_loss: 1.56   conf_loss: 261.53   prob_loss: 0.95   total_loss: 264.04\n",
      "=> STEP 1629/6250   lr: 0.000881   giou_loss: 1.70   conf_loss: 261.36   prob_loss: 0.96   total_loss: 264.01\n",
      "=> STEP 1630/6250   lr: 0.000881   giou_loss: 1.73   conf_loss: 261.16   prob_loss: 0.96   total_loss: 263.84\n",
      "=> STEP 1631/6250   lr: 0.000881   giou_loss: 1.61   conf_loss: 260.98   prob_loss: 0.94   total_loss: 263.54\n",
      "=> STEP 1632/6250   lr: 0.000881   giou_loss: 1.65   conf_loss: 260.81   prob_loss: 0.93   total_loss: 263.40\n",
      "=> STEP 1633/6250   lr: 0.000881   giou_loss: 1.62   conf_loss: 260.63   prob_loss: 0.93   total_loss: 263.18\n",
      "=> STEP 1634/6250   lr: 0.000881   giou_loss: 1.59   conf_loss: 260.43   prob_loss: 0.94   total_loss: 262.96\n",
      "=> STEP 1635/6250   lr: 0.000880   giou_loss: 1.56   conf_loss: 260.23   prob_loss: 0.94   total_loss: 262.74\n",
      "=> STEP 1636/6250   lr: 0.000880   giou_loss: 1.58   conf_loss: 260.05   prob_loss: 0.94   total_loss: 262.57\n",
      "=> STEP 1637/6250   lr: 0.000880   giou_loss: 1.57   conf_loss: 259.87   prob_loss: 0.94   total_loss: 262.37\n",
      "=> STEP 1638/6250   lr: 0.000880   giou_loss: 1.68   conf_loss: 259.68   prob_loss: 0.94   total_loss: 262.30\n",
      "=> STEP 1639/6250   lr: 0.000880   giou_loss: 1.60   conf_loss: 259.50   prob_loss: 0.94   total_loss: 262.04\n",
      "=> STEP 1640/6250   lr: 0.000880   giou_loss: 1.63   conf_loss: 259.33   prob_loss: 0.94   total_loss: 261.90\n",
      "=> STEP 1641/6250   lr: 0.000879   giou_loss: 1.70   conf_loss: 259.13   prob_loss: 0.95   total_loss: 261.78\n",
      "=> STEP 1642/6250   lr: 0.000879   giou_loss: 1.64   conf_loss: 258.97   prob_loss: 0.94   total_loss: 261.54\n",
      "=> STEP 1643/6250   lr: 0.000879   giou_loss: 1.62   conf_loss: 258.79   prob_loss: 0.93   total_loss: 261.34\n",
      "=> STEP 1644/6250   lr: 0.000879   giou_loss: 1.56   conf_loss: 258.63   prob_loss: 0.93   total_loss: 261.12\n",
      "=> STEP 1645/6250   lr: 0.000879   giou_loss: 1.56   conf_loss: 258.42   prob_loss: 0.93   total_loss: 260.91\n",
      "=> STEP 1646/6250   lr: 0.000879   giou_loss: 1.56   conf_loss: 258.25   prob_loss: 0.94   total_loss: 260.75\n",
      "=> STEP 1647/6250   lr: 0.000878   giou_loss: 1.60   conf_loss: 258.04   prob_loss: 0.94   total_loss: 260.59\n",
      "=> STEP 1648/6250   lr: 0.000878   giou_loss: 1.56   conf_loss: 257.86   prob_loss: 0.94   total_loss: 260.37\n",
      "=> STEP 1649/6250   lr: 0.000878   giou_loss: 1.64   conf_loss: 257.68   prob_loss: 0.94   total_loss: 260.26\n",
      "=> STEP 1650/6250   lr: 0.000878   giou_loss: 1.57   conf_loss: 257.50   prob_loss: 0.93   total_loss: 260.00\n",
      "=> STEP 1651/6250   lr: 0.000878   giou_loss: 1.56   conf_loss: 257.32   prob_loss: 0.93   total_loss: 259.82\n",
      "=> STEP 1652/6250   lr: 0.000878   giou_loss: 1.57   conf_loss: 257.14   prob_loss: 0.94   total_loss: 259.64\n",
      "=> STEP 1653/6250   lr: 0.000877   giou_loss: 1.57   conf_loss: 256.96   prob_loss: 0.93   total_loss: 259.47\n",
      "=> STEP 1654/6250   lr: 0.000877   giou_loss: 1.71   conf_loss: 256.79   prob_loss: 0.93   total_loss: 259.43\n",
      "=> STEP 1655/6250   lr: 0.000877   giou_loss: 1.63   conf_loss: 256.61   prob_loss: 0.93   total_loss: 259.18\n",
      "=> STEP 1656/6250   lr: 0.000877   giou_loss: 1.60   conf_loss: 256.42   prob_loss: 0.94   total_loss: 258.96\n",
      "=> STEP 1657/6250   lr: 0.000877   giou_loss: 1.58   conf_loss: 256.24   prob_loss: 0.94   total_loss: 258.77\n",
      "=> STEP 1658/6250   lr: 0.000877   giou_loss: 1.69   conf_loss: 256.07   prob_loss: 0.94   total_loss: 258.70\n",
      "=> STEP 1659/6250   lr: 0.000876   giou_loss: 1.63   conf_loss: 255.89   prob_loss: 0.94   total_loss: 258.45\n",
      "=> STEP 1660/6250   lr: 0.000876   giou_loss: 1.66   conf_loss: 255.71   prob_loss: 0.94   total_loss: 258.31\n",
      "=> STEP 1661/6250   lr: 0.000876   giou_loss: 1.60   conf_loss: 255.53   prob_loss: 0.93   total_loss: 258.07\n",
      "=> STEP 1662/6250   lr: 0.000876   giou_loss: 1.64   conf_loss: 255.37   prob_loss: 0.93   total_loss: 257.94\n",
      "=> STEP 1663/6250   lr: 0.000876   giou_loss: 1.68   conf_loss: 255.18   prob_loss: 0.93   total_loss: 257.80\n",
      "=> STEP 1664/6250   lr: 0.000876   giou_loss: 1.61   conf_loss: 255.01   prob_loss: 0.93   total_loss: 257.55\n",
      "=> STEP 1665/6250   lr: 0.000875   giou_loss: 1.62   conf_loss: 254.83   prob_loss: 0.93   total_loss: 257.37\n",
      "=> STEP 1666/6250   lr: 0.000875   giou_loss: 1.64   conf_loss: 254.66   prob_loss: 0.93   total_loss: 257.22\n",
      "=> STEP 1667/6250   lr: 0.000875   giou_loss: 1.67   conf_loss: 254.48   prob_loss: 0.93   total_loss: 257.08\n",
      "=> STEP 1668/6250   lr: 0.000875   giou_loss: 1.60   conf_loss: 254.31   prob_loss: 0.93   total_loss: 256.84\n",
      "=> STEP 1669/6250   lr: 0.000875   giou_loss: 1.62   conf_loss: 254.13   prob_loss: 0.93   total_loss: 256.68\n",
      "=> STEP 1670/6250   lr: 0.000875   giou_loss: 1.64   conf_loss: 253.97   prob_loss: 0.93   total_loss: 256.54\n",
      "=> STEP 1671/6250   lr: 0.000874   giou_loss: 1.69   conf_loss: 253.78   prob_loss: 0.93   total_loss: 256.40\n",
      "=> STEP 1672/6250   lr: 0.000874   giou_loss: 1.60   conf_loss: 253.62   prob_loss: 0.93   total_loss: 256.15\n",
      "=> STEP 1673/6250   lr: 0.000874   giou_loss: 1.59   conf_loss: 253.43   prob_loss: 0.93   total_loss: 255.94\n",
      "=> STEP 1674/6250   lr: 0.000874   giou_loss: 1.66   conf_loss: 253.27   prob_loss: 0.92   total_loss: 255.85\n",
      "=> STEP 1675/6250   lr: 0.000874   giou_loss: 1.64   conf_loss: 253.10   prob_loss: 0.93   total_loss: 255.67\n",
      "=> STEP 1676/6250   lr: 0.000874   giou_loss: 1.61   conf_loss: 252.90   prob_loss: 0.94   total_loss: 255.45\n",
      "=> STEP 1677/6250   lr: 0.000873   giou_loss: 1.56   conf_loss: 252.76   prob_loss: 0.93   total_loss: 255.26\n",
      "=> STEP 1678/6250   lr: 0.000873   giou_loss: 1.57   conf_loss: 252.55   prob_loss: 0.93   total_loss: 255.05\n",
      "=> STEP 1679/6250   lr: 0.000873   giou_loss: 1.58   conf_loss: 252.40   prob_loss: 0.92   total_loss: 254.90\n",
      "=> STEP 1680/6250   lr: 0.000873   giou_loss: 1.56   conf_loss: 252.22   prob_loss: 0.93   total_loss: 254.71\n",
      "=> STEP 1681/6250   lr: 0.000873   giou_loss: 1.63   conf_loss: 252.04   prob_loss: 0.93   total_loss: 254.59\n",
      "=> STEP 1682/6250   lr: 0.000873   giou_loss: 1.56   conf_loss: 251.90   prob_loss: 0.93   total_loss: 254.39\n",
      "=> STEP 1683/6250   lr: 0.000872   giou_loss: 1.66   conf_loss: 251.69   prob_loss: 0.93   total_loss: 254.28\n",
      "=> STEP 1684/6250   lr: 0.000872   giou_loss: 1.66   conf_loss: 251.53   prob_loss: 0.93   total_loss: 254.12\n",
      "=> STEP 1685/6250   lr: 0.000872   giou_loss: 1.58   conf_loss: 251.34   prob_loss: 0.92   total_loss: 253.84\n",
      "=> STEP 1686/6250   lr: 0.000872   giou_loss: 1.56   conf_loss: 251.18   prob_loss: 0.92   total_loss: 253.67\n",
      "=> STEP 1687/6250   lr: 0.000872   giou_loss: 1.66   conf_loss: 251.00   prob_loss: 0.92   total_loss: 253.58\n",
      "=> STEP 1688/6250   lr: 0.000872   giou_loss: 1.60   conf_loss: 250.84   prob_loss: 0.91   total_loss: 253.35\n",
      "=> STEP 1689/6250   lr: 0.000871   giou_loss: 1.63   conf_loss: 250.69   prob_loss: 0.91   total_loss: 253.22\n",
      "=> STEP 1690/6250   lr: 0.000871   giou_loss: 1.67   conf_loss: 250.50   prob_loss: 0.91   total_loss: 253.08\n",
      "=> STEP 1691/6250   lr: 0.000871   giou_loss: 1.63   conf_loss: 250.33   prob_loss: 0.91   total_loss: 252.87\n",
      "=> STEP 1692/6250   lr: 0.000871   giou_loss: 1.65   conf_loss: 250.16   prob_loss: 0.92   total_loss: 252.73\n",
      "=> STEP 1693/6250   lr: 0.000871   giou_loss: 1.65   conf_loss: 249.99   prob_loss: 0.92   total_loss: 252.56\n",
      "=> STEP 1694/6250   lr: 0.000871   giou_loss: 1.66   conf_loss: 249.83   prob_loss: 0.91   total_loss: 252.40\n",
      "=> STEP 1695/6250   lr: 0.000870   giou_loss: 1.57   conf_loss: 249.67   prob_loss: 0.91   total_loss: 252.14\n",
      "=> STEP 1696/6250   lr: 0.000870   giou_loss: 1.65   conf_loss: 249.50   prob_loss: 0.90   total_loss: 252.05\n",
      "=> STEP 1697/6250   lr: 0.000870   giou_loss: 1.62   conf_loss: 249.32   prob_loss: 0.91   total_loss: 251.84\n",
      "=> STEP 1698/6250   lr: 0.000870   giou_loss: 1.66   conf_loss: 249.14   prob_loss: 0.92   total_loss: 251.72\n",
      "=> STEP 1699/6250   lr: 0.000870   giou_loss: 1.75   conf_loss: 248.96   prob_loss: 0.92   total_loss: 251.63\n",
      "=> STEP 1700/6250   lr: 0.000870   giou_loss: 1.62   conf_loss: 248.80   prob_loss: 0.91   total_loss: 251.33\n",
      "=> STEP 1701/6250   lr: 0.000869   giou_loss: 1.60   conf_loss: 248.65   prob_loss: 0.90   total_loss: 251.15\n",
      "=> STEP 1702/6250   lr: 0.000869   giou_loss: 1.57   conf_loss: 248.47   prob_loss: 0.90   total_loss: 250.94\n",
      "=> STEP 1703/6250   lr: 0.000869   giou_loss: 1.61   conf_loss: 248.31   prob_loss: 0.91   total_loss: 250.83\n",
      "=> STEP 1704/6250   lr: 0.000869   giou_loss: 1.57   conf_loss: 248.12   prob_loss: 0.91   total_loss: 250.60\n",
      "=> STEP 1705/6250   lr: 0.000869   giou_loss: 1.56   conf_loss: 247.94   prob_loss: 0.92   total_loss: 250.42\n",
      "=> STEP 1706/6250   lr: 0.000869   giou_loss: 1.64   conf_loss: 247.75   prob_loss: 0.92   total_loss: 250.31\n",
      "=> STEP 1707/6250   lr: 0.000868   giou_loss: 1.56   conf_loss: 247.59   prob_loss: 0.91   total_loss: 250.07\n",
      "=> STEP 1708/6250   lr: 0.000868   giou_loss: 1.60   conf_loss: 247.41   prob_loss: 0.91   total_loss: 249.92\n",
      "=> STEP 1709/6250   lr: 0.000868   giou_loss: 1.56   conf_loss: 247.24   prob_loss: 0.91   total_loss: 249.71\n",
      "=> STEP 1710/6250   lr: 0.000868   giou_loss: 1.63   conf_loss: 247.05   prob_loss: 0.91   total_loss: 249.59\n",
      "=> STEP 1711/6250   lr: 0.000868   giou_loss: 1.56   conf_loss: 246.88   prob_loss: 0.91   total_loss: 249.35\n",
      "=> STEP 1712/6250   lr: 0.000867   giou_loss: 1.60   conf_loss: 246.71   prob_loss: 0.91   total_loss: 249.21\n",
      "=> STEP 1713/6250   lr: 0.000867   giou_loss: 1.56   conf_loss: 246.52   prob_loss: 0.91   total_loss: 249.00\n",
      "=> STEP 1714/6250   lr: 0.000867   giou_loss: 1.64   conf_loss: 246.35   prob_loss: 0.91   total_loss: 248.91\n",
      "=> STEP 1715/6250   lr: 0.000867   giou_loss: 1.56   conf_loss: 246.17   prob_loss: 0.91   total_loss: 248.65\n",
      "=> STEP 1716/6250   lr: 0.000867   giou_loss: 1.59   conf_loss: 246.01   prob_loss: 0.91   total_loss: 248.51\n",
      "=> STEP 1717/6250   lr: 0.000867   giou_loss: 1.62   conf_loss: 245.84   prob_loss: 0.91   total_loss: 248.37\n",
      "=> STEP 1718/6250   lr: 0.000866   giou_loss: 1.57   conf_loss: 245.66   prob_loss: 0.91   total_loss: 248.14\n",
      "=> STEP 1719/6250   lr: 0.000866   giou_loss: 1.60   conf_loss: 245.50   prob_loss: 0.90   total_loss: 248.00\n",
      "=> STEP 1720/6250   lr: 0.000866   giou_loss: 1.57   conf_loss: 245.32   prob_loss: 0.91   total_loss: 247.80\n",
      "=> STEP 1721/6250   lr: 0.000866   giou_loss: 1.56   conf_loss: 245.15   prob_loss: 0.91   total_loss: 247.62\n",
      "=> STEP 1722/6250   lr: 0.000866   giou_loss: 1.63   conf_loss: 244.98   prob_loss: 0.91   total_loss: 247.52\n",
      "=> STEP 1723/6250   lr: 0.000866   giou_loss: 1.58   conf_loss: 244.81   prob_loss: 0.91   total_loss: 247.30\n",
      "=> STEP 1724/6250   lr: 0.000865   giou_loss: 1.56   conf_loss: 244.65   prob_loss: 0.91   total_loss: 247.12\n",
      "=> STEP 1725/6250   lr: 0.000865   giou_loss: 1.61   conf_loss: 244.48   prob_loss: 0.91   total_loss: 247.00\n",
      "=> STEP 1726/6250   lr: 0.000865   giou_loss: 1.58   conf_loss: 244.31   prob_loss: 0.91   total_loss: 246.80\n",
      "=> STEP 1727/6250   lr: 0.000865   giou_loss: 1.68   conf_loss: 244.14   prob_loss: 0.91   total_loss: 246.74\n",
      "=> STEP 1728/6250   lr: 0.000865   giou_loss: 1.60   conf_loss: 243.98   prob_loss: 0.91   total_loss: 246.48\n",
      "=> STEP 1729/6250   lr: 0.000865   giou_loss: 1.69   conf_loss: 243.82   prob_loss: 0.90   total_loss: 246.41\n",
      "=> STEP 1730/6250   lr: 0.000864   giou_loss: 1.74   conf_loss: 243.66   prob_loss: 0.90   total_loss: 246.31\n",
      "=> STEP 1731/6250   lr: 0.000864   giou_loss: 1.60   conf_loss: 243.50   prob_loss: 0.91   total_loss: 246.01\n",
      "=> STEP 1732/6250   lr: 0.000864   giou_loss: 1.84   conf_loss: 243.33   prob_loss: 0.91   total_loss: 246.09\n",
      "=> STEP 1733/6250   lr: 0.000864   giou_loss: 1.89   conf_loss: 243.18   prob_loss: 0.92   total_loss: 245.99\n",
      "=> STEP 1734/6250   lr: 0.000864   giou_loss: 1.79   conf_loss: 243.00   prob_loss: 0.92   total_loss: 245.71\n",
      "=> STEP 1735/6250   lr: 0.000864   giou_loss: 1.73   conf_loss: 242.85   prob_loss: 0.91   total_loss: 245.49\n",
      "=> STEP 1736/6250   lr: 0.000863   giou_loss: 1.85   conf_loss: 242.71   prob_loss: 0.90   total_loss: 245.45\n",
      "=> STEP 1737/6250   lr: 0.000863   giou_loss: 1.84   conf_loss: 242.55   prob_loss: 0.89   total_loss: 245.28\n",
      "=> STEP 1738/6250   lr: 0.000863   giou_loss: 1.82   conf_loss: 242.39   prob_loss: 0.89   total_loss: 245.10\n",
      "=> STEP 1739/6250   lr: 0.000863   giou_loss: 1.65   conf_loss: 242.20   prob_loss: 0.90   total_loss: 244.75\n",
      "=> STEP 1740/6250   lr: 0.000863   giou_loss: 1.71   conf_loss: 242.03   prob_loss: 0.91   total_loss: 244.64\n",
      "=> STEP 1741/6250   lr: 0.000862   giou_loss: 1.81   conf_loss: 241.86   prob_loss: 0.91   total_loss: 244.58\n",
      "=> STEP 1742/6250   lr: 0.000862   giou_loss: 1.79   conf_loss: 241.68   prob_loss: 0.91   total_loss: 244.38\n",
      "=> STEP 1743/6250   lr: 0.000862   giou_loss: 1.63   conf_loss: 241.53   prob_loss: 0.91   total_loss: 244.07\n",
      "=> STEP 1744/6250   lr: 0.000862   giou_loss: 1.75   conf_loss: 241.38   prob_loss: 0.90   total_loss: 244.03\n",
      "=> STEP 1745/6250   lr: 0.000862   giou_loss: 1.81   conf_loss: 241.23   prob_loss: 0.89   total_loss: 243.93\n",
      "=> STEP 1746/6250   lr: 0.000862   giou_loss: 1.80   conf_loss: 241.05   prob_loss: 0.89   total_loss: 243.75\n",
      "=> STEP 1747/6250   lr: 0.000861   giou_loss: 1.60   conf_loss: 240.90   prob_loss: 0.90   total_loss: 243.40\n",
      "=> STEP 1748/6250   lr: 0.000861   giou_loss: 1.76   conf_loss: 240.72   prob_loss: 0.91   total_loss: 243.39\n",
      "=> STEP 1749/6250   lr: 0.000861   giou_loss: 1.88   conf_loss: 240.56   prob_loss: 0.92   total_loss: 243.35\n",
      "=> STEP 1750/6250   lr: 0.000861   giou_loss: 1.87   conf_loss: 240.38   prob_loss: 0.92   total_loss: 243.17\n",
      "=> STEP 1751/6250   lr: 0.000861   giou_loss: 1.76   conf_loss: 240.22   prob_loss: 0.92   total_loss: 242.90\n",
      "=> STEP 1752/6250   lr: 0.000861   giou_loss: 1.57   conf_loss: 240.07   prob_loss: 0.91   total_loss: 242.55\n",
      "=> STEP 1753/6250   lr: 0.000860   giou_loss: 1.75   conf_loss: 239.93   prob_loss: 0.90   total_loss: 242.58\n",
      "=> STEP 1754/6250   lr: 0.000860   giou_loss: 1.76   conf_loss: 239.76   prob_loss: 0.90   total_loss: 242.42\n",
      "=> STEP 1755/6250   lr: 0.000860   giou_loss: 1.67   conf_loss: 239.59   prob_loss: 0.90   total_loss: 242.17\n",
      "=> STEP 1756/6250   lr: 0.000860   giou_loss: 1.62   conf_loss: 239.43   prob_loss: 0.91   total_loss: 241.95\n",
      "=> STEP 1757/6250   lr: 0.000860   giou_loss: 1.68   conf_loss: 239.26   prob_loss: 0.91   total_loss: 241.85\n",
      "=> STEP 1758/6250   lr: 0.000859   giou_loss: 1.58   conf_loss: 239.11   prob_loss: 0.90   total_loss: 241.59\n",
      "=> STEP 1759/6250   lr: 0.000859   giou_loss: 1.73   conf_loss: 238.96   prob_loss: 0.90   total_loss: 241.59\n",
      "=> STEP 1760/6250   lr: 0.000859   giou_loss: 1.76   conf_loss: 238.80   prob_loss: 0.90   total_loss: 241.46\n",
      "=> STEP 1761/6250   lr: 0.000859   giou_loss: 1.69   conf_loss: 238.62   prob_loss: 0.90   total_loss: 241.21\n",
      "=> STEP 1762/6250   lr: 0.000859   giou_loss: 1.59   conf_loss: 238.47   prob_loss: 0.91   total_loss: 240.96\n",
      "=> STEP 1763/6250   lr: 0.000859   giou_loss: 1.63   conf_loss: 238.31   prob_loss: 0.90   total_loss: 240.84\n",
      "=> STEP 1764/6250   lr: 0.000858   giou_loss: 1.62   conf_loss: 238.15   prob_loss: 0.90   total_loss: 240.68\n",
      "=> STEP 1765/6250   lr: 0.000858   giou_loss: 1.63   conf_loss: 237.99   prob_loss: 0.90   total_loss: 240.52\n",
      "=> STEP 1766/6250   lr: 0.000858   giou_loss: 1.57   conf_loss: 237.82   prob_loss: 0.90   total_loss: 240.30\n",
      "=> STEP 1767/6250   lr: 0.000858   giou_loss: 1.69   conf_loss: 237.67   prob_loss: 0.90   total_loss: 240.26\n",
      "=> STEP 1768/6250   lr: 0.000858   giou_loss: 1.74   conf_loss: 237.51   prob_loss: 0.90   total_loss: 240.15\n",
      "=> STEP 1769/6250   lr: 0.000858   giou_loss: 1.70   conf_loss: 237.35   prob_loss: 0.90   total_loss: 239.95\n",
      "=> STEP 1770/6250   lr: 0.000857   giou_loss: 1.64   conf_loss: 237.20   prob_loss: 0.90   total_loss: 239.74\n",
      "=> STEP 1771/6250   lr: 0.000857   giou_loss: 1.69   conf_loss: 237.03   prob_loss: 0.90   total_loss: 239.62\n",
      "=> STEP 1772/6250   lr: 0.000857   giou_loss: 1.57   conf_loss: 236.90   prob_loss: 0.89   total_loss: 239.36\n",
      "=> STEP 1773/6250   lr: 0.000857   giou_loss: 1.83   conf_loss: 236.73   prob_loss: 0.89   total_loss: 239.44\n",
      "=> STEP 1774/6250   lr: 0.000857   giou_loss: 1.86   conf_loss: 236.57   prob_loss: 0.89   total_loss: 239.32\n",
      "=> STEP 1775/6250   lr: 0.000856   giou_loss: 1.79   conf_loss: 236.40   prob_loss: 0.90   total_loss: 239.09\n",
      "=> STEP 1776/6250   lr: 0.000856   giou_loss: 1.65   conf_loss: 236.26   prob_loss: 0.90   total_loss: 238.80\n",
      "=> STEP 1777/6250   lr: 0.000856   giou_loss: 1.72   conf_loss: 236.12   prob_loss: 0.89   total_loss: 238.73\n",
      "=> STEP 1778/6250   lr: 0.000856   giou_loss: 1.83   conf_loss: 235.95   prob_loss: 0.89   total_loss: 238.67\n",
      "=> STEP 1779/6250   lr: 0.000856   giou_loss: 1.78   conf_loss: 235.82   prob_loss: 0.89   total_loss: 238.49\n",
      "=> STEP 1780/6250   lr: 0.000856   giou_loss: 1.62   conf_loss: 235.62   prob_loss: 0.90   total_loss: 238.14\n",
      "=> STEP 1781/6250   lr: 0.000855   giou_loss: 1.72   conf_loss: 235.48   prob_loss: 0.90   total_loss: 238.10\n",
      "=> STEP 1782/6250   lr: 0.000855   giou_loss: 1.83   conf_loss: 235.30   prob_loss: 0.91   total_loss: 238.03\n",
      "=> STEP 1783/6250   lr: 0.000855   giou_loss: 1.82   conf_loss: 235.15   prob_loss: 0.91   total_loss: 237.87\n",
      "=> STEP 1784/6250   lr: 0.000855   giou_loss: 1.69   conf_loss: 234.98   prob_loss: 0.90   total_loss: 237.57\n",
      "=> STEP 1785/6250   lr: 0.000855   giou_loss: 1.65   conf_loss: 234.84   prob_loss: 0.90   total_loss: 237.39\n",
      "=> STEP 1786/6250   lr: 0.000854   giou_loss: 1.69   conf_loss: 234.69   prob_loss: 0.89   total_loss: 237.28\n",
      "=> STEP 1787/6250   lr: 0.000854   giou_loss: 1.69   conf_loss: 234.53   prob_loss: 0.89   total_loss: 237.10\n",
      "=> STEP 1788/6250   lr: 0.000854   giou_loss: 1.69   conf_loss: 234.39   prob_loss: 0.89   total_loss: 236.98\n",
      "=> STEP 1789/6250   lr: 0.000854   giou_loss: 1.68   conf_loss: 234.20   prob_loss: 0.90   total_loss: 236.78\n",
      "=> STEP 1790/6250   lr: 0.000854   giou_loss: 1.58   conf_loss: 234.08   prob_loss: 0.90   total_loss: 236.56\n",
      "=> STEP 1791/6250   lr: 0.000854   giou_loss: 1.71   conf_loss: 233.94   prob_loss: 0.89   total_loss: 236.54\n",
      "=> STEP 1792/6250   lr: 0.000853   giou_loss: 1.82   conf_loss: 233.79   prob_loss: 0.88   total_loss: 236.50\n",
      "=> STEP 1793/6250   lr: 0.000853   giou_loss: 1.72   conf_loss: 233.63   prob_loss: 0.89   total_loss: 236.24\n",
      "=> STEP 1794/6250   lr: 0.000853   giou_loss: 1.56   conf_loss: 233.46   prob_loss: 0.90   total_loss: 235.92\n",
      "=> STEP 1795/6250   lr: 0.000853   giou_loss: 1.82   conf_loss: 233.30   prob_loss: 0.90   total_loss: 236.02\n",
      "=> STEP 1796/6250   lr: 0.000853   giou_loss: 1.82   conf_loss: 233.14   prob_loss: 0.90   total_loss: 235.87\n",
      "=> STEP 1797/6250   lr: 0.000853   giou_loss: 1.66   conf_loss: 233.00   prob_loss: 0.89   total_loss: 235.55\n",
      "=> STEP 1798/6250   lr: 0.000852   giou_loss: 1.62   conf_loss: 232.85   prob_loss: 0.89   total_loss: 235.35\n",
      "=> STEP 1799/6250   lr: 0.000852   giou_loss: 1.74   conf_loss: 232.69   prob_loss: 0.88   total_loss: 235.31\n",
      "=> STEP 1800/6250   lr: 0.000852   giou_loss: 1.60   conf_loss: 232.55   prob_loss: 0.89   total_loss: 235.04\n",
      "=> STEP 1801/6250   lr: 0.000852   giou_loss: 1.63   conf_loss: 232.38   prob_loss: 0.90   total_loss: 234.91\n",
      "=> STEP 1802/6250   lr: 0.000852   giou_loss: 1.63   conf_loss: 232.23   prob_loss: 0.90   total_loss: 234.77\n",
      "=> STEP 1803/6250   lr: 0.000851   giou_loss: 1.61   conf_loss: 232.11   prob_loss: 0.89   total_loss: 234.61\n",
      "=> STEP 1804/6250   lr: 0.000851   giou_loss: 1.63   conf_loss: 231.94   prob_loss: 0.89   total_loss: 234.46\n",
      "=> STEP 1805/6250   lr: 0.000851   giou_loss: 1.65   conf_loss: 231.78   prob_loss: 0.89   total_loss: 234.32\n",
      "=> STEP 1806/6250   lr: 0.000851   giou_loss: 1.64   conf_loss: 231.64   prob_loss: 0.89   total_loss: 234.17\n",
      "=> STEP 1807/6250   lr: 0.000851   giou_loss: 1.67   conf_loss: 231.47   prob_loss: 0.89   total_loss: 234.03\n",
      "=> STEP 1808/6250   lr: 0.000851   giou_loss: 1.57   conf_loss: 231.31   prob_loss: 0.89   total_loss: 233.77\n",
      "=> STEP 1809/6250   lr: 0.000850   giou_loss: 1.59   conf_loss: 231.16   prob_loss: 0.88   total_loss: 233.64\n",
      "=> STEP 1810/6250   lr: 0.000850   giou_loss: 1.62   conf_loss: 231.02   prob_loss: 0.88   total_loss: 233.52\n",
      "=> STEP 1811/6250   lr: 0.000850   giou_loss: 1.56   conf_loss: 230.84   prob_loss: 0.88   total_loss: 233.28\n",
      "=> STEP 1812/6250   lr: 0.000850   giou_loss: 1.61   conf_loss: 230.70   prob_loss: 0.89   total_loss: 233.19\n",
      "=> STEP 1813/6250   lr: 0.000850   giou_loss: 1.61   conf_loss: 230.53   prob_loss: 0.89   total_loss: 233.03\n",
      "=> STEP 1814/6250   lr: 0.000849   giou_loss: 1.58   conf_loss: 230.37   prob_loss: 0.89   total_loss: 232.84\n",
      "=> STEP 1815/6250   lr: 0.000849   giou_loss: 1.61   conf_loss: 230.23   prob_loss: 0.89   total_loss: 232.72\n",
      "=> STEP 1816/6250   lr: 0.000849   giou_loss: 1.56   conf_loss: 230.06   prob_loss: 0.88   total_loss: 232.51\n",
      "=> STEP 1817/6250   lr: 0.000849   giou_loss: 1.61   conf_loss: 229.92   prob_loss: 0.88   total_loss: 232.41\n",
      "=> STEP 1818/6250   lr: 0.000849   giou_loss: 1.60   conf_loss: 229.76   prob_loss: 0.88   total_loss: 232.25\n",
      "=> STEP 1819/6250   lr: 0.000849   giou_loss: 1.65   conf_loss: 229.61   prob_loss: 0.88   total_loss: 232.14\n",
      "=> STEP 1820/6250   lr: 0.000848   giou_loss: 1.62   conf_loss: 229.46   prob_loss: 0.88   total_loss: 231.96\n",
      "=> STEP 1821/6250   lr: 0.000848   giou_loss: 1.68   conf_loss: 229.31   prob_loss: 0.88   total_loss: 231.87\n",
      "=> STEP 1822/6250   lr: 0.000848   giou_loss: 1.73   conf_loss: 229.16   prob_loss: 0.88   total_loss: 231.77\n",
      "=> STEP 1823/6250   lr: 0.000848   giou_loss: 1.60   conf_loss: 229.01   prob_loss: 0.88   total_loss: 231.49\n",
      "=> STEP 1824/6250   lr: 0.000848   giou_loss: 1.75   conf_loss: 228.85   prob_loss: 0.88   total_loss: 231.48\n",
      "=> STEP 1825/6250   lr: 0.000847   giou_loss: 1.79   conf_loss: 228.71   prob_loss: 0.88   total_loss: 231.38\n",
      "=> STEP 1826/6250   lr: 0.000847   giou_loss: 1.73   conf_loss: 228.55   prob_loss: 0.88   total_loss: 231.16\n",
      "=> STEP 1827/6250   lr: 0.000847   giou_loss: 1.60   conf_loss: 228.41   prob_loss: 0.88   total_loss: 230.89\n",
      "=> STEP 1828/6250   lr: 0.000847   giou_loss: 1.67   conf_loss: 228.27   prob_loss: 0.87   total_loss: 230.81\n",
      "=> STEP 1829/6250   lr: 0.000847   giou_loss: 1.65   conf_loss: 228.11   prob_loss: 0.87   total_loss: 230.63\n",
      "=> STEP 1830/6250   lr: 0.000847   giou_loss: 1.65   conf_loss: 227.96   prob_loss: 0.87   total_loss: 230.48\n",
      "=> STEP 1831/6250   lr: 0.000846   giou_loss: 1.57   conf_loss: 227.80   prob_loss: 0.88   total_loss: 230.24\n",
      "=> STEP 1832/6250   lr: 0.000846   giou_loss: 1.56   conf_loss: 227.65   prob_loss: 0.88   total_loss: 230.10\n",
      "=> STEP 1833/6250   lr: 0.000846   giou_loss: 1.60   conf_loss: 227.49   prob_loss: 0.89   total_loss: 229.97\n",
      "=> STEP 1834/6250   lr: 0.000846   giou_loss: 1.56   conf_loss: 227.34   prob_loss: 0.88   total_loss: 229.78\n",
      "=> STEP 1835/6250   lr: 0.000846   giou_loss: 1.57   conf_loss: 227.20   prob_loss: 0.87   total_loss: 229.64\n",
      "=> STEP 1836/6250   lr: 0.000845   giou_loss: 1.56   conf_loss: 227.05   prob_loss: 0.87   total_loss: 229.49\n",
      "=> STEP 1837/6250   lr: 0.000845   giou_loss: 1.60   conf_loss: 226.89   prob_loss: 0.88   total_loss: 229.36\n",
      "=> STEP 1838/6250   lr: 0.000845   giou_loss: 1.56   conf_loss: 226.75   prob_loss: 0.87   total_loss: 229.19\n",
      "=> STEP 1839/6250   lr: 0.000845   giou_loss: 1.57   conf_loss: 226.59   prob_loss: 0.87   total_loss: 229.03\n",
      "=> STEP 1840/6250   lr: 0.000845   giou_loss: 1.60   conf_loss: 226.44   prob_loss: 0.87   total_loss: 228.92\n",
      "=> STEP 1841/6250   lr: 0.000844   giou_loss: 1.56   conf_loss: 226.30   prob_loss: 0.88   total_loss: 228.74\n",
      "=> STEP 1842/6250   lr: 0.000844   giou_loss: 1.60   conf_loss: 226.14   prob_loss: 0.88   total_loss: 228.61\n",
      "=> STEP 1843/6250   lr: 0.000844   giou_loss: 1.65   conf_loss: 226.01   prob_loss: 0.87   total_loss: 228.53\n",
      "=> STEP 1844/6250   lr: 0.000844   giou_loss: 1.65   conf_loss: 225.85   prob_loss: 0.87   total_loss: 228.37\n",
      "=> STEP 1845/6250   lr: 0.000844   giou_loss: 1.59   conf_loss: 225.71   prob_loss: 0.87   total_loss: 228.16\n",
      "=> STEP 1846/6250   lr: 0.000844   giou_loss: 1.71   conf_loss: 225.58   prob_loss: 0.87   total_loss: 228.16\n",
      "=> STEP 1847/6250   lr: 0.000843   giou_loss: 1.76   conf_loss: 225.41   prob_loss: 0.87   total_loss: 228.04\n",
      "=> STEP 1848/6250   lr: 0.000843   giou_loss: 1.56   conf_loss: 225.28   prob_loss: 0.87   total_loss: 227.71\n",
      "=> STEP 1849/6250   lr: 0.000843   giou_loss: 1.68   conf_loss: 225.12   prob_loss: 0.87   total_loss: 227.67\n",
      "=> STEP 1850/6250   lr: 0.000843   giou_loss: 1.67   conf_loss: 224.97   prob_loss: 0.87   total_loss: 227.51\n",
      "=> STEP 1851/6250   lr: 0.000843   giou_loss: 1.56   conf_loss: 224.83   prob_loss: 0.86   total_loss: 227.26\n",
      "=> STEP 1852/6250   lr: 0.000842   giou_loss: 1.70   conf_loss: 224.69   prob_loss: 0.86   total_loss: 227.25\n",
      "=> STEP 1853/6250   lr: 0.000842   giou_loss: 1.67   conf_loss: 224.54   prob_loss: 0.86   total_loss: 227.07\n",
      "=> STEP 1854/6250   lr: 0.000842   giou_loss: 1.58   conf_loss: 224.38   prob_loss: 0.87   total_loss: 226.83\n",
      "=> STEP 1855/6250   lr: 0.000842   giou_loss: 1.70   conf_loss: 224.23   prob_loss: 0.88   total_loss: 226.81\n",
      "=> STEP 1856/6250   lr: 0.000842   giou_loss: 1.78   conf_loss: 224.08   prob_loss: 0.88   total_loss: 226.74\n",
      "=> STEP 1857/6250   lr: 0.000842   giou_loss: 1.66   conf_loss: 223.96   prob_loss: 0.87   total_loss: 226.49\n",
      "=> STEP 1858/6250   lr: 0.000841   giou_loss: 1.66   conf_loss: 223.82   prob_loss: 0.86   total_loss: 226.34\n",
      "=> STEP 1859/6250   lr: 0.000841   giou_loss: 1.69   conf_loss: 223.69   prob_loss: 0.85   total_loss: 226.23\n",
      "=> STEP 1860/6250   lr: 0.000841   giou_loss: 1.69   conf_loss: 223.55   prob_loss: 0.86   total_loss: 226.10\n",
      "=> STEP 1861/6250   lr: 0.000841   giou_loss: 1.62   conf_loss: 223.38   prob_loss: 0.87   total_loss: 225.86\n",
      "=> STEP 1862/6250   lr: 0.000841   giou_loss: 1.69   conf_loss: 223.26   prob_loss: 0.87   total_loss: 225.82\n",
      "=> STEP 1863/6250   lr: 0.000840   giou_loss: 1.64   conf_loss: 223.11   prob_loss: 0.87   total_loss: 225.62\n",
      "=> STEP 1864/6250   lr: 0.000840   giou_loss: 1.68   conf_loss: 222.95   prob_loss: 0.87   total_loss: 225.51\n",
      "=> STEP 1865/6250   lr: 0.000840   giou_loss: 1.81   conf_loss: 222.81   prob_loss: 0.86   total_loss: 225.49\n",
      "=> STEP 1866/6250   lr: 0.000840   giou_loss: 1.70   conf_loss: 222.67   prob_loss: 0.86   total_loss: 225.23\n",
      "=> STEP 1867/6250   lr: 0.000840   giou_loss: 1.73   conf_loss: 222.52   prob_loss: 0.86   total_loss: 225.11\n",
      "=> STEP 1868/6250   lr: 0.000839   giou_loss: 1.86   conf_loss: 222.38   prob_loss: 0.86   total_loss: 225.10\n",
      "=> STEP 1869/6250   lr: 0.000839   giou_loss: 1.79   conf_loss: 222.23   prob_loss: 0.87   total_loss: 224.88\n",
      "=> STEP 1870/6250   lr: 0.000839   giou_loss: 1.57   conf_loss: 222.08   prob_loss: 0.87   total_loss: 224.51\n",
      "=> STEP 1871/6250   lr: 0.000839   giou_loss: 1.80   conf_loss: 221.94   prob_loss: 0.87   total_loss: 224.60\n",
      "=> STEP 1872/6250   lr: 0.000839   giou_loss: 1.80   conf_loss: 221.79   prob_loss: 0.87   total_loss: 224.45\n",
      "=> STEP 1873/6250   lr: 0.000839   giou_loss: 1.57   conf_loss: 221.64   prob_loss: 0.87   total_loss: 224.07\n",
      "=> STEP 1874/6250   lr: 0.000838   giou_loss: 1.80   conf_loss: 221.51   prob_loss: 0.87   total_loss: 224.18\n",
      "=> STEP 1875/6250   lr: 0.000838   giou_loss: 1.87   conf_loss: 221.37   prob_loss: 0.87   total_loss: 224.11\n",
      "=> STEP 1876/6250   lr: 0.000838   giou_loss: 1.79   conf_loss: 221.22   prob_loss: 0.87   total_loss: 223.87\n",
      "=> STEP 1877/6250   lr: 0.000838   giou_loss: 1.69   conf_loss: 221.08   prob_loss: 0.87   total_loss: 223.63\n",
      "=> STEP 1878/6250   lr: 0.000838   giou_loss: 1.78   conf_loss: 220.93   prob_loss: 0.87   total_loss: 223.59\n",
      "=> STEP 1879/6250   lr: 0.000837   giou_loss: 1.84   conf_loss: 220.78   prob_loss: 0.88   total_loss: 223.50\n",
      "=> STEP 1880/6250   lr: 0.000837   giou_loss: 1.91   conf_loss: 220.64   prob_loss: 0.88   total_loss: 223.44\n",
      "=> STEP 1881/6250   lr: 0.000837   giou_loss: 1.81   conf_loss: 220.51   prob_loss: 0.87   total_loss: 223.18\n",
      "=> STEP 1882/6250   lr: 0.000837   giou_loss: 1.60   conf_loss: 220.37   prob_loss: 0.86   total_loss: 222.84\n",
      "=> STEP 1883/6250   lr: 0.000837   giou_loss: 1.76   conf_loss: 220.25   prob_loss: 0.85   total_loss: 222.86\n",
      "=> STEP 1884/6250   lr: 0.000837   giou_loss: 1.78   conf_loss: 220.11   prob_loss: 0.85   total_loss: 222.74\n",
      "=> STEP 1885/6250   lr: 0.000836   giou_loss: 1.78   conf_loss: 219.95   prob_loss: 0.86   total_loss: 222.59\n",
      "=> STEP 1886/6250   lr: 0.000836   giou_loss: 1.57   conf_loss: 219.80   prob_loss: 0.87   total_loss: 222.24\n",
      "=> STEP 1887/6250   lr: 0.000836   giou_loss: 1.77   conf_loss: 219.65   prob_loss: 0.89   total_loss: 222.31\n",
      "=> STEP 1888/6250   lr: 0.000836   giou_loss: 1.83   conf_loss: 219.51   prob_loss: 0.89   total_loss: 222.23\n",
      "=> STEP 1889/6250   lr: 0.000836   giou_loss: 1.82   conf_loss: 219.38   prob_loss: 0.88   total_loss: 222.08\n",
      "=> STEP 1890/6250   lr: 0.000835   giou_loss: 1.69   conf_loss: 219.25   prob_loss: 0.86   total_loss: 221.81\n",
      "=> STEP 1891/6250   lr: 0.000835   giou_loss: 1.57   conf_loss: 219.12   prob_loss: 0.85   total_loss: 221.55\n",
      "=> STEP 1892/6250   lr: 0.000835   giou_loss: 1.67   conf_loss: 218.99   prob_loss: 0.85   total_loss: 221.51\n",
      "=> STEP 1893/6250   lr: 0.000835   giou_loss: 1.57   conf_loss: 218.84   prob_loss: 0.86   total_loss: 221.26\n",
      "=> STEP 1894/6250   lr: 0.000835   giou_loss: 1.63   conf_loss: 218.68   prob_loss: 0.87   total_loss: 221.17\n",
      "=> STEP 1895/6250   lr: 0.000834   giou_loss: 1.57   conf_loss: 218.55   prob_loss: 0.87   total_loss: 220.99\n",
      "=> STEP 1896/6250   lr: 0.000834   giou_loss: 1.56   conf_loss: 218.39   prob_loss: 0.86   total_loss: 220.82\n",
      "=> STEP 1897/6250   lr: 0.000834   giou_loss: 1.60   conf_loss: 218.27   prob_loss: 0.86   total_loss: 220.73\n",
      "=> STEP 1898/6250   lr: 0.000834   giou_loss: 1.56   conf_loss: 218.13   prob_loss: 0.86   total_loss: 220.56\n",
      "=> STEP 1899/6250   lr: 0.000834   giou_loss: 1.64   conf_loss: 217.98   prob_loss: 0.86   total_loss: 220.47\n",
      "=> STEP 1900/6250   lr: 0.000833   giou_loss: 1.61   conf_loss: 217.85   prob_loss: 0.86   total_loss: 220.32\n",
      "=> STEP 1901/6250   lr: 0.000833   giou_loss: 1.56   conf_loss: 217.71   prob_loss: 0.86   total_loss: 220.13\n",
      "=> STEP 1902/6250   lr: 0.000833   giou_loss: 1.58   conf_loss: 217.57   prob_loss: 0.86   total_loss: 220.01\n",
      "=> STEP 1903/6250   lr: 0.000833   giou_loss: 1.66   conf_loss: 217.42   prob_loss: 0.86   total_loss: 219.94\n",
      "=> STEP 1904/6250   lr: 0.000833   giou_loss: 1.57   conf_loss: 217.29   prob_loss: 0.86   total_loss: 219.71\n",
      "=> STEP 1905/6250   lr: 0.000833   giou_loss: 1.57   conf_loss: 217.14   prob_loss: 0.86   total_loss: 219.56\n",
      "=> STEP 1906/6250   lr: 0.000832   giou_loss: 1.65   conf_loss: 217.00   prob_loss: 0.86   total_loss: 219.51\n",
      "=> STEP 1907/6250   lr: 0.000832   giou_loss: 1.56   conf_loss: 216.85   prob_loss: 0.86   total_loss: 219.28\n",
      "=> STEP 1908/6250   lr: 0.000832   giou_loss: 1.68   conf_loss: 216.73   prob_loss: 0.86   total_loss: 219.27\n",
      "=> STEP 1909/6250   lr: 0.000832   giou_loss: 1.70   conf_loss: 216.58   prob_loss: 0.85   total_loss: 219.13\n",
      "=> STEP 1910/6250   lr: 0.000832   giou_loss: 1.56   conf_loss: 216.46   prob_loss: 0.86   total_loss: 218.88\n",
      "=> STEP 1911/6250   lr: 0.000831   giou_loss: 1.61   conf_loss: 216.30   prob_loss: 0.86   total_loss: 218.77\n",
      "=> STEP 1912/6250   lr: 0.000831   giou_loss: 1.57   conf_loss: 216.19   prob_loss: 0.86   total_loss: 218.61\n",
      "=> STEP 1913/6250   lr: 0.000831   giou_loss: 1.57   conf_loss: 216.03   prob_loss: 0.85   total_loss: 218.46\n",
      "=> STEP 1914/6250   lr: 0.000831   giou_loss: 1.57   conf_loss: 215.91   prob_loss: 0.86   total_loss: 218.34\n",
      "=> STEP 1915/6250   lr: 0.000831   giou_loss: 1.64   conf_loss: 215.76   prob_loss: 0.86   total_loss: 218.26\n",
      "=> STEP 1916/6250   lr: 0.000830   giou_loss: 1.56   conf_loss: 215.63   prob_loss: 0.86   total_loss: 218.05\n",
      "=> STEP 1917/6250   lr: 0.000830   giou_loss: 1.57   conf_loss: 215.50   prob_loss: 0.85   total_loss: 217.92\n",
      "=> STEP 1918/6250   lr: 0.000830   giou_loss: 1.57   conf_loss: 215.33   prob_loss: 0.85   total_loss: 217.75\n",
      "=> STEP 1919/6250   lr: 0.000830   giou_loss: 1.58   conf_loss: 215.24   prob_loss: 0.86   total_loss: 217.68\n",
      "=> STEP 1920/6250   lr: 0.000830   giou_loss: 1.65   conf_loss: 215.05   prob_loss: 0.86   total_loss: 217.56\n",
      "=> STEP 1921/6250   lr: 0.000829   giou_loss: 1.56   conf_loss: 214.97   prob_loss: 0.86   total_loss: 217.39\n",
      "=> STEP 1922/6250   lr: 0.000829   giou_loss: 1.56   conf_loss: 214.79   prob_loss: 0.85   total_loss: 217.21\n",
      "=> STEP 1923/6250   lr: 0.000829   giou_loss: 1.56   conf_loss: 214.67   prob_loss: 0.85   total_loss: 217.08\n",
      "=> STEP 1924/6250   lr: 0.000829   giou_loss: 1.57   conf_loss: 214.53   prob_loss: 0.85   total_loss: 216.95\n",
      "=> STEP 1925/6250   lr: 0.000829   giou_loss: 1.58   conf_loss: 214.37   prob_loss: 0.85   total_loss: 216.80\n",
      "=> STEP 1926/6250   lr: 0.000829   giou_loss: 1.61   conf_loss: 214.24   prob_loss: 0.86   total_loss: 216.71\n",
      "=> STEP 1927/6250   lr: 0.000828   giou_loss: 1.68   conf_loss: 214.09   prob_loss: 0.86   total_loss: 216.63\n",
      "=> STEP 1928/6250   lr: 0.000828   giou_loss: 1.74   conf_loss: 213.96   prob_loss: 0.85   total_loss: 216.56\n",
      "=> STEP 1929/6250   lr: 0.000828   giou_loss: 1.64   conf_loss: 213.83   prob_loss: 0.85   total_loss: 216.32\n",
      "=> STEP 1930/6250   lr: 0.000828   giou_loss: 1.71   conf_loss: 213.69   prob_loss: 0.85   total_loss: 216.25\n",
      "=> STEP 1931/6250   lr: 0.000828   giou_loss: 1.69   conf_loss: 213.55   prob_loss: 0.85   total_loss: 216.09\n",
      "=> STEP 1932/6250   lr: 0.000827   giou_loss: 1.65   conf_loss: 213.42   prob_loss: 0.85   total_loss: 215.92\n",
      "=> STEP 1933/6250   lr: 0.000827   giou_loss: 1.72   conf_loss: 213.29   prob_loss: 0.85   total_loss: 215.86\n",
      "=> STEP 1934/6250   lr: 0.000827   giou_loss: 1.59   conf_loss: 213.15   prob_loss: 0.86   total_loss: 215.59\n",
      "=> STEP 1935/6250   lr: 0.000827   giou_loss: 1.74   conf_loss: 213.02   prob_loss: 0.85   total_loss: 215.62\n",
      "=> STEP 1936/6250   lr: 0.000827   giou_loss: 1.74   conf_loss: 212.88   prob_loss: 0.85   total_loss: 215.47\n",
      "=> STEP 1937/6250   lr: 0.000826   giou_loss: 1.56   conf_loss: 212.76   prob_loss: 0.85   total_loss: 215.17\n",
      "=> STEP 1938/6250   lr: 0.000826   giou_loss: 1.74   conf_loss: 212.61   prob_loss: 0.85   total_loss: 215.20\n",
      "=> STEP 1939/6250   lr: 0.000826   giou_loss: 1.67   conf_loss: 212.49   prob_loss: 0.85   total_loss: 215.01\n",
      "=> STEP 1940/6250   lr: 0.000826   giou_loss: 1.63   conf_loss: 212.35   prob_loss: 0.85   total_loss: 214.83\n",
      "=> STEP 1941/6250   lr: 0.000826   giou_loss: 1.64   conf_loss: 212.21   prob_loss: 0.85   total_loss: 214.70\n",
      "=> STEP 1942/6250   lr: 0.000825   giou_loss: 1.61   conf_loss: 212.10   prob_loss: 0.85   total_loss: 214.56\n",
      "=> STEP 1943/6250   lr: 0.000825   giou_loss: 1.58   conf_loss: 211.94   prob_loss: 0.85   total_loss: 214.37\n",
      "=> STEP 1944/6250   lr: 0.000825   giou_loss: 1.65   conf_loss: 211.83   prob_loss: 0.85   total_loss: 214.33\n",
      "=> STEP 1945/6250   lr: 0.000825   giou_loss: 1.66   conf_loss: 211.67   prob_loss: 0.85   total_loss: 214.18\n",
      "=> STEP 1946/6250   lr: 0.000825   giou_loss: 1.57   conf_loss: 211.55   prob_loss: 0.85   total_loss: 213.97\n",
      "=> STEP 1947/6250   lr: 0.000825   giou_loss: 1.65   conf_loss: 211.42   prob_loss: 0.84   total_loss: 213.91\n",
      "=> STEP 1948/6250   lr: 0.000824   giou_loss: 1.60   conf_loss: 211.28   prob_loss: 0.84   total_loss: 213.73\n",
      "=> STEP 1949/6250   lr: 0.000824   giou_loss: 1.67   conf_loss: 211.14   prob_loss: 0.85   total_loss: 213.66\n",
      "=> STEP 1950/6250   lr: 0.000824   giou_loss: 1.66   conf_loss: 211.01   prob_loss: 0.85   total_loss: 213.53\n",
      "=> STEP 1951/6250   lr: 0.000824   giou_loss: 1.69   conf_loss: 210.89   prob_loss: 0.85   total_loss: 213.43\n",
      "=> STEP 1952/6250   lr: 0.000824   giou_loss: 1.72   conf_loss: 210.74   prob_loss: 0.85   total_loss: 213.31\n",
      "=> STEP 1953/6250   lr: 0.000823   giou_loss: 1.57   conf_loss: 210.64   prob_loss: 0.84   total_loss: 213.05\n",
      "=> STEP 1954/6250   lr: 0.000823   giou_loss: 1.80   conf_loss: 210.48   prob_loss: 0.84   total_loss: 213.12\n",
      "=> STEP 1955/6250   lr: 0.000823   giou_loss: 1.84   conf_loss: 210.37   prob_loss: 0.84   total_loss: 213.04\n",
      "=> STEP 1956/6250   lr: 0.000823   giou_loss: 1.65   conf_loss: 210.23   prob_loss: 0.84   total_loss: 212.72\n",
      "=> STEP 1957/6250   lr: 0.000823   giou_loss: 1.64   conf_loss: 210.09   prob_loss: 0.84   total_loss: 212.57\n",
      "=> STEP 1958/6250   lr: 0.000822   giou_loss: 1.67   conf_loss: 209.99   prob_loss: 0.84   total_loss: 212.50\n",
      "=> STEP 1959/6250   lr: 0.000822   giou_loss: 1.56   conf_loss: 209.83   prob_loss: 0.84   total_loss: 212.23\n",
      "=> STEP 1960/6250   lr: 0.000822   giou_loss: 1.61   conf_loss: 209.71   prob_loss: 0.84   total_loss: 212.16\n",
      "=> STEP 1961/6250   lr: 0.000822   giou_loss: 1.58   conf_loss: 209.58   prob_loss: 0.84   total_loss: 212.00\n",
      "=> STEP 1962/6250   lr: 0.000822   giou_loss: 1.58   conf_loss: 209.44   prob_loss: 0.84   total_loss: 211.85\n",
      "=> STEP 1963/6250   lr: 0.000821   giou_loss: 1.66   conf_loss: 209.31   prob_loss: 0.85   total_loss: 211.82\n",
      "=> STEP 1964/6250   lr: 0.000821   giou_loss: 1.59   conf_loss: 209.17   prob_loss: 0.84   total_loss: 211.61\n",
      "=> STEP 1965/6250   lr: 0.000821   giou_loss: 1.60   conf_loss: 209.07   prob_loss: 0.84   total_loss: 211.50\n",
      "=> STEP 1966/6250   lr: 0.000821   giou_loss: 1.65   conf_loss: 208.92   prob_loss: 0.84   total_loss: 211.41\n",
      "=> STEP 1967/6250   lr: 0.000821   giou_loss: 1.56   conf_loss: 208.79   prob_loss: 0.84   total_loss: 211.20\n",
      "=> STEP 1968/6250   lr: 0.000820   giou_loss: 1.64   conf_loss: 208.66   prob_loss: 0.85   total_loss: 211.15\n",
      "=> STEP 1969/6250   lr: 0.000820   giou_loss: 1.62   conf_loss: 208.54   prob_loss: 0.84   total_loss: 211.00\n",
      "=> STEP 1970/6250   lr: 0.000820   giou_loss: 1.64   conf_loss: 208.41   prob_loss: 0.84   total_loss: 210.89\n",
      "=> STEP 1971/6250   lr: 0.000820   giou_loss: 1.61   conf_loss: 208.28   prob_loss: 0.83   total_loss: 210.73\n",
      "=> STEP 1972/6250   lr: 0.000820   giou_loss: 1.68   conf_loss: 208.16   prob_loss: 0.84   total_loss: 210.67\n",
      "=> STEP 1973/6250   lr: 0.000819   giou_loss: 1.79   conf_loss: 208.01   prob_loss: 0.84   total_loss: 210.65\n",
      "=> STEP 1974/6250   lr: 0.000819   giou_loss: 1.64   conf_loss: 207.90   prob_loss: 0.85   total_loss: 210.38\n",
      "=> STEP 1975/6250   lr: 0.000819   giou_loss: 1.73   conf_loss: 207.76   prob_loss: 0.85   total_loss: 210.34\n",
      "=> STEP 1976/6250   lr: 0.000819   giou_loss: 1.82   conf_loss: 207.63   prob_loss: 0.85   total_loss: 210.30\n",
      "=> STEP 1977/6250   lr: 0.000819   giou_loss: 1.76   conf_loss: 207.51   prob_loss: 0.85   total_loss: 210.12\n",
      "=> STEP 1978/6250   lr: 0.000818   giou_loss: 1.64   conf_loss: 207.36   prob_loss: 0.84   total_loss: 209.85\n",
      "=> STEP 1979/6250   lr: 0.000818   giou_loss: 1.78   conf_loss: 207.28   prob_loss: 0.83   total_loss: 209.89\n",
      "=> STEP 1980/6250   lr: 0.000818   giou_loss: 1.89   conf_loss: 207.15   prob_loss: 0.82   total_loss: 209.87\n",
      "=> STEP 1981/6250   lr: 0.000818   giou_loss: 1.89   conf_loss: 207.01   prob_loss: 0.83   total_loss: 209.73\n",
      "=> STEP 1982/6250   lr: 0.000818   giou_loss: 1.74   conf_loss: 206.89   prob_loss: 0.84   total_loss: 209.47\n",
      "=> STEP 1983/6250   lr: 0.000818   giou_loss: 1.56   conf_loss: 206.72   prob_loss: 0.85   total_loss: 209.14\n",
      "=> STEP 1984/6250   lr: 0.000817   giou_loss: 1.77   conf_loss: 206.63   prob_loss: 0.85   total_loss: 209.26\n",
      "=> STEP 1985/6250   lr: 0.000817   giou_loss: 1.87   conf_loss: 206.47   prob_loss: 0.84   total_loss: 209.19\n",
      "=> STEP 1986/6250   lr: 0.000817   giou_loss: 1.78   conf_loss: 206.36   prob_loss: 0.84   total_loss: 208.98\n",
      "=> STEP 1987/6250   lr: 0.000817   giou_loss: 1.61   conf_loss: 206.24   prob_loss: 0.84   total_loss: 208.69\n",
      "=> STEP 1988/6250   lr: 0.000817   giou_loss: 1.78   conf_loss: 206.09   prob_loss: 0.83   total_loss: 208.71\n",
      "=> STEP 1989/6250   lr: 0.000816   giou_loss: 1.86   conf_loss: 206.01   prob_loss: 0.83   total_loss: 208.70\n",
      "=> STEP 1990/6250   lr: 0.000816   giou_loss: 1.85   conf_loss: 205.83   prob_loss: 0.83   total_loss: 208.52\n",
      "=> STEP 1991/6250   lr: 0.000816   giou_loss: 1.75   conf_loss: 205.71   prob_loss: 0.84   total_loss: 208.30\n",
      "=> STEP 1992/6250   lr: 0.000816   giou_loss: 1.56   conf_loss: 205.59   prob_loss: 0.85   total_loss: 208.00\n",
      "=> STEP 1993/6250   lr: 0.000816   giou_loss: 1.75   conf_loss: 205.41   prob_loss: 0.85   total_loss: 208.01\n",
      "=> STEP 1994/6250   lr: 0.000815   giou_loss: 1.82   conf_loss: 205.31   prob_loss: 0.85   total_loss: 207.98\n",
      "=> STEP 1995/6250   lr: 0.000815   giou_loss: 1.73   conf_loss: 205.17   prob_loss: 0.85   total_loss: 207.75\n",
      "=> STEP 1996/6250   lr: 0.000815   giou_loss: 1.56   conf_loss: 205.04   prob_loss: 0.84   total_loss: 207.45\n",
      "=> STEP 1997/6250   lr: 0.000815   giou_loss: 1.67   conf_loss: 204.94   prob_loss: 0.83   total_loss: 207.44\n",
      "=> STEP 1998/6250   lr: 0.000815   giou_loss: 1.67   conf_loss: 204.77   prob_loss: 0.83   total_loss: 207.27\n",
      "=> STEP 1999/6250   lr: 0.000814   giou_loss: 1.61   conf_loss: 204.66   prob_loss: 0.84   total_loss: 207.10\n",
      "=> STEP 2000/6250   lr: 0.000814   giou_loss: 1.63   conf_loss: 204.50   prob_loss: 0.84   total_loss: 206.98\n",
      "=> STEP 2001/6250   lr: 0.000814   giou_loss: 1.63   conf_loss: 204.36   prob_loss: 0.85   total_loss: 206.84\n",
      "=> STEP 2002/6250   lr: 0.000814   giou_loss: 1.65   conf_loss: 204.25   prob_loss: 0.85   total_loss: 206.75\n",
      "=> STEP 2003/6250   lr: 0.000814   giou_loss: 1.68   conf_loss: 204.12   prob_loss: 0.84   total_loss: 206.64\n",
      "=> STEP 2004/6250   lr: 0.000813   giou_loss: 1.65   conf_loss: 203.99   prob_loss: 0.84   total_loss: 206.48\n",
      "=> STEP 2005/6250   lr: 0.000813   giou_loss: 1.66   conf_loss: 203.87   prob_loss: 0.84   total_loss: 206.37\n",
      "=> STEP 2006/6250   lr: 0.000813   giou_loss: 1.62   conf_loss: 203.73   prob_loss: 0.84   total_loss: 206.20\n",
      "=> STEP 2007/6250   lr: 0.000813   giou_loss: 1.61   conf_loss: 203.62   prob_loss: 0.84   total_loss: 206.07\n",
      "=> STEP 2008/6250   lr: 0.000813   giou_loss: 1.61   conf_loss: 203.50   prob_loss: 0.84   total_loss: 205.95\n",
      "=> STEP 2009/6250   lr: 0.000812   giou_loss: 1.59   conf_loss: 203.37   prob_loss: 0.84   total_loss: 205.80\n",
      "=> STEP 2010/6250   lr: 0.000812   giou_loss: 1.57   conf_loss: 203.22   prob_loss: 0.84   total_loss: 205.63\n",
      "=> STEP 2011/6250   lr: 0.000812   giou_loss: 1.68   conf_loss: 203.13   prob_loss: 0.84   total_loss: 205.64\n",
      "=> STEP 2012/6250   lr: 0.000812   giou_loss: 1.66   conf_loss: 202.98   prob_loss: 0.84   total_loss: 205.48\n",
      "=> STEP 2013/6250   lr: 0.000812   giou_loss: 1.56   conf_loss: 202.85   prob_loss: 0.83   total_loss: 205.25\n",
      "=> STEP 2014/6250   lr: 0.000811   giou_loss: 1.66   conf_loss: 202.73   prob_loss: 0.83   total_loss: 205.22\n",
      "=> STEP 2015/6250   lr: 0.000811   giou_loss: 1.62   conf_loss: 202.60   prob_loss: 0.83   total_loss: 205.05\n",
      "=> STEP 2016/6250   lr: 0.000811   giou_loss: 1.61   conf_loss: 202.49   prob_loss: 0.83   total_loss: 204.93\n",
      "=> STEP 2017/6250   lr: 0.000811   giou_loss: 1.61   conf_loss: 202.34   prob_loss: 0.83   total_loss: 204.78\n",
      "=> STEP 2018/6250   lr: 0.000811   giou_loss: 1.59   conf_loss: 202.23   prob_loss: 0.83   total_loss: 204.66\n",
      "=> STEP 2019/6250   lr: 0.000810   giou_loss: 1.57   conf_loss: 202.09   prob_loss: 0.83   total_loss: 204.50\n",
      "=> STEP 2020/6250   lr: 0.000810   giou_loss: 1.64   conf_loss: 201.97   prob_loss: 0.83   total_loss: 204.45\n",
      "=> STEP 2021/6250   lr: 0.000810   giou_loss: 1.64   conf_loss: 201.84   prob_loss: 0.83   total_loss: 204.32\n",
      "=> STEP 2022/6250   lr: 0.000810   giou_loss: 1.57   conf_loss: 201.71   prob_loss: 0.84   total_loss: 204.11\n",
      "=> STEP 2023/6250   lr: 0.000810   giou_loss: 1.68   conf_loss: 201.59   prob_loss: 0.84   total_loss: 204.10\n",
      "=> STEP 2024/6250   lr: 0.000809   giou_loss: 1.65   conf_loss: 201.46   prob_loss: 0.83   total_loss: 203.94\n",
      "=> STEP 2025/6250   lr: 0.000809   giou_loss: 1.57   conf_loss: 201.35   prob_loss: 0.83   total_loss: 203.75\n",
      "=> STEP 2026/6250   lr: 0.000809   giou_loss: 1.56   conf_loss: 201.21   prob_loss: 0.83   total_loss: 203.61\n",
      "=> STEP 2027/6250   lr: 0.000809   giou_loss: 1.65   conf_loss: 201.09   prob_loss: 0.83   total_loss: 203.57\n",
      "=> STEP 2028/6250   lr: 0.000809   giou_loss: 1.64   conf_loss: 200.96   prob_loss: 0.83   total_loss: 203.43\n",
      "=> STEP 2029/6250   lr: 0.000808   giou_loss: 1.56   conf_loss: 200.84   prob_loss: 0.83   total_loss: 203.24\n",
      "=> STEP 2030/6250   lr: 0.000808   giou_loss: 1.70   conf_loss: 200.72   prob_loss: 0.83   total_loss: 203.25\n",
      "=> STEP 2031/6250   lr: 0.000808   giou_loss: 1.64   conf_loss: 200.60   prob_loss: 0.82   total_loss: 203.07\n",
      "=> STEP 2032/6250   lr: 0.000808   giou_loss: 1.69   conf_loss: 200.47   prob_loss: 0.83   total_loss: 202.99\n",
      "=> STEP 2033/6250   lr: 0.000808   giou_loss: 1.66   conf_loss: 200.34   prob_loss: 0.83   total_loss: 202.84\n",
      "=> STEP 2034/6250   lr: 0.000807   giou_loss: 1.59   conf_loss: 200.22   prob_loss: 0.83   total_loss: 202.64\n",
      "=> STEP 2035/6250   lr: 0.000807   giou_loss: 1.57   conf_loss: 200.10   prob_loss: 0.83   total_loss: 202.50\n",
      "=> STEP 2036/6250   lr: 0.000807   giou_loss: 1.58   conf_loss: 199.98   prob_loss: 0.83   total_loss: 202.38\n",
      "=> STEP 2037/6250   lr: 0.000807   giou_loss: 1.58   conf_loss: 199.86   prob_loss: 0.83   total_loss: 202.27\n",
      "=> STEP 2038/6250   lr: 0.000807   giou_loss: 1.60   conf_loss: 199.73   prob_loss: 0.83   total_loss: 202.16\n",
      "=> STEP 2039/6250   lr: 0.000806   giou_loss: 1.56   conf_loss: 199.61   prob_loss: 0.83   total_loss: 202.00\n",
      "=> STEP 2040/6250   lr: 0.000806   giou_loss: 1.56   conf_loss: 199.48   prob_loss: 0.83   total_loss: 201.87\n",
      "=> STEP 2041/6250   lr: 0.000806   giou_loss: 1.56   conf_loss: 199.36   prob_loss: 0.83   total_loss: 201.75\n",
      "=> STEP 2042/6250   lr: 0.000806   giou_loss: 1.59   conf_loss: 199.23   prob_loss: 0.83   total_loss: 201.65\n",
      "=> STEP 2043/6250   lr: 0.000806   giou_loss: 1.62   conf_loss: 199.11   prob_loss: 0.83   total_loss: 201.56\n",
      "=> STEP 2044/6250   lr: 0.000805   giou_loss: 1.57   conf_loss: 199.00   prob_loss: 0.82   total_loss: 201.39\n",
      "=> STEP 2045/6250   lr: 0.000805   giou_loss: 1.66   conf_loss: 198.88   prob_loss: 0.82   total_loss: 201.36\n",
      "=> STEP 2046/6250   lr: 0.000805   giou_loss: 1.61   conf_loss: 198.76   prob_loss: 0.82   total_loss: 201.19\n",
      "=> STEP 2047/6250   lr: 0.000805   giou_loss: 1.65   conf_loss: 198.64   prob_loss: 0.83   total_loss: 201.12\n",
      "=> STEP 2048/6250   lr: 0.000805   giou_loss: 1.64   conf_loss: 198.51   prob_loss: 0.83   total_loss: 200.98\n",
      "=> STEP 2049/6250   lr: 0.000804   giou_loss: 1.66   conf_loss: 198.40   prob_loss: 0.83   total_loss: 200.89\n",
      "=> STEP 2050/6250   lr: 0.000804   giou_loss: 1.75   conf_loss: 198.27   prob_loss: 0.82   total_loss: 200.84\n",
      "=> STEP 2051/6250   lr: 0.000804   giou_loss: 1.58   conf_loss: 198.17   prob_loss: 0.82   total_loss: 200.57\n",
      "=> STEP 2052/6250   lr: 0.000804   giou_loss: 1.65   conf_loss: 198.05   prob_loss: 0.82   total_loss: 200.53\n",
      "=> STEP 2053/6250   lr: 0.000804   giou_loss: 1.61   conf_loss: 197.91   prob_loss: 0.83   total_loss: 200.35\n",
      "=> STEP 2054/6250   lr: 0.000803   giou_loss: 1.57   conf_loss: 197.82   prob_loss: 0.82   total_loss: 200.21\n",
      "=> STEP 2055/6250   lr: 0.000803   giou_loss: 1.67   conf_loss: 197.68   prob_loss: 0.82   total_loss: 200.17\n",
      "=> STEP 2056/6250   lr: 0.000803   giou_loss: 1.58   conf_loss: 197.57   prob_loss: 0.82   total_loss: 199.98\n",
      "=> STEP 2057/6250   lr: 0.000803   giou_loss: 1.71   conf_loss: 197.45   prob_loss: 0.83   total_loss: 199.99\n",
      "=> STEP 2058/6250   lr: 0.000803   giou_loss: 1.67   conf_loss: 197.31   prob_loss: 0.83   total_loss: 199.81\n",
      "=> STEP 2059/6250   lr: 0.000802   giou_loss: 1.64   conf_loss: 197.22   prob_loss: 0.83   total_loss: 199.68\n",
      "=> STEP 2060/6250   lr: 0.000802   giou_loss: 1.72   conf_loss: 197.08   prob_loss: 0.82   total_loss: 199.62\n",
      "=> STEP 2061/6250   lr: 0.000802   giou_loss: 1.57   conf_loss: 196.97   prob_loss: 0.82   total_loss: 199.36\n",
      "=> STEP 2062/6250   lr: 0.000802   giou_loss: 1.59   conf_loss: 196.85   prob_loss: 0.82   total_loss: 199.27\n",
      "=> STEP 2063/6250   lr: 0.000802   giou_loss: 1.56   conf_loss: 196.71   prob_loss: 0.83   total_loss: 199.10\n",
      "=> STEP 2064/6250   lr: 0.000801   giou_loss: 1.57   conf_loss: 196.60   prob_loss: 0.83   total_loss: 199.00\n",
      "=> STEP 2065/6250   lr: 0.000801   giou_loss: 1.60   conf_loss: 196.48   prob_loss: 0.82   total_loss: 198.90\n",
      "=> STEP 2066/6250   lr: 0.000801   giou_loss: 1.57   conf_loss: 196.36   prob_loss: 0.82   total_loss: 198.74\n",
      "=> STEP 2067/6250   lr: 0.000801   giou_loss: 1.68   conf_loss: 196.25   prob_loss: 0.82   total_loss: 198.74\n",
      "=> STEP 2068/6250   lr: 0.000801   giou_loss: 1.59   conf_loss: 196.12   prob_loss: 0.82   total_loss: 198.53\n",
      "=> STEP 2069/6250   lr: 0.000800   giou_loss: 1.69   conf_loss: 196.00   prob_loss: 0.82   total_loss: 198.50\n",
      "=> STEP 2070/6250   lr: 0.000800   giou_loss: 1.65   conf_loss: 195.89   prob_loss: 0.82   total_loss: 198.35\n",
      "=> STEP 2071/6250   lr: 0.000800   giou_loss: 1.69   conf_loss: 195.76   prob_loss: 0.82   total_loss: 198.27\n",
      "=> STEP 2072/6250   lr: 0.000800   giou_loss: 1.76   conf_loss: 195.64   prob_loss: 0.82   total_loss: 198.22\n",
      "=> STEP 2073/6250   lr: 0.000800   giou_loss: 1.65   conf_loss: 195.53   prob_loss: 0.81   total_loss: 198.00\n",
      "=> STEP 2074/6250   lr: 0.000799   giou_loss: 1.64   conf_loss: 195.42   prob_loss: 0.81   total_loss: 197.86\n",
      "=> STEP 2075/6250   lr: 0.000799   giou_loss: 1.69   conf_loss: 195.30   prob_loss: 0.81   total_loss: 197.80\n",
      "=> STEP 2076/6250   lr: 0.000799   giou_loss: 1.60   conf_loss: 195.17   prob_loss: 0.82   total_loss: 197.59\n",
      "=> STEP 2077/6250   lr: 0.000799   giou_loss: 1.70   conf_loss: 195.05   prob_loss: 0.82   total_loss: 197.57\n",
      "=> STEP 2078/6250   lr: 0.000799   giou_loss: 1.86   conf_loss: 194.94   prob_loss: 0.82   total_loss: 197.61\n",
      "=> STEP 2079/6250   lr: 0.000798   giou_loss: 1.78   conf_loss: 194.82   prob_loss: 0.82   total_loss: 197.42\n",
      "=> STEP 2080/6250   lr: 0.000798   giou_loss: 1.56   conf_loss: 194.70   prob_loss: 0.82   total_loss: 197.08\n",
      "=> STEP 2081/6250   lr: 0.000798   giou_loss: 1.77   conf_loss: 194.59   prob_loss: 0.82   total_loss: 197.18\n",
      "=> STEP 2082/6250   lr: 0.000798   giou_loss: 1.77   conf_loss: 194.47   prob_loss: 0.81   total_loss: 197.06\n",
      "=> STEP 2083/6250   lr: 0.000798   giou_loss: 1.56   conf_loss: 194.35   prob_loss: 0.81   total_loss: 196.73\n",
      "=> STEP 2084/6250   lr: 0.000797   giou_loss: 1.65   conf_loss: 194.23   prob_loss: 0.81   total_loss: 196.69\n",
      "=> STEP 2085/6250   lr: 0.000797   giou_loss: 1.57   conf_loss: 194.12   prob_loss: 0.81   total_loss: 196.50\n",
      "=> STEP 2086/6250   lr: 0.000797   giou_loss: 1.68   conf_loss: 194.00   prob_loss: 0.81   total_loss: 196.49\n",
      "=> STEP 2087/6250   lr: 0.000797   giou_loss: 1.70   conf_loss: 193.88   prob_loss: 0.81   total_loss: 196.40\n",
      "=> STEP 2088/6250   lr: 0.000796   giou_loss: 1.59   conf_loss: 193.76   prob_loss: 0.82   total_loss: 196.16\n",
      "=> STEP 2089/6250   lr: 0.000796   giou_loss: 1.73   conf_loss: 193.64   prob_loss: 0.83   total_loss: 196.20\n",
      "=> STEP 2090/6250   lr: 0.000796   giou_loss: 1.79   conf_loss: 193.53   prob_loss: 0.82   total_loss: 196.14\n",
      "=> STEP 2091/6250   lr: 0.000796   giou_loss: 1.82   conf_loss: 193.42   prob_loss: 0.81   total_loss: 196.05\n",
      "=> STEP 2092/6250   lr: 0.000796   giou_loss: 1.63   conf_loss: 193.32   prob_loss: 0.81   total_loss: 195.76\n",
      "=> STEP 2093/6250   lr: 0.000795   giou_loss: 1.76   conf_loss: 193.20   prob_loss: 0.81   total_loss: 195.77\n",
      "=> STEP 2094/6250   lr: 0.000795   giou_loss: 1.88   conf_loss: 193.10   prob_loss: 0.81   total_loss: 195.78\n",
      "=> STEP 2095/6250   lr: 0.000795   giou_loss: 1.83   conf_loss: 192.98   prob_loss: 0.81   total_loss: 195.62\n",
      "=> STEP 2096/6250   lr: 0.000795   giou_loss: 1.72   conf_loss: 192.86   prob_loss: 0.81   total_loss: 195.39\n",
      "=> STEP 2097/6250   lr: 0.000795   giou_loss: 1.59   conf_loss: 192.73   prob_loss: 0.83   total_loss: 195.14\n",
      "=> STEP 2098/6250   lr: 0.000794   giou_loss: 1.66   conf_loss: 192.62   prob_loss: 0.83   total_loss: 195.11\n",
      "=> STEP 2099/6250   lr: 0.000794   giou_loss: 1.62   conf_loss: 192.49   prob_loss: 0.82   total_loss: 194.93\n",
      "=> STEP 2100/6250   lr: 0.000794   giou_loss: 1.67   conf_loss: 192.40   prob_loss: 0.81   total_loss: 194.88\n",
      "=> STEP 2101/6250   lr: 0.000794   giou_loss: 1.70   conf_loss: 192.28   prob_loss: 0.81   total_loss: 194.79\n",
      "=> STEP 2102/6250   lr: 0.000794   giou_loss: 1.61   conf_loss: 192.17   prob_loss: 0.81   total_loss: 194.59\n",
      "=> STEP 2103/6250   lr: 0.000793   giou_loss: 1.67   conf_loss: 192.05   prob_loss: 0.82   total_loss: 194.54\n",
      "=> STEP 2104/6250   lr: 0.000793   giou_loss: 1.76   conf_loss: 191.92   prob_loss: 0.83   total_loss: 194.50\n",
      "=> STEP 2105/6250   lr: 0.000793   giou_loss: 1.68   conf_loss: 191.83   prob_loss: 0.82   total_loss: 194.33\n",
      "=> STEP 2106/6250   lr: 0.000793   giou_loss: 1.57   conf_loss: 191.70   prob_loss: 0.81   total_loss: 194.08\n",
      "=> STEP 2107/6250   lr: 0.000793   giou_loss: 1.73   conf_loss: 191.60   prob_loss: 0.81   total_loss: 194.14\n",
      "=> STEP 2108/6250   lr: 0.000792   giou_loss: 1.73   conf_loss: 191.51   prob_loss: 0.81   total_loss: 194.05\n",
      "=> STEP 2109/6250   lr: 0.000792   giou_loss: 1.62   conf_loss: 191.38   prob_loss: 0.81   total_loss: 193.81\n",
      "=> STEP 2110/6250   lr: 0.000792   giou_loss: 1.69   conf_loss: 191.28   prob_loss: 0.81   total_loss: 193.78\n",
      "=> STEP 2111/6250   lr: 0.000792   giou_loss: 1.75   conf_loss: 191.18   prob_loss: 0.81   total_loss: 193.74\n",
      "=> STEP 2112/6250   lr: 0.000792   giou_loss: 1.72   conf_loss: 191.05   prob_loss: 0.81   total_loss: 193.58\n",
      "=> STEP 2113/6250   lr: 0.000791   giou_loss: 1.59   conf_loss: 190.94   prob_loss: 0.81   total_loss: 193.35\n",
      "=> STEP 2114/6250   lr: 0.000791   giou_loss: 1.65   conf_loss: 190.82   prob_loss: 0.81   total_loss: 193.28\n",
      "=> STEP 2115/6250   lr: 0.000791   giou_loss: 1.69   conf_loss: 190.70   prob_loss: 0.81   total_loss: 193.20\n",
      "=> STEP 2116/6250   lr: 0.000791   giou_loss: 1.56   conf_loss: 190.60   prob_loss: 0.81   total_loss: 192.97\n",
      "=> STEP 2117/6250   lr: 0.000791   giou_loss: 1.66   conf_loss: 190.47   prob_loss: 0.81   total_loss: 192.94\n",
      "=> STEP 2118/6250   lr: 0.000790   giou_loss: 1.65   conf_loss: 190.36   prob_loss: 0.80   total_loss: 192.82\n",
      "=> STEP 2119/6250   lr: 0.000790   giou_loss: 1.58   conf_loss: 190.26   prob_loss: 0.81   total_loss: 192.64\n",
      "=> STEP 2120/6250   lr: 0.000790   giou_loss: 1.64   conf_loss: 190.13   prob_loss: 0.81   total_loss: 192.57\n",
      "=> STEP 2121/6250   lr: 0.000790   giou_loss: 1.61   conf_loss: 190.02   prob_loss: 0.81   total_loss: 192.45\n",
      "=> STEP 2122/6250   lr: 0.000790   giou_loss: 1.60   conf_loss: 189.91   prob_loss: 0.81   total_loss: 192.32\n",
      "=> STEP 2123/6250   lr: 0.000789   giou_loss: 1.60   conf_loss: 189.79   prob_loss: 0.81   total_loss: 192.20\n",
      "=> STEP 2124/6250   lr: 0.000789   giou_loss: 1.59   conf_loss: 189.69   prob_loss: 0.81   total_loss: 192.09\n",
      "=> STEP 2125/6250   lr: 0.000789   giou_loss: 1.66   conf_loss: 189.56   prob_loss: 0.81   total_loss: 192.03\n",
      "=> STEP 2126/6250   lr: 0.000789   giou_loss: 1.61   conf_loss: 189.44   prob_loss: 0.81   total_loss: 191.86\n",
      "=> STEP 2127/6250   lr: 0.000788   giou_loss: 1.64   conf_loss: 189.34   prob_loss: 0.81   total_loss: 191.79\n",
      "=> STEP 2128/6250   lr: 0.000788   giou_loss: 1.58   conf_loss: 189.23   prob_loss: 0.81   total_loss: 191.61\n",
      "=> STEP 2129/6250   lr: 0.000788   giou_loss: 1.64   conf_loss: 189.11   prob_loss: 0.81   total_loss: 191.55\n",
      "=> STEP 2130/6250   lr: 0.000788   giou_loss: 1.64   conf_loss: 189.00   prob_loss: 0.81   total_loss: 191.45\n",
      "=> STEP 2131/6250   lr: 0.000788   giou_loss: 1.57   conf_loss: 188.88   prob_loss: 0.81   total_loss: 191.25\n",
      "=> STEP 2132/6250   lr: 0.000787   giou_loss: 1.67   conf_loss: 188.78   prob_loss: 0.80   total_loss: 191.26\n",
      "=> STEP 2133/6250   lr: 0.000787   giou_loss: 1.64   conf_loss: 188.66   prob_loss: 0.80   total_loss: 191.10\n",
      "=> STEP 2134/6250   lr: 0.000787   giou_loss: 1.58   conf_loss: 188.56   prob_loss: 0.81   total_loss: 190.95\n",
      "=> STEP 2135/6250   lr: 0.000787   giou_loss: 1.68   conf_loss: 188.43   prob_loss: 0.81   total_loss: 190.92\n",
      "=> STEP 2136/6250   lr: 0.000787   giou_loss: 1.66   conf_loss: 188.33   prob_loss: 0.81   total_loss: 190.80\n",
      "=> STEP 2137/6250   lr: 0.000786   giou_loss: 1.57   conf_loss: 188.23   prob_loss: 0.80   total_loss: 190.60\n",
      "=> STEP 2138/6250   lr: 0.000786   giou_loss: 1.79   conf_loss: 188.12   prob_loss: 0.80   total_loss: 190.71\n",
      "=> STEP 2139/6250   lr: 0.000786   giou_loss: 1.82   conf_loss: 188.01   prob_loss: 0.80   total_loss: 190.63\n",
      "=> STEP 2140/6250   lr: 0.000786   giou_loss: 1.61   conf_loss: 187.90   prob_loss: 0.80   total_loss: 190.31\n",
      "=> STEP 2141/6250   lr: 0.000786   giou_loss: 1.79   conf_loss: 187.78   prob_loss: 0.81   total_loss: 190.38\n",
      "=> STEP 2142/6250   lr: 0.000785   giou_loss: 1.92   conf_loss: 187.68   prob_loss: 0.81   total_loss: 190.41\n",
      "=> STEP 2143/6250   lr: 0.000785   giou_loss: 1.86   conf_loss: 187.57   prob_loss: 0.80   total_loss: 190.23\n",
      "=> STEP 2144/6250   lr: 0.000785   giou_loss: 1.58   conf_loss: 187.45   prob_loss: 0.80   total_loss: 189.83\n",
      "=> STEP 2145/6250   lr: 0.000785   giou_loss: 1.78   conf_loss: 187.35   prob_loss: 0.80   total_loss: 189.93\n",
      "=> STEP 2146/6250   lr: 0.000785   giou_loss: 1.88   conf_loss: 187.24   prob_loss: 0.79   total_loss: 189.91\n",
      "=> STEP 2147/6250   lr: 0.000784   giou_loss: 1.78   conf_loss: 187.12   prob_loss: 0.79   total_loss: 189.69\n",
      "=> STEP 2148/6250   lr: 0.000784   giou_loss: 1.68   conf_loss: 187.01   prob_loss: 0.80   total_loss: 189.48\n",
      "=> STEP 2149/6250   lr: 0.000784   giou_loss: 1.60   conf_loss: 186.89   prob_loss: 0.81   total_loss: 189.30\n",
      "=> STEP 2150/6250   lr: 0.000784   giou_loss: 1.65   conf_loss: 186.77   prob_loss: 0.82   total_loss: 189.24\n",
      "=> STEP 2151/6250   lr: 0.000783   giou_loss: 1.63   conf_loss: 186.67   prob_loss: 0.82   total_loss: 189.12\n",
      "=> STEP 2152/6250   lr: 0.000783   giou_loss: 1.57   conf_loss: 186.56   prob_loss: 0.81   total_loss: 188.93\n",
      "=> STEP 2153/6250   lr: 0.000783   giou_loss: 1.63   conf_loss: 186.46   prob_loss: 0.80   total_loss: 188.89\n",
      "=> STEP 2154/6250   lr: 0.000783   giou_loss: 1.56   conf_loss: 186.37   prob_loss: 0.80   total_loss: 188.74\n",
      "=> STEP 2155/6250   lr: 0.000783   giou_loss: 1.57   conf_loss: 186.24   prob_loss: 0.80   total_loss: 188.61\n",
      "=> STEP 2156/6250   lr: 0.000782   giou_loss: 1.64   conf_loss: 186.15   prob_loss: 0.81   total_loss: 188.59\n",
      "=> STEP 2157/6250   lr: 0.000782   giou_loss: 1.61   conf_loss: 186.04   prob_loss: 0.81   total_loss: 188.46\n",
      "=> STEP 2158/6250   lr: 0.000782   giou_loss: 1.62   conf_loss: 185.94   prob_loss: 0.80   total_loss: 188.36\n",
      "=> STEP 2159/6250   lr: 0.000782   giou_loss: 1.67   conf_loss: 185.83   prob_loss: 0.80   total_loss: 188.30\n",
      "=> STEP 2160/6250   lr: 0.000782   giou_loss: 1.69   conf_loss: 185.70   prob_loss: 0.81   total_loss: 188.20\n",
      "=> STEP 2161/6250   lr: 0.000781   giou_loss: 1.73   conf_loss: 185.60   prob_loss: 0.81   total_loss: 188.13\n",
      "=> STEP 2162/6250   lr: 0.000781   giou_loss: 1.63   conf_loss: 185.49   prob_loss: 0.80   total_loss: 187.91\n",
      "=> STEP 2163/6250   lr: 0.000781   giou_loss: 1.66   conf_loss: 185.39   prob_loss: 0.79   total_loss: 187.84\n",
      "=> STEP 2164/6250   lr: 0.000781   giou_loss: 1.72   conf_loss: 185.29   prob_loss: 0.79   total_loss: 187.80\n",
      "=> STEP 2165/6250   lr: 0.000781   giou_loss: 1.65   conf_loss: 185.16   prob_loss: 0.79   total_loss: 187.60\n",
      "=> STEP 2166/6250   lr: 0.000780   giou_loss: 1.63   conf_loss: 185.04   prob_loss: 0.80   total_loss: 187.47\n",
      "=> STEP 2167/6250   lr: 0.000780   giou_loss: 1.65   conf_loss: 184.93   prob_loss: 0.80   total_loss: 187.38\n",
      "=> STEP 2168/6250   lr: 0.000780   giou_loss: 1.56   conf_loss: 184.81   prob_loss: 0.80   total_loss: 187.18\n",
      "=> STEP 2169/6250   lr: 0.000780   giou_loss: 1.57   conf_loss: 184.71   prob_loss: 0.80   total_loss: 187.08\n",
      "=> STEP 2170/6250   lr: 0.000780   giou_loss: 1.56   conf_loss: 184.60   prob_loss: 0.80   total_loss: 186.96\n",
      "=> STEP 2171/6250   lr: 0.000779   giou_loss: 1.61   conf_loss: 184.49   prob_loss: 0.80   total_loss: 186.90\n",
      "=> STEP 2172/6250   lr: 0.000779   giou_loss: 1.56   conf_loss: 184.39   prob_loss: 0.80   total_loss: 186.75\n",
      "=> STEP 2173/6250   lr: 0.000779   giou_loss: 1.59   conf_loss: 184.27   prob_loss: 0.80   total_loss: 186.66\n",
      "=> STEP 2174/6250   lr: 0.000779   giou_loss: 1.56   conf_loss: 184.17   prob_loss: 0.80   total_loss: 186.53\n",
      "=> STEP 2175/6250   lr: 0.000778   giou_loss: 1.61   conf_loss: 184.05   prob_loss: 0.80   total_loss: 186.47\n",
      "=> STEP 2176/6250   lr: 0.000778   giou_loss: 1.56   conf_loss: 183.94   prob_loss: 0.80   total_loss: 186.31\n",
      "=> STEP 2177/6250   lr: 0.000778   giou_loss: 1.59   conf_loss: 183.84   prob_loss: 0.80   total_loss: 186.23\n",
      "=> STEP 2178/6250   lr: 0.000778   giou_loss: 1.56   conf_loss: 183.72   prob_loss: 0.80   total_loss: 186.09\n",
      "=> STEP 2179/6250   lr: 0.000778   giou_loss: 1.62   conf_loss: 183.62   prob_loss: 0.80   total_loss: 186.04\n",
      "=> STEP 2180/6250   lr: 0.000777   giou_loss: 1.56   conf_loss: 183.51   prob_loss: 0.80   total_loss: 185.87\n",
      "=> STEP 2181/6250   lr: 0.000777   giou_loss: 1.59   conf_loss: 183.41   prob_loss: 0.80   total_loss: 185.79\n",
      "=> STEP 2182/6250   lr: 0.000777   giou_loss: 1.56   conf_loss: 183.29   prob_loss: 0.80   total_loss: 185.66\n",
      "=> STEP 2183/6250   lr: 0.000777   giou_loss: 1.65   conf_loss: 183.19   prob_loss: 0.79   total_loss: 185.64\n",
      "=> STEP 2184/6250   lr: 0.000777   giou_loss: 1.57   conf_loss: 183.09   prob_loss: 0.79   total_loss: 185.45\n",
      "=> STEP 2185/6250   lr: 0.000776   giou_loss: 1.70   conf_loss: 182.98   prob_loss: 0.79   total_loss: 185.47\n",
      "=> STEP 2186/6250   lr: 0.000776   giou_loss: 1.74   conf_loss: 182.88   prob_loss: 0.79   total_loss: 185.41\n",
      "=> STEP 2187/6250   lr: 0.000776   giou_loss: 1.66   conf_loss: 182.76   prob_loss: 0.80   total_loss: 185.22\n",
      "=> STEP 2188/6250   lr: 0.000776   giou_loss: 1.61   conf_loss: 182.65   prob_loss: 0.80   total_loss: 185.07\n",
      "=> STEP 2189/6250   lr: 0.000776   giou_loss: 1.66   conf_loss: 182.54   prob_loss: 0.80   total_loss: 185.00\n",
      "=> STEP 2190/6250   lr: 0.000775   giou_loss: 1.58   conf_loss: 182.44   prob_loss: 0.80   total_loss: 184.82\n",
      "=> STEP 2191/6250   lr: 0.000775   giou_loss: 1.70   conf_loss: 182.35   prob_loss: 0.79   total_loss: 184.85\n",
      "=> STEP 2192/6250   lr: 0.000775   giou_loss: 1.82   conf_loss: 182.25   prob_loss: 0.79   total_loss: 184.85\n",
      "=> STEP 2193/6250   lr: 0.000775   giou_loss: 1.71   conf_loss: 182.14   prob_loss: 0.79   total_loss: 184.64\n",
      "=> STEP 2194/6250   lr: 0.000774   giou_loss: 1.57   conf_loss: 182.04   prob_loss: 0.80   total_loss: 184.41\n",
      "=> STEP 2195/6250   lr: 0.000774   giou_loss: 1.72   conf_loss: 181.92   prob_loss: 0.80   total_loss: 184.44\n",
      "=> STEP 2196/6250   lr: 0.000774   giou_loss: 1.74   conf_loss: 181.84   prob_loss: 0.80   total_loss: 184.38\n",
      "=> STEP 2197/6250   lr: 0.000774   giou_loss: 1.65   conf_loss: 181.73   prob_loss: 0.79   total_loss: 184.17\n",
      "=> STEP 2198/6250   lr: 0.000774   giou_loss: 1.64   conf_loss: 181.63   prob_loss: 0.79   total_loss: 184.06\n",
      "=> STEP 2199/6250   lr: 0.000773   giou_loss: 1.68   conf_loss: 181.53   prob_loss: 0.79   total_loss: 184.00\n",
      "=> STEP 2200/6250   lr: 0.000773   giou_loss: 1.71   conf_loss: 181.41   prob_loss: 0.79   total_loss: 183.91\n",
      "=> STEP 2201/6250   lr: 0.000773   giou_loss: 1.77   conf_loss: 181.31   prob_loss: 0.79   total_loss: 183.88\n",
      "=> STEP 2202/6250   lr: 0.000773   giou_loss: 1.72   conf_loss: 181.19   prob_loss: 0.79   total_loss: 183.70\n",
      "=> STEP 2203/6250   lr: 0.000773   giou_loss: 1.68   conf_loss: 181.09   prob_loss: 0.80   total_loss: 183.57\n",
      "=> STEP 2204/6250   lr: 0.000772   giou_loss: 1.67   conf_loss: 181.00   prob_loss: 0.79   total_loss: 183.46\n",
      "=> STEP 2205/6250   lr: 0.000772   giou_loss: 1.65   conf_loss: 180.91   prob_loss: 0.78   total_loss: 183.34\n",
      "=> STEP 2206/6250   lr: 0.000772   giou_loss: 1.68   conf_loss: 180.80   prob_loss: 0.78   total_loss: 183.25\n",
      "=> STEP 2207/6250   lr: 0.000772   giou_loss: 1.71   conf_loss: 180.71   prob_loss: 0.78   total_loss: 183.19\n",
      "=> STEP 2208/6250   lr: 0.000771   giou_loss: 1.56   conf_loss: 180.59   prob_loss: 0.78   total_loss: 182.94\n",
      "=> STEP 2209/6250   lr: 0.000771   giou_loss: 1.62   conf_loss: 180.48   prob_loss: 0.79   total_loss: 182.89\n",
      "=> STEP 2210/6250   lr: 0.000771   giou_loss: 1.61   conf_loss: 180.39   prob_loss: 0.79   total_loss: 182.79\n",
      "=> STEP 2211/6250   lr: 0.000771   giou_loss: 1.60   conf_loss: 180.28   prob_loss: 0.78   total_loss: 182.67\n",
      "=> STEP 2212/6250   lr: 0.000771   giou_loss: 1.57   conf_loss: 180.18   prob_loss: 0.78   total_loss: 182.52\n",
      "=> STEP 2213/6250   lr: 0.000770   giou_loss: 1.57   conf_loss: 180.08   prob_loss: 0.78   total_loss: 182.43\n",
      "=> STEP 2214/6250   lr: 0.000770   giou_loss: 1.58   conf_loss: 179.97   prob_loss: 0.78   total_loss: 182.33\n",
      "=> STEP 2215/6250   lr: 0.000770   giou_loss: 1.59   conf_loss: 179.86   prob_loss: 0.78   total_loss: 182.23\n",
      "=> STEP 2216/6250   lr: 0.000770   giou_loss: 1.57   conf_loss: 179.76   prob_loss: 0.78   total_loss: 182.11\n",
      "=> STEP 2217/6250   lr: 0.000770   giou_loss: 1.56   conf_loss: 179.64   prob_loss: 0.79   total_loss: 181.99\n",
      "=> STEP 2218/6250   lr: 0.000769   giou_loss: 1.58   conf_loss: 179.54   prob_loss: 0.79   total_loss: 181.91\n",
      "=> STEP 2219/6250   lr: 0.000769   giou_loss: 1.59   conf_loss: 179.43   prob_loss: 0.79   total_loss: 181.81\n",
      "=> STEP 2220/6250   lr: 0.000769   giou_loss: 1.56   conf_loss: 179.33   prob_loss: 0.78   total_loss: 181.68\n",
      "=> STEP 2221/6250   lr: 0.000769   giou_loss: 1.56   conf_loss: 179.23   prob_loss: 0.78   total_loss: 181.58\n",
      "=> STEP 2222/6250   lr: 0.000769   giou_loss: 1.57   conf_loss: 179.12   prob_loss: 0.78   total_loss: 181.48\n",
      "=> STEP 2223/6250   lr: 0.000768   giou_loss: 1.60   conf_loss: 179.02   prob_loss: 0.78   total_loss: 181.41\n",
      "=> STEP 2224/6250   lr: 0.000768   giou_loss: 1.59   conf_loss: 178.91   prob_loss: 0.78   total_loss: 181.28\n",
      "=> STEP 2225/6250   lr: 0.000768   giou_loss: 1.67   conf_loss: 178.82   prob_loss: 0.78   total_loss: 181.27\n",
      "=> STEP 2226/6250   lr: 0.000768   giou_loss: 1.77   conf_loss: 178.72   prob_loss: 0.78   total_loss: 181.27\n",
      "=> STEP 2227/6250   lr: 0.000767   giou_loss: 1.63   conf_loss: 178.61   prob_loss: 0.78   total_loss: 181.02\n",
      "=> STEP 2228/6250   lr: 0.000767   giou_loss: 1.61   conf_loss: 178.52   prob_loss: 0.79   total_loss: 180.91\n",
      "=> STEP 2229/6250   lr: 0.000767   giou_loss: 1.71   conf_loss: 178.41   prob_loss: 0.79   total_loss: 180.90\n",
      "=> STEP 2230/6250   lr: 0.000767   giou_loss: 1.70   conf_loss: 178.33   prob_loss: 0.78   total_loss: 180.81\n",
      "=> STEP 2231/6250   lr: 0.000767   giou_loss: 1.60   conf_loss: 178.23   prob_loss: 0.78   total_loss: 180.61\n",
      "=> STEP 2232/6250   lr: 0.000766   giou_loss: 1.71   conf_loss: 178.13   prob_loss: 0.77   total_loss: 180.61\n",
      "=> STEP 2233/6250   lr: 0.000766   giou_loss: 1.78   conf_loss: 178.03   prob_loss: 0.77   total_loss: 180.59\n",
      "=> STEP 2234/6250   lr: 0.000766   giou_loss: 1.74   conf_loss: 177.93   prob_loss: 0.77   total_loss: 180.45\n",
      "=> STEP 2235/6250   lr: 0.000766   giou_loss: 1.59   conf_loss: 177.80   prob_loss: 0.78   total_loss: 180.17\n",
      "=> STEP 2236/6250   lr: 0.000766   giou_loss: 1.73   conf_loss: 177.70   prob_loss: 0.80   total_loss: 180.22\n",
      "=> STEP 2237/6250   lr: 0.000765   giou_loss: 1.80   conf_loss: 177.61   prob_loss: 0.79   total_loss: 180.20\n",
      "=> STEP 2238/6250   lr: 0.000765   giou_loss: 1.83   conf_loss: 177.51   prob_loss: 0.78   total_loss: 180.12\n",
      "=> STEP 2239/6250   lr: 0.000765   giou_loss: 1.72   conf_loss: 177.41   prob_loss: 0.77   total_loss: 179.91\n",
      "=> STEP 2240/6250   lr: 0.000765   giou_loss: 1.60   conf_loss: 177.32   prob_loss: 0.77   total_loss: 179.69\n",
      "=> STEP 2241/6250   lr: 0.000764   giou_loss: 1.67   conf_loss: 177.22   prob_loss: 0.76   total_loss: 179.65\n",
      "=> STEP 2242/6250   lr: 0.000764   giou_loss: 1.59   conf_loss: 177.11   prob_loss: 0.77   total_loss: 179.47\n",
      "=> STEP 2243/6250   lr: 0.000764   giou_loss: 1.62   conf_loss: 177.00   prob_loss: 0.78   total_loss: 179.39\n",
      "=> STEP 2244/6250   lr: 0.000764   giou_loss: 1.67   conf_loss: 176.90   prob_loss: 0.78   total_loss: 179.35\n",
      "=> STEP 2245/6250   lr: 0.000764   giou_loss: 1.57   conf_loss: 176.80   prob_loss: 0.78   total_loss: 179.16\n",
      "=> STEP 2246/6250   lr: 0.000763   giou_loss: 1.65   conf_loss: 176.70   prob_loss: 0.78   total_loss: 179.12\n",
      "=> STEP 2247/6250   lr: 0.000763   giou_loss: 1.70   conf_loss: 176.60   prob_loss: 0.77   total_loss: 179.07\n",
      "=> STEP 2248/6250   lr: 0.000763   giou_loss: 1.57   conf_loss: 176.51   prob_loss: 0.78   total_loss: 178.85\n",
      "=> STEP 2249/6250   lr: 0.000763   giou_loss: 1.64   conf_loss: 176.39   prob_loss: 0.78   total_loss: 178.81\n",
      "=> STEP 2250/6250   lr: 0.000762   giou_loss: 1.60   conf_loss: 176.30   prob_loss: 0.78   total_loss: 178.67\n",
      "=> STEP 2251/6250   lr: 0.000762   giou_loss: 1.68   conf_loss: 176.20   prob_loss: 0.78   total_loss: 178.65\n",
      "=> STEP 2252/6250   lr: 0.000762   giou_loss: 1.65   conf_loss: 176.09   prob_loss: 0.78   total_loss: 178.52\n",
      "=> STEP 2253/6250   lr: 0.000762   giou_loss: 1.57   conf_loss: 175.99   prob_loss: 0.78   total_loss: 178.34\n",
      "=> STEP 2254/6250   lr: 0.000762   giou_loss: 1.69   conf_loss: 175.89   prob_loss: 0.78   total_loss: 178.35\n",
      "=> STEP 2255/6250   lr: 0.000761   giou_loss: 1.60   conf_loss: 175.79   prob_loss: 0.78   total_loss: 178.16\n",
      "=> STEP 2256/6250   lr: 0.000761   giou_loss: 1.64   conf_loss: 175.69   prob_loss: 0.78   total_loss: 178.10\n",
      "=> STEP 2257/6250   lr: 0.000761   giou_loss: 1.71   conf_loss: 175.59   prob_loss: 0.78   total_loss: 178.07\n",
      "=> STEP 2258/6250   lr: 0.000761   giou_loss: 1.56   conf_loss: 175.49   prob_loss: 0.78   total_loss: 177.83\n",
      "=> STEP 2259/6250   lr: 0.000761   giou_loss: 1.62   conf_loss: 175.38   prob_loss: 0.78   total_loss: 177.78\n",
      "=> STEP 2260/6250   lr: 0.000760   giou_loss: 1.59   conf_loss: 175.28   prob_loss: 0.77   total_loss: 177.65\n",
      "=> STEP 2261/6250   lr: 0.000760   giou_loss: 1.67   conf_loss: 175.19   prob_loss: 0.77   total_loss: 177.63\n",
      "=> STEP 2262/6250   lr: 0.000760   giou_loss: 1.75   conf_loss: 175.09   prob_loss: 0.78   total_loss: 177.61\n",
      "=> STEP 2263/6250   lr: 0.000760   giou_loss: 1.60   conf_loss: 174.99   prob_loss: 0.78   total_loss: 177.36\n",
      "=> STEP 2264/6250   lr: 0.000759   giou_loss: 1.69   conf_loss: 174.88   prob_loss: 0.78   total_loss: 177.35\n",
      "=> STEP 2265/6250   lr: 0.000759   giou_loss: 1.77   conf_loss: 174.78   prob_loss: 0.78   total_loss: 177.33\n",
      "=> STEP 2266/6250   lr: 0.000759   giou_loss: 1.72   conf_loss: 174.68   prob_loss: 0.78   total_loss: 177.18\n",
      "=> STEP 2267/6250   lr: 0.000759   giou_loss: 1.58   conf_loss: 174.58   prob_loss: 0.78   total_loss: 176.94\n",
      "=> STEP 2268/6250   lr: 0.000759   giou_loss: 1.67   conf_loss: 174.49   prob_loss: 0.78   total_loss: 176.94\n",
      "=> STEP 2269/6250   lr: 0.000758   giou_loss: 1.76   conf_loss: 174.39   prob_loss: 0.78   total_loss: 176.93\n",
      "=> STEP 2270/6250   lr: 0.000758   giou_loss: 1.61   conf_loss: 174.29   prob_loss: 0.78   total_loss: 176.68\n",
      "=> STEP 2271/6250   lr: 0.000758   giou_loss: 1.62   conf_loss: 174.21   prob_loss: 0.78   total_loss: 176.60\n",
      "=> STEP 2272/6250   lr: 0.000758   giou_loss: 1.71   conf_loss: 174.11   prob_loss: 0.77   total_loss: 176.59\n",
      "=> STEP 2273/6250   lr: 0.000758   giou_loss: 1.75   conf_loss: 174.02   prob_loss: 0.77   total_loss: 176.54\n",
      "=> STEP 2274/6250   lr: 0.000757   giou_loss: 1.65   conf_loss: 173.91   prob_loss: 0.78   total_loss: 176.34\n",
      "=> STEP 2275/6250   lr: 0.000757   giou_loss: 1.56   conf_loss: 173.82   prob_loss: 0.78   total_loss: 176.16\n",
      "=> STEP 2276/6250   lr: 0.000757   giou_loss: 1.70   conf_loss: 173.71   prob_loss: 0.78   total_loss: 176.19\n",
      "=> STEP 2277/6250   lr: 0.000757   giou_loss: 1.62   conf_loss: 173.64   prob_loss: 0.78   total_loss: 176.04\n",
      "=> STEP 2278/6250   lr: 0.000756   giou_loss: 1.63   conf_loss: 173.56   prob_loss: 0.77   total_loss: 175.96\n",
      "=> STEP 2279/6250   lr: 0.000756   giou_loss: 1.77   conf_loss: 173.46   prob_loss: 0.77   total_loss: 176.00\n",
      "=> STEP 2280/6250   lr: 0.000756   giou_loss: 1.70   conf_loss: 173.36   prob_loss: 0.77   total_loss: 175.83\n",
      "=> STEP 2281/6250   lr: 0.000756   giou_loss: 1.67   conf_loss: 173.25   prob_loss: 0.78   total_loss: 175.70\n",
      "=> STEP 2282/6250   lr: 0.000756   giou_loss: 1.69   conf_loss: 173.15   prob_loss: 0.79   total_loss: 175.62\n",
      "=> STEP 2283/6250   lr: 0.000755   giou_loss: 1.77   conf_loss: 173.06   prob_loss: 0.78   total_loss: 175.61\n",
      "=> STEP 2284/6250   lr: 0.000755   giou_loss: 1.73   conf_loss: 172.95   prob_loss: 0.78   total_loss: 175.46\n",
      "=> STEP 2285/6250   lr: 0.000755   giou_loss: 1.56   conf_loss: 172.87   prob_loss: 0.77   total_loss: 175.21\n",
      "=> STEP 2286/6250   lr: 0.000755   giou_loss: 1.75   conf_loss: 172.76   prob_loss: 0.77   total_loss: 175.28\n",
      "=> STEP 2287/6250   lr: 0.000754   giou_loss: 1.72   conf_loss: 172.68   prob_loss: 0.77   total_loss: 175.16\n",
      "=> STEP 2288/6250   lr: 0.000754   giou_loss: 1.59   conf_loss: 172.58   prob_loss: 0.76   total_loss: 174.94\n",
      "=> STEP 2289/6250   lr: 0.000754   giou_loss: 1.59   conf_loss: 172.47   prob_loss: 0.77   total_loss: 174.83\n",
      "=> STEP 2290/6250   lr: 0.000754   giou_loss: 1.61   conf_loss: 172.38   prob_loss: 0.77   total_loss: 174.77\n",
      "=> STEP 2291/6250   lr: 0.000754   giou_loss: 1.60   conf_loss: 172.27   prob_loss: 0.77   total_loss: 174.65\n",
      "=> STEP 2292/6250   lr: 0.000753   giou_loss: 1.60   conf_loss: 172.19   prob_loss: 0.77   total_loss: 174.56\n",
      "=> STEP 2293/6250   lr: 0.000753   giou_loss: 1.59   conf_loss: 172.08   prob_loss: 0.77   total_loss: 174.44\n",
      "=> STEP 2294/6250   lr: 0.000753   giou_loss: 1.62   conf_loss: 171.99   prob_loss: 0.78   total_loss: 174.39\n",
      "=> STEP 2295/6250   lr: 0.000753   giou_loss: 1.69   conf_loss: 171.88   prob_loss: 0.78   total_loss: 174.34\n",
      "=> STEP 2296/6250   lr: 0.000753   giou_loss: 1.61   conf_loss: 171.80   prob_loss: 0.77   total_loss: 174.17\n",
      "=> STEP 2297/6250   lr: 0.000752   giou_loss: 1.58   conf_loss: 171.71   prob_loss: 0.77   total_loss: 174.06\n",
      "=> STEP 2298/6250   lr: 0.000752   giou_loss: 1.58   conf_loss: 171.59   prob_loss: 0.77   total_loss: 173.94\n",
      "=> STEP 2299/6250   lr: 0.000752   giou_loss: 1.56   conf_loss: 171.53   prob_loss: 0.77   total_loss: 173.87\n",
      "=> STEP 2300/6250   lr: 0.000752   giou_loss: 1.67   conf_loss: 171.41   prob_loss: 0.78   total_loss: 173.85\n",
      "=> STEP 2301/6250   lr: 0.000751   giou_loss: 1.62   conf_loss: 171.34   prob_loss: 0.77   total_loss: 173.73\n",
      "=> STEP 2302/6250   lr: 0.000751   giou_loss: 1.57   conf_loss: 171.27   prob_loss: 0.77   total_loss: 173.61\n",
      "=> STEP 2303/6250   lr: 0.000751   giou_loss: 1.71   conf_loss: 171.14   prob_loss: 0.77   total_loss: 173.62\n",
      "=> STEP 2304/6250   lr: 0.000751   giou_loss: 1.68   conf_loss: 171.08   prob_loss: 0.77   total_loss: 173.52\n",
      "=> STEP 2305/6250   lr: 0.000751   giou_loss: 1.61   conf_loss: 170.97   prob_loss: 0.78   total_loss: 173.36\n",
      "=> STEP 2306/6250   lr: 0.000750   giou_loss: 1.61   conf_loss: 170.83   prob_loss: 0.78   total_loss: 173.23\n",
      "=> STEP 2307/6250   lr: 0.000750   giou_loss: 1.59   conf_loss: 170.80   prob_loss: 0.77   total_loss: 173.17\n",
      "=> STEP 2308/6250   lr: 0.000750   giou_loss: 1.57   conf_loss: 170.66   prob_loss: 0.77   total_loss: 173.00\n",
      "=> STEP 2309/6250   lr: 0.000750   giou_loss: 1.64   conf_loss: 170.57   prob_loss: 0.77   total_loss: 172.98\n",
      "=> STEP 2310/6250   lr: 0.000749   giou_loss: 1.65   conf_loss: 170.48   prob_loss: 0.77   total_loss: 172.90\n",
      "=> STEP 2311/6250   lr: 0.000749   giou_loss: 1.56   conf_loss: 170.35   prob_loss: 0.77   total_loss: 172.69\n",
      "=> STEP 2312/6250   lr: 0.000749   giou_loss: 1.65   conf_loss: 170.29   prob_loss: 0.76   total_loss: 172.70\n",
      "=> STEP 2313/6250   lr: 0.000749   giou_loss: 1.61   conf_loss: 170.17   prob_loss: 0.77   total_loss: 172.55\n",
      "=> STEP 2314/6250   lr: 0.000749   giou_loss: 1.62   conf_loss: 170.07   prob_loss: 0.77   total_loss: 172.45\n",
      "=> STEP 2315/6250   lr: 0.000748   giou_loss: 1.63   conf_loss: 169.98   prob_loss: 0.77   total_loss: 172.38\n",
      "=> STEP 2316/6250   lr: 0.000748   giou_loss: 1.56   conf_loss: 169.87   prob_loss: 0.77   total_loss: 172.20\n",
      "=> STEP 2317/6250   lr: 0.000748   giou_loss: 1.66   conf_loss: 169.78   prob_loss: 0.77   total_loss: 172.21\n",
      "=> STEP 2318/6250   lr: 0.000748   giou_loss: 1.62   conf_loss: 169.68   prob_loss: 0.77   total_loss: 172.07\n",
      "=> STEP 2319/6250   lr: 0.000747   giou_loss: 1.62   conf_loss: 169.58   prob_loss: 0.77   total_loss: 171.97\n",
      "=> STEP 2320/6250   lr: 0.000747   giou_loss: 1.63   conf_loss: 169.48   prob_loss: 0.77   total_loss: 171.89\n",
      "=> STEP 2321/6250   lr: 0.000747   giou_loss: 1.56   conf_loss: 169.39   prob_loss: 0.77   total_loss: 171.73\n",
      "=> STEP 2322/6250   lr: 0.000747   giou_loss: 1.67   conf_loss: 169.30   prob_loss: 0.77   total_loss: 171.74\n",
      "=> STEP 2323/6250   lr: 0.000747   giou_loss: 1.61   conf_loss: 169.21   prob_loss: 0.77   total_loss: 171.59\n",
      "=> STEP 2324/6250   lr: 0.000746   giou_loss: 1.73   conf_loss: 169.11   prob_loss: 0.77   total_loss: 171.61\n",
      "=> STEP 2325/6250   lr: 0.000746   giou_loss: 1.71   conf_loss: 169.01   prob_loss: 0.77   total_loss: 171.49\n",
      "=> STEP 2326/6250   lr: 0.000746   giou_loss: 1.56   conf_loss: 168.94   prob_loss: 0.77   total_loss: 171.27\n",
      "=> STEP 2327/6250   lr: 0.000746   giou_loss: 1.74   conf_loss: 168.83   prob_loss: 0.77   total_loss: 171.34\n",
      "=> STEP 2328/6250   lr: 0.000746   giou_loss: 1.68   conf_loss: 168.74   prob_loss: 0.77   total_loss: 171.19\n",
      "=> STEP 2329/6250   lr: 0.000745   giou_loss: 1.56   conf_loss: 168.67   prob_loss: 0.77   total_loss: 171.00\n",
      "=> STEP 2330/6250   lr: 0.000745   giou_loss: 1.58   conf_loss: 168.55   prob_loss: 0.76   total_loss: 170.90\n",
      "=> STEP 2331/6250   lr: 0.000745   giou_loss: 1.66   conf_loss: 168.47   prob_loss: 0.76   total_loss: 170.89\n",
      "=> STEP 2332/6250   lr: 0.000745   giou_loss: 1.60   conf_loss: 168.39   prob_loss: 0.76   total_loss: 170.75\n",
      "=> STEP 2333/6250   lr: 0.000744   giou_loss: 1.58   conf_loss: 168.26   prob_loss: 0.77   total_loss: 170.61\n",
      "=> STEP 2334/6250   lr: 0.000744   giou_loss: 1.59   conf_loss: 168.19   prob_loss: 0.77   total_loss: 170.55\n",
      "=> STEP 2335/6250   lr: 0.000744   giou_loss: 1.59   conf_loss: 168.12   prob_loss: 0.76   total_loss: 170.48\n",
      "=> STEP 2336/6250   lr: 0.000744   giou_loss: 1.64   conf_loss: 167.99   prob_loss: 0.76   total_loss: 170.39\n",
      "=> STEP 2337/6250   lr: 0.000744   giou_loss: 1.58   conf_loss: 167.93   prob_loss: 0.76   total_loss: 170.27\n",
      "=> STEP 2338/6250   lr: 0.000743   giou_loss: 1.61   conf_loss: 167.85   prob_loss: 0.77   total_loss: 170.23\n",
      "=> STEP 2339/6250   lr: 0.000743   giou_loss: 1.62   conf_loss: 167.71   prob_loss: 0.76   total_loss: 170.10\n",
      "=> STEP 2340/6250   lr: 0.000743   giou_loss: 1.57   conf_loss: 167.68   prob_loss: 0.76   total_loss: 170.01\n",
      "=> STEP 2341/6250   lr: 0.000743   giou_loss: 1.65   conf_loss: 167.57   prob_loss: 0.76   total_loss: 169.97\n",
      "=> STEP 2342/6250   lr: 0.000742   giou_loss: 1.62   conf_loss: 167.45   prob_loss: 0.76   total_loss: 169.83\n",
      "=> STEP 2343/6250   lr: 0.000742   giou_loss: 1.62   conf_loss: 167.41   prob_loss: 0.76   total_loss: 169.79\n",
      "=> STEP 2344/6250   lr: 0.000742   giou_loss: 1.60   conf_loss: 167.28   prob_loss: 0.76   total_loss: 169.64\n",
      "=> STEP 2345/6250   lr: 0.000742   giou_loss: 1.57   conf_loss: 167.19   prob_loss: 0.76   total_loss: 169.52\n",
      "=> STEP 2346/6250   lr: 0.000742   giou_loss: 1.67   conf_loss: 167.14   prob_loss: 0.75   total_loss: 169.57\n",
      "=> STEP 2347/6250   lr: 0.000741   giou_loss: 1.60   conf_loss: 166.99   prob_loss: 0.75   total_loss: 169.34\n",
      "=> STEP 2348/6250   lr: 0.000741   giou_loss: 1.57   conf_loss: 166.95   prob_loss: 0.76   total_loss: 169.27\n",
      "=> STEP 2349/6250   lr: 0.000741   giou_loss: 1.58   conf_loss: 166.83   prob_loss: 0.76   total_loss: 169.16\n",
      "=> STEP 2350/6250   lr: 0.000741   giou_loss: 1.59   conf_loss: 166.72   prob_loss: 0.76   total_loss: 169.06\n",
      "=> STEP 2351/6250   lr: 0.000740   giou_loss: 1.68   conf_loss: 166.67   prob_loss: 0.76   total_loss: 169.11\n",
      "=> STEP 2352/6250   lr: 0.000740   giou_loss: 1.65   conf_loss: 166.53   prob_loss: 0.76   total_loss: 168.94\n",
      "=> STEP 2353/6250   lr: 0.000740   giou_loss: 1.59   conf_loss: 166.45   prob_loss: 0.76   total_loss: 168.80\n",
      "=> STEP 2354/6250   lr: 0.000740   giou_loss: 1.68   conf_loss: 166.38   prob_loss: 0.76   total_loss: 168.82\n",
      "=> STEP 2355/6250   lr: 0.000740   giou_loss: 1.64   conf_loss: 166.25   prob_loss: 0.76   total_loss: 168.65\n",
      "=> STEP 2356/6250   lr: 0.000739   giou_loss: 1.57   conf_loss: 166.19   prob_loss: 0.76   total_loss: 168.51\n",
      "=> STEP 2357/6250   lr: 0.000739   giou_loss: 1.66   conf_loss: 166.10   prob_loss: 0.75   total_loss: 168.52\n",
      "=> STEP 2358/6250   lr: 0.000739   giou_loss: 1.59   conf_loss: 165.99   prob_loss: 0.75   total_loss: 168.33\n",
      "=> STEP 2359/6250   lr: 0.000739   giou_loss: 1.67   conf_loss: 165.93   prob_loss: 0.75   total_loss: 168.35\n",
      "=> STEP 2360/6250   lr: 0.000738   giou_loss: 1.62   conf_loss: 165.82   prob_loss: 0.75   total_loss: 168.20\n",
      "=> STEP 2361/6250   lr: 0.000738   giou_loss: 1.65   conf_loss: 165.71   prob_loss: 0.76   total_loss: 168.11\n",
      "=> STEP 2362/6250   lr: 0.000738   giou_loss: 1.68   conf_loss: 165.64   prob_loss: 0.76   total_loss: 168.08\n",
      "=> STEP 2363/6250   lr: 0.000738   giou_loss: 1.57   conf_loss: 165.53   prob_loss: 0.75   total_loss: 167.85\n",
      "=> STEP 2364/6250   lr: 0.000738   giou_loss: 1.59   conf_loss: 165.44   prob_loss: 0.75   total_loss: 167.78\n",
      "=> STEP 2365/6250   lr: 0.000737   giou_loss: 1.56   conf_loss: 165.36   prob_loss: 0.75   total_loss: 167.68\n",
      "=> STEP 2366/6250   lr: 0.000737   giou_loss: 1.57   conf_loss: 165.26   prob_loss: 0.75   total_loss: 167.59\n",
      "=> STEP 2367/6250   lr: 0.000737   giou_loss: 1.58   conf_loss: 165.17   prob_loss: 0.75   total_loss: 167.50\n",
      "=> STEP 2368/6250   lr: 0.000737   giou_loss: 1.56   conf_loss: 165.08   prob_loss: 0.76   total_loss: 167.40\n",
      "=> STEP 2369/6250   lr: 0.000736   giou_loss: 1.61   conf_loss: 164.98   prob_loss: 0.76   total_loss: 167.34\n",
      "=> STEP 2370/6250   lr: 0.000736   giou_loss: 1.56   conf_loss: 164.89   prob_loss: 0.75   total_loss: 167.20\n",
      "=> STEP 2371/6250   lr: 0.000736   giou_loss: 1.69   conf_loss: 164.81   prob_loss: 0.75   total_loss: 167.25\n",
      "=> STEP 2372/6250   lr: 0.000736   giou_loss: 1.59   conf_loss: 164.71   prob_loss: 0.75   total_loss: 167.05\n",
      "=> STEP 2373/6250   lr: 0.000736   giou_loss: 1.70   conf_loss: 164.62   prob_loss: 0.76   total_loss: 167.07\n",
      "=> STEP 2374/6250   lr: 0.000735   giou_loss: 1.75   conf_loss: 164.53   prob_loss: 0.76   total_loss: 167.04\n",
      "=> STEP 2375/6250   lr: 0.000735   giou_loss: 1.63   conf_loss: 164.45   prob_loss: 0.75   total_loss: 166.84\n",
      "=> STEP 2376/6250   lr: 0.000735   giou_loss: 1.63   conf_loss: 164.37   prob_loss: 0.74   total_loss: 166.75\n",
      "=> STEP 2377/6250   lr: 0.000735   giou_loss: 1.68   conf_loss: 164.28   prob_loss: 0.75   total_loss: 166.70\n",
      "=> STEP 2378/6250   lr: 0.000734   giou_loss: 1.59   conf_loss: 164.19   prob_loss: 0.75   total_loss: 166.53\n",
      "=> STEP 2379/6250   lr: 0.000734   giou_loss: 1.69   conf_loss: 164.08   prob_loss: 0.76   total_loss: 166.54\n",
      "=> STEP 2380/6250   lr: 0.000734   giou_loss: 1.75   conf_loss: 163.99   prob_loss: 0.76   total_loss: 166.51\n",
      "=> STEP 2381/6250   lr: 0.000734   giou_loss: 1.73   conf_loss: 163.91   prob_loss: 0.76   total_loss: 166.39\n",
      "=> STEP 2382/6250   lr: 0.000734   giou_loss: 1.56   conf_loss: 163.82   prob_loss: 0.75   total_loss: 166.14\n",
      "=> STEP 2383/6250   lr: 0.000733   giou_loss: 1.69   conf_loss: 163.74   prob_loss: 0.75   total_loss: 166.18\n",
      "=> STEP 2384/6250   lr: 0.000733   giou_loss: 1.74   conf_loss: 163.65   prob_loss: 0.75   total_loss: 166.14\n",
      "=> STEP 2385/6250   lr: 0.000733   giou_loss: 1.59   conf_loss: 163.55   prob_loss: 0.75   total_loss: 165.89\n",
      "=> STEP 2386/6250   lr: 0.000733   giou_loss: 1.73   conf_loss: 163.46   prob_loss: 0.76   total_loss: 165.95\n",
      "=> STEP 2387/6250   lr: 0.000732   giou_loss: 1.80   conf_loss: 163.36   prob_loss: 0.76   total_loss: 165.92\n",
      "=> STEP 2388/6250   lr: 0.000732   giou_loss: 1.77   conf_loss: 163.27   prob_loss: 0.76   total_loss: 165.80\n",
      "=> STEP 2389/6250   lr: 0.000732   giou_loss: 1.64   conf_loss: 163.19   prob_loss: 0.76   total_loss: 165.58\n",
      "=> STEP 2390/6250   lr: 0.000732   giou_loss: 1.69   conf_loss: 163.10   prob_loss: 0.75   total_loss: 165.54\n",
      "=> STEP 2391/6250   lr: 0.000732   giou_loss: 1.78   conf_loss: 163.02   prob_loss: 0.75   total_loss: 165.56\n",
      "=> STEP 2392/6250   lr: 0.000731   giou_loss: 1.81   conf_loss: 162.92   prob_loss: 0.75   total_loss: 165.48\n",
      "=> STEP 2393/6250   lr: 0.000731   giou_loss: 1.61   conf_loss: 162.83   prob_loss: 0.76   total_loss: 165.20\n",
      "=> STEP 2394/6250   lr: 0.000731   giou_loss: 1.71   conf_loss: 162.74   prob_loss: 0.76   total_loss: 165.21\n",
      "=> STEP 2395/6250   lr: 0.000731   giou_loss: 1.81   conf_loss: 162.64   prob_loss: 0.76   total_loss: 165.21\n",
      "=> STEP 2396/6250   lr: 0.000730   giou_loss: 1.80   conf_loss: 162.56   prob_loss: 0.76   total_loss: 165.12\n",
      "=> STEP 2397/6250   lr: 0.000730   giou_loss: 1.68   conf_loss: 162.49   prob_loss: 0.75   total_loss: 164.92\n",
      "=> STEP 2398/6250   lr: 0.000730   giou_loss: 1.61   conf_loss: 162.40   prob_loss: 0.75   total_loss: 164.76\n",
      "=> STEP 2399/6250   lr: 0.000730   giou_loss: 1.71   conf_loss: 162.31   prob_loss: 0.75   total_loss: 164.77\n",
      "=> STEP 2400/6250   lr: 0.000729   giou_loss: 1.63   conf_loss: 162.23   prob_loss: 0.75   total_loss: 164.62\n",
      "=> STEP 2401/6250   lr: 0.000729   giou_loss: 1.60   conf_loss: 162.12   prob_loss: 0.76   total_loss: 164.48\n",
      "=> STEP 2402/6250   lr: 0.000729   giou_loss: 1.65   conf_loss: 162.04   prob_loss: 0.76   total_loss: 164.45\n",
      "=> STEP 2403/6250   lr: 0.000729   giou_loss: 1.56   conf_loss: 161.97   prob_loss: 0.75   total_loss: 164.28\n",
      "=> STEP 2404/6250   lr: 0.000729   giou_loss: 1.61   conf_loss: 161.87   prob_loss: 0.75   total_loss: 164.23\n",
      "=> STEP 2405/6250   lr: 0.000728   giou_loss: 1.58   conf_loss: 161.78   prob_loss: 0.75   total_loss: 164.11\n",
      "=> STEP 2406/6250   lr: 0.000728   giou_loss: 1.56   conf_loss: 161.70   prob_loss: 0.75   total_loss: 164.02\n",
      "=> STEP 2407/6250   lr: 0.000728   giou_loss: 1.61   conf_loss: 161.60   prob_loss: 0.76   total_loss: 163.97\n",
      "=> STEP 2408/6250   lr: 0.000728   giou_loss: 1.56   conf_loss: 161.52   prob_loss: 0.75   total_loss: 163.84\n",
      "=> STEP 2409/6250   lr: 0.000727   giou_loss: 1.64   conf_loss: 161.44   prob_loss: 0.75   total_loss: 163.83\n",
      "=> STEP 2410/6250   lr: 0.000727   giou_loss: 1.57   conf_loss: 161.35   prob_loss: 0.75   total_loss: 163.67\n",
      "=> STEP 2411/6250   lr: 0.000727   giou_loss: 1.72   conf_loss: 161.26   prob_loss: 0.75   total_loss: 163.73\n",
      "=> STEP 2412/6250   lr: 0.000727   giou_loss: 1.71   conf_loss: 161.17   prob_loss: 0.75   total_loss: 163.64\n",
      "=> STEP 2413/6250   lr: 0.000727   giou_loss: 1.64   conf_loss: 161.08   prob_loss: 0.75   total_loss: 163.48\n",
      "=> STEP 2414/6250   lr: 0.000726   giou_loss: 1.64   conf_loss: 161.01   prob_loss: 0.75   total_loss: 163.39\n",
      "=> STEP 2415/6250   lr: 0.000726   giou_loss: 1.68   conf_loss: 160.92   prob_loss: 0.75   total_loss: 163.35\n",
      "=> STEP 2416/6250   lr: 0.000726   giou_loss: 1.59   conf_loss: 160.83   prob_loss: 0.75   total_loss: 163.17\n",
      "=> STEP 2417/6250   lr: 0.000726   giou_loss: 1.70   conf_loss: 160.74   prob_loss: 0.75   total_loss: 163.19\n",
      "=> STEP 2418/6250   lr: 0.000725   giou_loss: 1.73   conf_loss: 160.65   prob_loss: 0.75   total_loss: 163.14\n",
      "=> STEP 2419/6250   lr: 0.000725   giou_loss: 1.70   conf_loss: 160.57   prob_loss: 0.76   total_loss: 163.02\n",
      "=> STEP 2420/6250   lr: 0.000725   giou_loss: 1.56   conf_loss: 160.49   prob_loss: 0.75   total_loss: 162.80\n",
      "=> STEP 2421/6250   lr: 0.000725   giou_loss: 1.71   conf_loss: 160.40   prob_loss: 0.75   total_loss: 162.87\n",
      "=> STEP 2422/6250   lr: 0.000725   giou_loss: 1.72   conf_loss: 160.32   prob_loss: 0.75   total_loss: 162.79\n",
      "=> STEP 2423/6250   lr: 0.000724   giou_loss: 1.62   conf_loss: 160.23   prob_loss: 0.75   total_loss: 162.59\n",
      "=> STEP 2424/6250   lr: 0.000724   giou_loss: 1.68   conf_loss: 160.14   prob_loss: 0.75   total_loss: 162.57\n",
      "=> STEP 2425/6250   lr: 0.000724   giou_loss: 1.74   conf_loss: 160.05   prob_loss: 0.75   total_loss: 162.54\n",
      "=> STEP 2426/6250   lr: 0.000724   giou_loss: 1.76   conf_loss: 159.96   prob_loss: 0.75   total_loss: 162.47\n",
      "=> STEP 2427/6250   lr: 0.000723   giou_loss: 1.61   conf_loss: 159.89   prob_loss: 0.75   total_loss: 162.25\n",
      "=> STEP 2428/6250   lr: 0.000723   giou_loss: 1.65   conf_loss: 159.81   prob_loss: 0.74   total_loss: 162.21\n",
      "=> STEP 2429/6250   lr: 0.000723   giou_loss: 1.76   conf_loss: 159.72   prob_loss: 0.74   total_loss: 162.22\n",
      "=> STEP 2430/6250   lr: 0.000723   giou_loss: 1.70   conf_loss: 159.64   prob_loss: 0.75   total_loss: 162.08\n",
      "=> STEP 2431/6250   lr: 0.000723   giou_loss: 1.66   conf_loss: 159.54   prob_loss: 0.75   total_loss: 161.96\n",
      "=> STEP 2432/6250   lr: 0.000722   giou_loss: 1.68   conf_loss: 159.45   prob_loss: 0.76   total_loss: 161.89\n",
      "=> STEP 2433/6250   lr: 0.000722   giou_loss: 1.77   conf_loss: 159.38   prob_loss: 0.76   total_loss: 161.91\n",
      "=> STEP 2434/6250   lr: 0.000722   giou_loss: 1.74   conf_loss: 159.30   prob_loss: 0.75   total_loss: 161.80\n",
      "=> STEP 2435/6250   lr: 0.000722   giou_loss: 1.56   conf_loss: 159.21   prob_loss: 0.75   total_loss: 161.53\n",
      "=> STEP 2436/6250   lr: 0.000721   giou_loss: 1.72   conf_loss: 159.12   prob_loss: 0.74   total_loss: 161.59\n",
      "=> STEP 2437/6250   lr: 0.000721   giou_loss: 1.69   conf_loss: 159.05   prob_loss: 0.74   total_loss: 161.48\n",
      "=> STEP 2438/6250   lr: 0.000721   giou_loss: 1.59   conf_loss: 158.97   prob_loss: 0.74   total_loss: 161.30\n",
      "=> STEP 2439/6250   lr: 0.000721   giou_loss: 1.65   conf_loss: 158.86   prob_loss: 0.75   total_loss: 161.26\n",
      "=> STEP 2440/6250   lr: 0.000720   giou_loss: 1.58   conf_loss: 158.79   prob_loss: 0.75   total_loss: 161.12\n",
      "=> STEP 2441/6250   lr: 0.000720   giou_loss: 1.56   conf_loss: 158.70   prob_loss: 0.75   total_loss: 161.01\n",
      "=> STEP 2442/6250   lr: 0.000720   giou_loss: 1.56   conf_loss: 158.61   prob_loss: 0.75   total_loss: 160.92\n",
      "=> STEP 2443/6250   lr: 0.000720   giou_loss: 1.56   conf_loss: 158.53   prob_loss: 0.74   total_loss: 160.84\n",
      "=> STEP 2444/6250   lr: 0.000720   giou_loss: 1.56   conf_loss: 158.45   prob_loss: 0.74   total_loss: 160.75\n",
      "=> STEP 2445/6250   lr: 0.000719   giou_loss: 1.56   conf_loss: 158.36   prob_loss: 0.74   total_loss: 160.67\n",
      "=> STEP 2446/6250   lr: 0.000719   giou_loss: 1.62   conf_loss: 158.27   prob_loss: 0.74   total_loss: 160.64\n",
      "=> STEP 2447/6250   lr: 0.000719   giou_loss: 1.60   conf_loss: 158.19   prob_loss: 0.74   total_loss: 160.53\n",
      "=> STEP 2448/6250   lr: 0.000719   giou_loss: 1.62   conf_loss: 158.11   prob_loss: 0.74   total_loss: 160.46\n",
      "=> STEP 2449/6250   lr: 0.000718   giou_loss: 1.62   conf_loss: 158.02   prob_loss: 0.74   total_loss: 160.38\n",
      "=> STEP 2450/6250   lr: 0.000718   giou_loss: 1.57   conf_loss: 157.93   prob_loss: 0.74   total_loss: 160.25\n",
      "=> STEP 2451/6250   lr: 0.000718   giou_loss: 1.56   conf_loss: 157.84   prob_loss: 0.74   total_loss: 160.15\n",
      "=> STEP 2452/6250   lr: 0.000718   giou_loss: 1.56   conf_loss: 157.76   prob_loss: 0.74   total_loss: 160.07\n",
      "=> STEP 2453/6250   lr: 0.000718   giou_loss: 1.56   conf_loss: 157.68   prob_loss: 0.74   total_loss: 159.98\n",
      "=> STEP 2454/6250   lr: 0.000717   giou_loss: 1.56   conf_loss: 157.59   prob_loss: 0.74   total_loss: 159.90\n",
      "=> STEP 2455/6250   lr: 0.000717   giou_loss: 1.57   conf_loss: 157.51   prob_loss: 0.74   total_loss: 159.82\n",
      "=> STEP 2456/6250   lr: 0.000717   giou_loss: 1.57   conf_loss: 157.43   prob_loss: 0.74   total_loss: 159.74\n",
      "=> STEP 2457/6250   lr: 0.000717   giou_loss: 1.74   conf_loss: 157.34   prob_loss: 0.74   total_loss: 159.82\n",
      "=> STEP 2458/6250   lr: 0.000716   giou_loss: 1.70   conf_loss: 157.26   prob_loss: 0.74   total_loss: 159.70\n",
      "=> STEP 2459/6250   lr: 0.000716   giou_loss: 1.59   conf_loss: 157.18   prob_loss: 0.74   total_loss: 159.51\n",
      "=> STEP 2460/6250   lr: 0.000716   giou_loss: 1.66   conf_loss: 157.10   prob_loss: 0.74   total_loss: 159.50\n",
      "=> STEP 2461/6250   lr: 0.000716   giou_loss: 1.68   conf_loss: 157.01   prob_loss: 0.74   total_loss: 159.43\n",
      "=> STEP 2462/6250   lr: 0.000715   giou_loss: 1.64   conf_loss: 156.94   prob_loss: 0.74   total_loss: 159.32\n",
      "=> STEP 2463/6250   lr: 0.000715   giou_loss: 1.65   conf_loss: 156.86   prob_loss: 0.73   total_loss: 159.24\n",
      "=> STEP 2464/6250   lr: 0.000715   giou_loss: 1.70   conf_loss: 156.78   prob_loss: 0.73   total_loss: 159.21\n",
      "=> STEP 2465/6250   lr: 0.000715   giou_loss: 1.63   conf_loss: 156.71   prob_loss: 0.74   total_loss: 159.07\n",
      "=> STEP 2466/6250   lr: 0.000715   giou_loss: 1.67   conf_loss: 156.62   prob_loss: 0.75   total_loss: 159.04\n",
      "=> STEP 2467/6250   lr: 0.000714   giou_loss: 1.66   conf_loss: 156.53   prob_loss: 0.74   total_loss: 158.94\n",
      "=> STEP 2468/6250   lr: 0.000714   giou_loss: 1.57   conf_loss: 156.48   prob_loss: 0.74   total_loss: 158.79\n",
      "=> STEP 2469/6250   lr: 0.000714   giou_loss: 1.62   conf_loss: 156.38   prob_loss: 0.73   total_loss: 158.73\n",
      "=> STEP 2470/6250   lr: 0.000714   giou_loss: 1.58   conf_loss: 156.30   prob_loss: 0.74   total_loss: 158.62\n",
      "=> STEP 2471/6250   lr: 0.000713   giou_loss: 1.59   conf_loss: 156.23   prob_loss: 0.74   total_loss: 158.56\n",
      "=> STEP 2472/6250   lr: 0.000713   giou_loss: 1.60   conf_loss: 156.14   prob_loss: 0.74   total_loss: 158.48\n",
      "=> STEP 2473/6250   lr: 0.000713   giou_loss: 1.57   conf_loss: 156.07   prob_loss: 0.73   total_loss: 158.37\n",
      "=> STEP 2474/6250   lr: 0.000713   giou_loss: 1.57   conf_loss: 155.99   prob_loss: 0.73   total_loss: 158.29\n",
      "=> STEP 2475/6250   lr: 0.000713   giou_loss: 1.57   conf_loss: 155.89   prob_loss: 0.73   total_loss: 158.19\n",
      "=> STEP 2476/6250   lr: 0.000712   giou_loss: 1.60   conf_loss: 155.82   prob_loss: 0.73   total_loss: 158.15\n",
      "=> STEP 2477/6250   lr: 0.000712   giou_loss: 1.56   conf_loss: 155.72   prob_loss: 0.74   total_loss: 158.02\n",
      "=> STEP 2478/6250   lr: 0.000712   giou_loss: 1.66   conf_loss: 155.63   prob_loss: 0.74   total_loss: 158.03\n",
      "=> STEP 2479/6250   lr: 0.000712   giou_loss: 1.68   conf_loss: 155.55   prob_loss: 0.74   total_loss: 157.97\n",
      "=> STEP 2480/6250   lr: 0.000711   giou_loss: 1.58   conf_loss: 155.47   prob_loss: 0.73   total_loss: 157.78\n",
      "=> STEP 2481/6250   lr: 0.000711   giou_loss: 1.71   conf_loss: 155.41   prob_loss: 0.72   total_loss: 157.85\n",
      "=> STEP 2482/6250   lr: 0.000711   giou_loss: 1.82   conf_loss: 155.32   prob_loss: 0.72   total_loss: 157.87\n",
      "=> STEP 2483/6250   lr: 0.000711   giou_loss: 1.74   conf_loss: 155.25   prob_loss: 0.73   total_loss: 157.72\n",
      "=> STEP 2484/6250   lr: 0.000710   giou_loss: 1.57   conf_loss: 155.15   prob_loss: 0.74   total_loss: 157.45\n",
      "=> STEP 2485/6250   lr: 0.000710   giou_loss: 1.84   conf_loss: 155.07   prob_loss: 0.74   total_loss: 157.65\n",
      "=> STEP 2486/6250   lr: 0.000710   giou_loss: 1.94   conf_loss: 154.99   prob_loss: 0.74   total_loss: 157.67\n",
      "=> STEP 2487/6250   lr: 0.000710   giou_loss: 1.90   conf_loss: 154.90   prob_loss: 0.74   total_loss: 157.54\n",
      "=> STEP 2488/6250   lr: 0.000710   giou_loss: 1.84   conf_loss: 154.83   prob_loss: 0.74   total_loss: 157.41\n",
      "=> STEP 2489/6250   lr: 0.000709   giou_loss: 1.74   conf_loss: 154.74   prob_loss: 0.73   total_loss: 157.21\n",
      "=> STEP 2490/6250   lr: 0.000709   giou_loss: 1.66   conf_loss: 154.67   prob_loss: 0.73   total_loss: 157.06\n",
      "=> STEP 2491/6250   lr: 0.000709   giou_loss: 1.77   conf_loss: 154.59   prob_loss: 0.73   total_loss: 157.09\n",
      "=> STEP 2492/6250   lr: 0.000709   giou_loss: 1.77   conf_loss: 154.50   prob_loss: 0.74   total_loss: 157.00\n",
      "=> STEP 2493/6250   lr: 0.000708   giou_loss: 1.65   conf_loss: 154.41   prob_loss: 0.74   total_loss: 156.80\n",
      "=> STEP 2494/6250   lr: 0.000708   giou_loss: 1.65   conf_loss: 154.33   prob_loss: 0.74   total_loss: 156.73\n",
      "=> STEP 2495/6250   lr: 0.000708   giou_loss: 1.82   conf_loss: 154.25   prob_loss: 0.74   total_loss: 156.81\n",
      "=> STEP 2496/6250   lr: 0.000708   giou_loss: 1.75   conf_loss: 154.17   prob_loss: 0.74   total_loss: 156.66\n",
      "=> STEP 2497/6250   lr: 0.000707   giou_loss: 1.57   conf_loss: 154.10   prob_loss: 0.74   total_loss: 156.41\n",
      "=> STEP 2498/6250   lr: 0.000707   giou_loss: 1.75   conf_loss: 154.02   prob_loss: 0.74   total_loss: 156.51\n",
      "=> STEP 2499/6250   lr: 0.000707   giou_loss: 1.84   conf_loss: 153.97   prob_loss: 0.73   total_loss: 156.54\n",
      "=> STEP 2500/6250   lr: 0.000707   giou_loss: 1.88   conf_loss: 153.85   prob_loss: 0.73   total_loss: 156.46\n",
      "=> STEP 2501/6250   lr: 0.000707   giou_loss: 1.74   conf_loss: 153.81   prob_loss: 0.74   total_loss: 156.28\n",
      "=> STEP 2502/6250   lr: 0.000706   giou_loss: 1.56   conf_loss: 153.70   prob_loss: 0.74   total_loss: 156.01\n",
      "=> STEP 2503/6250   lr: 0.000706   giou_loss: 1.79   conf_loss: 153.62   prob_loss: 0.75   total_loss: 156.16\n",
      "=> STEP 2504/6250   lr: 0.000706   giou_loss: 1.83   conf_loss: 153.56   prob_loss: 0.75   total_loss: 156.13\n",
      "=> STEP 2505/6250   lr: 0.000706   giou_loss: 1.76   conf_loss: 153.47   prob_loss: 0.74   total_loss: 155.96\n",
      "=> STEP 2506/6250   lr: 0.000705   giou_loss: 1.63   conf_loss: 153.41   prob_loss: 0.73   total_loss: 155.78\n",
      "=> STEP 2507/6250   lr: 0.000705   giou_loss: 1.71   conf_loss: 153.31   prob_loss: 0.73   total_loss: 155.76\n",
      "=> STEP 2508/6250   lr: 0.000705   giou_loss: 1.81   conf_loss: 153.24   prob_loss: 0.74   total_loss: 155.79\n",
      "=> STEP 2509/6250   lr: 0.000705   giou_loss: 1.80   conf_loss: 153.15   prob_loss: 0.74   total_loss: 155.69\n",
      "=> STEP 2510/6250   lr: 0.000704   giou_loss: 1.68   conf_loss: 153.06   prob_loss: 0.74   total_loss: 155.47\n",
      "=> STEP 2511/6250   lr: 0.000704   giou_loss: 1.65   conf_loss: 152.99   prob_loss: 0.74   total_loss: 155.38\n",
      "=> STEP 2512/6250   lr: 0.000704   giou_loss: 1.71   conf_loss: 152.90   prob_loss: 0.74   total_loss: 155.34\n",
      "=> STEP 2513/6250   lr: 0.000704   giou_loss: 1.65   conf_loss: 152.82   prob_loss: 0.74   total_loss: 155.21\n",
      "=> STEP 2514/6250   lr: 0.000704   giou_loss: 1.63   conf_loss: 152.74   prob_loss: 0.74   total_loss: 155.11\n",
      "=> STEP 2515/6250   lr: 0.000703   giou_loss: 1.62   conf_loss: 152.66   prob_loss: 0.73   total_loss: 155.02\n",
      "=> STEP 2516/6250   lr: 0.000703   giou_loss: 1.57   conf_loss: 152.59   prob_loss: 0.73   total_loss: 154.88\n",
      "=> STEP 2517/6250   lr: 0.000703   giou_loss: 1.71   conf_loss: 152.50   prob_loss: 0.73   total_loss: 154.94\n",
      "=> STEP 2518/6250   lr: 0.000703   giou_loss: 1.61   conf_loss: 152.42   prob_loss: 0.73   total_loss: 154.76\n",
      "=> STEP 2519/6250   lr: 0.000702   giou_loss: 1.62   conf_loss: 152.34   prob_loss: 0.73   total_loss: 154.70\n",
      "=> STEP 2520/6250   lr: 0.000702   giou_loss: 1.65   conf_loss: 152.26   prob_loss: 0.73   total_loss: 154.64\n",
      "=> STEP 2521/6250   lr: 0.000702   giou_loss: 1.56   conf_loss: 152.18   prob_loss: 0.73   total_loss: 154.48\n",
      "=> STEP 2522/6250   lr: 0.000702   giou_loss: 1.67   conf_loss: 152.10   prob_loss: 0.73   total_loss: 154.50\n",
      "=> STEP 2523/6250   lr: 0.000701   giou_loss: 1.62   conf_loss: 152.03   prob_loss: 0.73   total_loss: 154.38\n",
      "=> STEP 2524/6250   lr: 0.000701   giou_loss: 1.59   conf_loss: 151.95   prob_loss: 0.73   total_loss: 154.27\n",
      "=> STEP 2525/6250   lr: 0.000701   giou_loss: 1.59   conf_loss: 151.88   prob_loss: 0.73   total_loss: 154.20\n",
      "=> STEP 2526/6250   lr: 0.000701   giou_loss: 1.63   conf_loss: 151.79   prob_loss: 0.73   total_loss: 154.15\n",
      "=> STEP 2527/6250   lr: 0.000701   giou_loss: 1.58   conf_loss: 151.72   prob_loss: 0.73   total_loss: 154.03\n",
      "=> STEP 2528/6250   lr: 0.000700   giou_loss: 1.62   conf_loss: 151.64   prob_loss: 0.73   total_loss: 153.99\n",
      "=> STEP 2529/6250   lr: 0.000700   giou_loss: 1.65   conf_loss: 151.56   prob_loss: 0.73   total_loss: 153.94\n",
      "=> STEP 2530/6250   lr: 0.000700   giou_loss: 1.58   conf_loss: 151.50   prob_loss: 0.73   total_loss: 153.80\n",
      "=> STEP 2531/6250   lr: 0.000700   giou_loss: 1.57   conf_loss: 151.40   prob_loss: 0.73   total_loss: 153.69\n",
      "=> STEP 2532/6250   lr: 0.000699   giou_loss: 1.60   conf_loss: 151.34   prob_loss: 0.72   total_loss: 153.66\n",
      "=> STEP 2533/6250   lr: 0.000699   giou_loss: 1.56   conf_loss: 151.25   prob_loss: 0.73   total_loss: 153.54\n",
      "=> STEP 2534/6250   lr: 0.000699   giou_loss: 1.56   conf_loss: 151.17   prob_loss: 0.73   total_loss: 153.46\n",
      "=> STEP 2535/6250   lr: 0.000699   giou_loss: 1.61   conf_loss: 151.09   prob_loss: 0.73   total_loss: 153.43\n",
      "=> STEP 2536/6250   lr: 0.000698   giou_loss: 1.57   conf_loss: 151.01   prob_loss: 0.73   total_loss: 153.30\n",
      "=> STEP 2537/6250   lr: 0.000698   giou_loss: 1.62   conf_loss: 150.94   prob_loss: 0.73   total_loss: 153.28\n",
      "=> STEP 2538/6250   lr: 0.000698   giou_loss: 1.60   conf_loss: 150.85   prob_loss: 0.72   total_loss: 153.18\n",
      "=> STEP 2539/6250   lr: 0.000698   giou_loss: 1.64   conf_loss: 150.79   prob_loss: 0.72   total_loss: 153.15\n",
      "=> STEP 2540/6250   lr: 0.000698   giou_loss: 1.61   conf_loss: 150.70   prob_loss: 0.72   total_loss: 153.03\n",
      "=> STEP 2541/6250   lr: 0.000697   giou_loss: 1.58   conf_loss: 150.62   prob_loss: 0.73   total_loss: 152.93\n",
      "=> STEP 2542/6250   lr: 0.000697   giou_loss: 1.57   conf_loss: 150.54   prob_loss: 0.73   total_loss: 152.83\n",
      "=> STEP 2543/6250   lr: 0.000697   giou_loss: 1.63   conf_loss: 150.47   prob_loss: 0.72   total_loss: 152.83\n",
      "=> STEP 2544/6250   lr: 0.000697   giou_loss: 1.64   conf_loss: 150.39   prob_loss: 0.72   total_loss: 152.76\n",
      "=> STEP 2545/6250   lr: 0.000696   giou_loss: 1.56   conf_loss: 150.32   prob_loss: 0.73   total_loss: 152.61\n",
      "=> STEP 2546/6250   lr: 0.000696   giou_loss: 1.68   conf_loss: 150.24   prob_loss: 0.73   total_loss: 152.64\n",
      "=> STEP 2547/6250   lr: 0.000696   giou_loss: 1.64   conf_loss: 150.17   prob_loss: 0.73   total_loss: 152.54\n",
      "=> STEP 2548/6250   lr: 0.000696   giou_loss: 1.56   conf_loss: 150.10   prob_loss: 0.72   total_loss: 152.39\n",
      "=> STEP 2549/6250   lr: 0.000695   giou_loss: 1.69   conf_loss: 150.02   prob_loss: 0.72   total_loss: 152.43\n",
      "=> STEP 2550/6250   lr: 0.000695   giou_loss: 1.67   conf_loss: 149.95   prob_loss: 0.72   total_loss: 152.34\n",
      "=> STEP 2551/6250   lr: 0.000695   giou_loss: 1.60   conf_loss: 149.86   prob_loss: 0.72   total_loss: 152.18\n",
      "=> STEP 2552/6250   lr: 0.000695   giou_loss: 1.64   conf_loss: 149.78   prob_loss: 0.73   total_loss: 152.15\n",
      "=> STEP 2553/6250   lr: 0.000695   giou_loss: 1.65   conf_loss: 149.70   prob_loss: 0.73   total_loss: 152.08\n",
      "=> STEP 2554/6250   lr: 0.000694   giou_loss: 1.60   conf_loss: 149.64   prob_loss: 0.72   total_loss: 151.96\n",
      "=> STEP 2555/6250   lr: 0.000694   giou_loss: 1.60   conf_loss: 149.57   prob_loss: 0.71   total_loss: 151.88\n",
      "=> STEP 2556/6250   lr: 0.000694   giou_loss: 1.68   conf_loss: 149.49   prob_loss: 0.72   total_loss: 151.89\n",
      "=> STEP 2557/6250   lr: 0.000694   giou_loss: 1.63   conf_loss: 149.42   prob_loss: 0.72   total_loss: 151.77\n",
      "=> STEP 2558/6250   lr: 0.000693   giou_loss: 1.61   conf_loss: 149.33   prob_loss: 0.72   total_loss: 151.66\n",
      "=> STEP 2559/6250   lr: 0.000693   giou_loss: 1.56   conf_loss: 149.27   prob_loss: 0.73   total_loss: 151.56\n",
      "=> STEP 2560/6250   lr: 0.000693   giou_loss: 1.56   conf_loss: 149.19   prob_loss: 0.72   total_loss: 151.47\n",
      "=> STEP 2561/6250   lr: 0.000693   giou_loss: 1.56   conf_loss: 149.11   prob_loss: 0.72   total_loss: 151.39\n",
      "=> STEP 2562/6250   lr: 0.000692   giou_loss: 1.57   conf_loss: 149.04   prob_loss: 0.72   total_loss: 151.32\n",
      "=> STEP 2563/6250   lr: 0.000692   giou_loss: 1.57   conf_loss: 148.95   prob_loss: 0.72   total_loss: 151.23\n",
      "=> STEP 2564/6250   lr: 0.000692   giou_loss: 1.58   conf_loss: 148.88   prob_loss: 0.72   total_loss: 151.18\n",
      "=> STEP 2565/6250   lr: 0.000692   giou_loss: 1.62   conf_loss: 148.80   prob_loss: 0.72   total_loss: 151.14\n",
      "=> STEP 2566/6250   lr: 0.000692   giou_loss: 1.73   conf_loss: 148.73   prob_loss: 0.72   total_loss: 151.17\n",
      "=> STEP 2567/6250   lr: 0.000691   giou_loss: 1.65   conf_loss: 148.64   prob_loss: 0.72   total_loss: 151.01\n",
      "=> STEP 2568/6250   lr: 0.000691   giou_loss: 1.58   conf_loss: 148.58   prob_loss: 0.72   total_loss: 150.89\n",
      "=> STEP 2569/6250   lr: 0.000691   giou_loss: 1.68   conf_loss: 148.49   prob_loss: 0.72   total_loss: 150.88\n",
      "=> STEP 2570/6250   lr: 0.000691   giou_loss: 1.64   conf_loss: 148.44   prob_loss: 0.72   total_loss: 150.80\n",
      "=> STEP 2571/6250   lr: 0.000690   giou_loss: 1.59   conf_loss: 148.35   prob_loss: 0.73   total_loss: 150.66\n",
      "=> STEP 2572/6250   lr: 0.000690   giou_loss: 1.64   conf_loss: 148.26   prob_loss: 0.73   total_loss: 150.63\n",
      "=> STEP 2573/6250   lr: 0.000690   giou_loss: 1.61   conf_loss: 148.22   prob_loss: 0.72   total_loss: 150.56\n",
      "=> STEP 2574/6250   lr: 0.000690   giou_loss: 1.57   conf_loss: 148.13   prob_loss: 0.72   total_loss: 150.41\n",
      "=> STEP 2575/6250   lr: 0.000689   giou_loss: 1.64   conf_loss: 148.07   prob_loss: 0.71   total_loss: 150.42\n",
      "=> STEP 2576/6250   lr: 0.000689   giou_loss: 1.62   conf_loss: 148.00   prob_loss: 0.71   total_loss: 150.33\n",
      "=> STEP 2577/6250   lr: 0.000689   giou_loss: 1.56   conf_loss: 147.90   prob_loss: 0.72   total_loss: 150.18\n",
      "=> STEP 2578/6250   lr: 0.000689   giou_loss: 1.65   conf_loss: 147.84   prob_loss: 0.72   total_loss: 150.21\n",
      "=> STEP 2579/6250   lr: 0.000688   giou_loss: 1.64   conf_loss: 147.75   prob_loss: 0.72   total_loss: 150.11\n",
      "=> STEP 2580/6250   lr: 0.000688   giou_loss: 1.62   conf_loss: 147.68   prob_loss: 0.72   total_loss: 150.02\n",
      "=> STEP 2581/6250   lr: 0.000688   giou_loss: 1.66   conf_loss: 147.60   prob_loss: 0.72   total_loss: 149.98\n",
      "=> STEP 2582/6250   lr: 0.000688   giou_loss: 1.61   conf_loss: 147.53   prob_loss: 0.71   total_loss: 149.85\n",
      "=> STEP 2583/6250   lr: 0.000688   giou_loss: 1.59   conf_loss: 147.45   prob_loss: 0.72   total_loss: 149.77\n",
      "=> STEP 2584/6250   lr: 0.000687   giou_loss: 1.70   conf_loss: 147.37   prob_loss: 0.72   total_loss: 149.79\n",
      "=> STEP 2585/6250   lr: 0.000687   giou_loss: 1.68   conf_loss: 147.30   prob_loss: 0.72   total_loss: 149.70\n",
      "=> STEP 2586/6250   lr: 0.000687   giou_loss: 1.59   conf_loss: 147.24   prob_loss: 0.72   total_loss: 149.55\n",
      "=> STEP 2587/6250   lr: 0.000687   giou_loss: 1.57   conf_loss: 147.15   prob_loss: 0.71   total_loss: 149.43\n",
      "=> STEP 2588/6250   lr: 0.000686   giou_loss: 1.65   conf_loss: 147.08   prob_loss: 0.71   total_loss: 149.45\n",
      "=> STEP 2589/6250   lr: 0.000686   giou_loss: 1.61   conf_loss: 147.01   prob_loss: 0.72   total_loss: 149.34\n",
      "=> STEP 2590/6250   lr: 0.000686   giou_loss: 1.56   conf_loss: 146.92   prob_loss: 0.72   total_loss: 149.21\n",
      "=> STEP 2591/6250   lr: 0.000686   giou_loss: 1.69   conf_loss: 146.85   prob_loss: 0.72   total_loss: 149.25\n",
      "=> STEP 2592/6250   lr: 0.000685   giou_loss: 1.68   conf_loss: 146.79   prob_loss: 0.72   total_loss: 149.19\n",
      "=> STEP 2593/6250   lr: 0.000685   giou_loss: 1.57   conf_loss: 146.71   prob_loss: 0.72   total_loss: 149.00\n",
      "=> STEP 2594/6250   lr: 0.000685   giou_loss: 1.67   conf_loss: 146.64   prob_loss: 0.71   total_loss: 149.02\n",
      "=> STEP 2595/6250   lr: 0.000685   giou_loss: 1.72   conf_loss: 146.58   prob_loss: 0.71   total_loss: 149.01\n",
      "=> STEP 2596/6250   lr: 0.000684   giou_loss: 1.67   conf_loss: 146.50   prob_loss: 0.71   total_loss: 148.89\n",
      "=> STEP 2597/6250   lr: 0.000684   giou_loss: 1.56   conf_loss: 146.41   prob_loss: 0.72   total_loss: 148.69\n",
      "=> STEP 2598/6250   lr: 0.000684   giou_loss: 1.61   conf_loss: 146.34   prob_loss: 0.72   total_loss: 148.67\n",
      "=> STEP 2599/6250   lr: 0.000684   giou_loss: 1.61   conf_loss: 146.27   prob_loss: 0.72   total_loss: 148.60\n",
      "=> STEP 2600/6250   lr: 0.000684   giou_loss: 1.56   conf_loss: 146.19   prob_loss: 0.72   total_loss: 148.47\n",
      "=> STEP 2601/6250   lr: 0.000683   giou_loss: 1.61   conf_loss: 146.12   prob_loss: 0.71   total_loss: 148.44\n",
      "=> STEP 2602/6250   lr: 0.000683   giou_loss: 1.57   conf_loss: 146.05   prob_loss: 0.71   total_loss: 148.34\n",
      "=> STEP 2603/6250   lr: 0.000683   giou_loss: 1.58   conf_loss: 145.99   prob_loss: 0.72   total_loss: 148.29\n",
      "=> STEP 2604/6250   lr: 0.000683   giou_loss: 1.56   conf_loss: 145.92   prob_loss: 0.71   total_loss: 148.20\n",
      "=> STEP 2605/6250   lr: 0.000682   giou_loss: 1.57   conf_loss: 145.88   prob_loss: 0.71   total_loss: 148.16\n",
      "=> STEP 2606/6250   lr: 0.000682   giou_loss: 1.57   conf_loss: 145.79   prob_loss: 0.71   total_loss: 148.06\n",
      "=> STEP 2607/6250   lr: 0.000682   giou_loss: 1.56   conf_loss: 145.74   prob_loss: 0.71   total_loss: 148.02\n",
      "=> STEP 2608/6250   lr: 0.000682   giou_loss: 1.56   conf_loss: 145.63   prob_loss: 0.71   total_loss: 147.91\n",
      "=> STEP 2609/6250   lr: 0.000681   giou_loss: 1.56   conf_loss: 145.57   prob_loss: 0.71   total_loss: 147.85\n",
      "=> STEP 2610/6250   lr: 0.000681   giou_loss: 1.56   conf_loss: 145.48   prob_loss: 0.71   total_loss: 147.75\n",
      "=> STEP 2611/6250   lr: 0.000681   giou_loss: 1.57   conf_loss: 145.40   prob_loss: 0.71   total_loss: 147.69\n",
      "=> STEP 2612/6250   lr: 0.000681   giou_loss: 1.57   conf_loss: 145.33   prob_loss: 0.71   total_loss: 147.60\n",
      "=> STEP 2613/6250   lr: 0.000681   giou_loss: 1.65   conf_loss: 145.25   prob_loss: 0.71   total_loss: 147.61\n",
      "=> STEP 2614/6250   lr: 0.000680   giou_loss: 1.61   conf_loss: 145.18   prob_loss: 0.71   total_loss: 147.50\n",
      "=> STEP 2615/6250   lr: 0.000680   giou_loss: 1.56   conf_loss: 145.11   prob_loss: 0.71   total_loss: 147.38\n",
      "=> STEP 2616/6250   lr: 0.000680   giou_loss: 1.70   conf_loss: 145.03   prob_loss: 0.71   total_loss: 147.44\n",
      "=> STEP 2617/6250   lr: 0.000680   giou_loss: 1.70   conf_loss: 144.95   prob_loss: 0.71   total_loss: 147.37\n",
      "=> STEP 2618/6250   lr: 0.000679   giou_loss: 1.61   conf_loss: 144.90   prob_loss: 0.71   total_loss: 147.23\n",
      "=> STEP 2619/6250   lr: 0.000679   giou_loss: 1.60   conf_loss: 144.80   prob_loss: 0.71   total_loss: 147.11\n",
      "=> STEP 2620/6250   lr: 0.000679   giou_loss: 1.62   conf_loss: 144.75   prob_loss: 0.71   total_loss: 147.07\n",
      "=> STEP 2621/6250   lr: 0.000679   giou_loss: 1.61   conf_loss: 144.68   prob_loss: 0.71   total_loss: 147.00\n",
      "=> STEP 2622/6250   lr: 0.000678   giou_loss: 1.57   conf_loss: 144.58   prob_loss: 0.71   total_loss: 146.87\n",
      "=> STEP 2623/6250   lr: 0.000678   giou_loss: 1.56   conf_loss: 144.53   prob_loss: 0.71   total_loss: 146.80\n",
      "=> STEP 2624/6250   lr: 0.000678   giou_loss: 1.59   conf_loss: 144.44   prob_loss: 0.71   total_loss: 146.74\n",
      "=> STEP 2625/6250   lr: 0.000678   giou_loss: 1.56   conf_loss: 144.37   prob_loss: 0.71   total_loss: 146.65\n",
      "=> STEP 2626/6250   lr: 0.000677   giou_loss: 1.57   conf_loss: 144.30   prob_loss: 0.71   total_loss: 146.57\n",
      "=> STEP 2627/6250   lr: 0.000677   giou_loss: 1.56   conf_loss: 144.23   prob_loss: 0.71   total_loss: 146.50\n",
      "=> STEP 2628/6250   lr: 0.000677   giou_loss: 1.61   conf_loss: 144.15   prob_loss: 0.71   total_loss: 146.47\n",
      "=> STEP 2629/6250   lr: 0.000677   giou_loss: 1.61   conf_loss: 144.07   prob_loss: 0.71   total_loss: 146.39\n",
      "=> STEP 2630/6250   lr: 0.000677   giou_loss: 1.62   conf_loss: 144.02   prob_loss: 0.71   total_loss: 146.35\n",
      "=> STEP 2631/6250   lr: 0.000676   giou_loss: 1.62   conf_loss: 143.94   prob_loss: 0.71   total_loss: 146.27\n",
      "=> STEP 2632/6250   lr: 0.000676   giou_loss: 1.63   conf_loss: 143.86   prob_loss: 0.71   total_loss: 146.20\n",
      "=> STEP 2633/6250   lr: 0.000676   giou_loss: 1.57   conf_loss: 143.80   prob_loss: 0.71   total_loss: 146.07\n",
      "=> STEP 2634/6250   lr: 0.000676   giou_loss: 1.56   conf_loss: 143.72   prob_loss: 0.71   total_loss: 145.99\n",
      "=> STEP 2635/6250   lr: 0.000675   giou_loss: 1.59   conf_loss: 143.64   prob_loss: 0.71   total_loss: 145.94\n",
      "=> STEP 2636/6250   lr: 0.000675   giou_loss: 1.56   conf_loss: 143.58   prob_loss: 0.71   total_loss: 145.85\n",
      "=> STEP 2637/6250   lr: 0.000675   giou_loss: 1.57   conf_loss: 143.50   prob_loss: 0.71   total_loss: 145.78\n",
      "=> STEP 2638/6250   lr: 0.000675   giou_loss: 1.59   conf_loss: 143.43   prob_loss: 0.71   total_loss: 145.73\n",
      "=> STEP 2639/6250   lr: 0.000674   giou_loss: 1.56   conf_loss: 143.37   prob_loss: 0.71   total_loss: 145.64\n",
      "=> STEP 2640/6250   lr: 0.000674   giou_loss: 1.58   conf_loss: 143.29   prob_loss: 0.71   total_loss: 145.58\n",
      "=> STEP 2641/6250   lr: 0.000674   giou_loss: 1.62   conf_loss: 143.23   prob_loss: 0.71   total_loss: 145.56\n",
      "=> STEP 2642/6250   lr: 0.000674   giou_loss: 1.62   conf_loss: 143.17   prob_loss: 0.71   total_loss: 145.49\n",
      "=> STEP 2643/6250   lr: 0.000673   giou_loss: 1.60   conf_loss: 143.10   prob_loss: 0.71   total_loss: 145.41\n",
      "=> STEP 2644/6250   lr: 0.000673   giou_loss: 1.65   conf_loss: 143.02   prob_loss: 0.71   total_loss: 145.38\n",
      "=> STEP 2645/6250   lr: 0.000673   giou_loss: 1.60   conf_loss: 142.94   prob_loss: 0.71   total_loss: 145.25\n",
      "=> STEP 2646/6250   lr: 0.000673   giou_loss: 1.59   conf_loss: 142.88   prob_loss: 0.71   total_loss: 145.18\n",
      "=> STEP 2647/6250   lr: 0.000672   giou_loss: 1.61   conf_loss: 142.80   prob_loss: 0.70   total_loss: 145.12\n",
      "=> STEP 2648/6250   lr: 0.000672   giou_loss: 1.62   conf_loss: 142.75   prob_loss: 0.70   total_loss: 145.07\n",
      "=> STEP 2649/6250   lr: 0.000672   giou_loss: 1.59   conf_loss: 142.67   prob_loss: 0.71   total_loss: 144.97\n",
      "=> STEP 2650/6250   lr: 0.000672   giou_loss: 1.59   conf_loss: 142.59   prob_loss: 0.71   total_loss: 144.89\n",
      "=> STEP 2651/6250   lr: 0.000672   giou_loss: 1.63   conf_loss: 142.56   prob_loss: 0.70   total_loss: 144.89\n",
      "=> STEP 2652/6250   lr: 0.000671   giou_loss: 1.63   conf_loss: 142.45   prob_loss: 0.70   total_loss: 144.78\n",
      "=> STEP 2653/6250   lr: 0.000671   giou_loss: 1.57   conf_loss: 142.40   prob_loss: 0.70   total_loss: 144.67\n",
      "=> STEP 2654/6250   lr: 0.000671   giou_loss: 1.57   conf_loss: 142.32   prob_loss: 0.70   total_loss: 144.59\n",
      "=> STEP 2655/6250   lr: 0.000671   giou_loss: 1.60   conf_loss: 142.24   prob_loss: 0.70   total_loss: 144.53\n",
      "=> STEP 2656/6250   lr: 0.000670   giou_loss: 1.56   conf_loss: 142.17   prob_loss: 0.70   total_loss: 144.44\n",
      "=> STEP 2657/6250   lr: 0.000670   giou_loss: 1.62   conf_loss: 142.09   prob_loss: 0.71   total_loss: 144.41\n",
      "=> STEP 2658/6250   lr: 0.000670   giou_loss: 1.58   conf_loss: 142.02   prob_loss: 0.70   total_loss: 144.31\n",
      "=> STEP 2659/6250   lr: 0.000670   giou_loss: 1.67   conf_loss: 141.95   prob_loss: 0.70   total_loss: 144.32\n",
      "=> STEP 2660/6250   lr: 0.000669   giou_loss: 1.68   conf_loss: 141.88   prob_loss: 0.70   total_loss: 144.26\n",
      "=> STEP 2661/6250   lr: 0.000669   giou_loss: 1.58   conf_loss: 141.81   prob_loss: 0.70   total_loss: 144.10\n",
      "=> STEP 2662/6250   lr: 0.000669   giou_loss: 1.58   conf_loss: 141.74   prob_loss: 0.70   total_loss: 144.03\n",
      "=> STEP 2663/6250   lr: 0.000669   giou_loss: 1.64   conf_loss: 141.67   prob_loss: 0.71   total_loss: 144.01\n",
      "=> STEP 2664/6250   lr: 0.000668   giou_loss: 1.64   conf_loss: 141.60   prob_loss: 0.70   total_loss: 143.95\n",
      "=> STEP 2665/6250   lr: 0.000668   giou_loss: 1.58   conf_loss: 141.54   prob_loss: 0.70   total_loss: 143.82\n",
      "=> STEP 2666/6250   lr: 0.000668   giou_loss: 1.65   conf_loss: 141.47   prob_loss: 0.70   total_loss: 143.82\n",
      "=> STEP 2667/6250   lr: 0.000668   giou_loss: 1.68   conf_loss: 141.40   prob_loss: 0.70   total_loss: 143.79\n",
      "=> STEP 2668/6250   lr: 0.000668   giou_loss: 1.58   conf_loss: 141.32   prob_loss: 0.71   total_loss: 143.61\n",
      "=> STEP 2669/6250   lr: 0.000667   giou_loss: 1.59   conf_loss: 141.26   prob_loss: 0.70   total_loss: 143.55\n",
      "=> STEP 2670/6250   lr: 0.000667   giou_loss: 1.60   conf_loss: 141.19   prob_loss: 0.70   total_loss: 143.48\n",
      "=> STEP 2671/6250   lr: 0.000667   giou_loss: 1.57   conf_loss: 141.13   prob_loss: 0.70   total_loss: 143.40\n",
      "=> STEP 2672/6250   lr: 0.000667   giou_loss: 1.64   conf_loss: 141.05   prob_loss: 0.70   total_loss: 143.39\n",
      "=> STEP 2673/6250   lr: 0.000666   giou_loss: 1.65   conf_loss: 140.98   prob_loss: 0.70   total_loss: 143.33\n",
      "=> STEP 2674/6250   lr: 0.000666   giou_loss: 1.56   conf_loss: 140.91   prob_loss: 0.70   total_loss: 143.17\n",
      "=> STEP 2675/6250   lr: 0.000666   giou_loss: 1.75   conf_loss: 140.84   prob_loss: 0.70   total_loss: 143.29\n",
      "=> STEP 2676/6250   lr: 0.000666   giou_loss: 1.71   conf_loss: 140.77   prob_loss: 0.70   total_loss: 143.18\n",
      "=> STEP 2677/6250   lr: 0.000665   giou_loss: 1.57   conf_loss: 140.70   prob_loss: 0.70   total_loss: 142.97\n",
      "=> STEP 2678/6250   lr: 0.000665   giou_loss: 1.65   conf_loss: 140.63   prob_loss: 0.70   total_loss: 142.98\n",
      "=> STEP 2679/6250   lr: 0.000665   giou_loss: 1.67   conf_loss: 140.57   prob_loss: 0.70   total_loss: 142.93\n",
      "=> STEP 2680/6250   lr: 0.000665   giou_loss: 1.62   conf_loss: 140.51   prob_loss: 0.70   total_loss: 142.83\n",
      "=> STEP 2681/6250   lr: 0.000664   giou_loss: 1.64   conf_loss: 140.42   prob_loss: 0.71   total_loss: 142.77\n",
      "=> STEP 2682/6250   lr: 0.000664   giou_loss: 1.61   conf_loss: 140.37   prob_loss: 0.70   total_loss: 142.68\n",
      "=> STEP 2683/6250   lr: 0.000664   giou_loss: 1.57   conf_loss: 140.32   prob_loss: 0.70   total_loss: 142.59\n",
      "=> STEP 2684/6250   lr: 0.000664   giou_loss: 1.69   conf_loss: 140.23   prob_loss: 0.69   total_loss: 142.61\n",
      "=> STEP 2685/6250   lr: 0.000663   giou_loss: 1.65   conf_loss: 140.19   prob_loss: 0.69   total_loss: 142.53\n",
      "=> STEP 2686/6250   lr: 0.000663   giou_loss: 1.57   conf_loss: 140.13   prob_loss: 0.70   total_loss: 142.40\n",
      "=> STEP 2687/6250   lr: 0.000663   giou_loss: 1.63   conf_loss: 140.03   prob_loss: 0.70   total_loss: 142.36\n",
      "=> STEP 2688/6250   lr: 0.000663   giou_loss: 1.60   conf_loss: 139.99   prob_loss: 0.70   total_loss: 142.29\n",
      "=> STEP 2689/6250   lr: 0.000663   giou_loss: 1.57   conf_loss: 139.92   prob_loss: 0.70   total_loss: 142.18\n",
      "=> STEP 2690/6250   lr: 0.000662   giou_loss: 1.65   conf_loss: 139.85   prob_loss: 0.69   total_loss: 142.19\n",
      "=> STEP 2691/6250   lr: 0.000662   giou_loss: 1.62   conf_loss: 139.80   prob_loss: 0.69   total_loss: 142.12\n",
      "=> STEP 2692/6250   lr: 0.000662   giou_loss: 1.56   conf_loss: 139.71   prob_loss: 0.70   total_loss: 141.97\n",
      "=> STEP 2693/6250   lr: 0.000662   giou_loss: 1.64   conf_loss: 139.64   prob_loss: 0.70   total_loss: 141.99\n",
      "=> STEP 2694/6250   lr: 0.000661   giou_loss: 1.62   conf_loss: 139.58   prob_loss: 0.70   total_loss: 141.90\n",
      "=> STEP 2695/6250   lr: 0.000661   giou_loss: 1.57   conf_loss: 139.51   prob_loss: 0.70   total_loss: 141.78\n",
      "=> STEP 2696/6250   lr: 0.000661   giou_loss: 1.70   conf_loss: 139.45   prob_loss: 0.69   total_loss: 141.84\n",
      "=> STEP 2697/6250   lr: 0.000661   giou_loss: 1.69   conf_loss: 139.39   prob_loss: 0.69   total_loss: 141.77\n",
      "=> STEP 2698/6250   lr: 0.000660   giou_loss: 1.56   conf_loss: 139.31   prob_loss: 0.70   total_loss: 141.57\n",
      "=> STEP 2699/6250   lr: 0.000660   giou_loss: 1.64   conf_loss: 139.23   prob_loss: 0.70   total_loss: 141.58\n",
      "=> STEP 2700/6250   lr: 0.000660   giou_loss: 1.57   conf_loss: 139.17   prob_loss: 0.70   total_loss: 141.45\n",
      "=> STEP 2701/6250   lr: 0.000660   giou_loss: 1.65   conf_loss: 139.12   prob_loss: 0.70   total_loss: 141.46\n",
      "=> STEP 2702/6250   lr: 0.000659   giou_loss: 1.75   conf_loss: 139.04   prob_loss: 0.69   total_loss: 141.48\n",
      "=> STEP 2703/6250   lr: 0.000659   giou_loss: 1.66   conf_loss: 138.99   prob_loss: 0.70   total_loss: 141.34\n",
      "=> STEP 2704/6250   lr: 0.000659   giou_loss: 1.60   conf_loss: 138.91   prob_loss: 0.70   total_loss: 141.21\n",
      "=> STEP 2705/6250   lr: 0.000659   giou_loss: 1.56   conf_loss: 138.83   prob_loss: 0.71   total_loss: 141.10\n",
      "=> STEP 2706/6250   lr: 0.000658   giou_loss: 1.66   conf_loss: 138.76   prob_loss: 0.71   total_loss: 141.13\n",
      "=> STEP 2707/6250   lr: 0.000658   giou_loss: 1.62   conf_loss: 138.70   prob_loss: 0.71   total_loss: 141.03\n",
      "=> STEP 2708/6250   lr: 0.000658   giou_loss: 1.57   conf_loss: 138.63   prob_loss: 0.70   total_loss: 140.89\n",
      "=> STEP 2709/6250   lr: 0.000658   giou_loss: 1.64   conf_loss: 138.55   prob_loss: 0.70   total_loss: 140.89\n",
      "=> STEP 2710/6250   lr: 0.000658   giou_loss: 1.65   conf_loss: 138.50   prob_loss: 0.70   total_loss: 140.85\n",
      "=> STEP 2711/6250   lr: 0.000657   giou_loss: 1.56   conf_loss: 138.43   prob_loss: 0.70   total_loss: 140.70\n",
      "=> STEP 2712/6250   lr: 0.000657   giou_loss: 1.64   conf_loss: 138.35   prob_loss: 0.70   total_loss: 140.68\n",
      "=> STEP 2713/6250   lr: 0.000657   giou_loss: 1.56   conf_loss: 138.31   prob_loss: 0.70   total_loss: 140.56\n",
      "=> STEP 2714/6250   lr: 0.000657   giou_loss: 1.56   conf_loss: 138.24   prob_loss: 0.69   total_loss: 140.50\n",
      "=> STEP 2715/6250   lr: 0.000656   giou_loss: 1.59   conf_loss: 138.16   prob_loss: 0.69   total_loss: 140.44\n",
      "=> STEP 2716/6250   lr: 0.000656   giou_loss: 1.56   conf_loss: 138.10   prob_loss: 0.69   total_loss: 140.36\n",
      "=> STEP 2717/6250   lr: 0.000656   giou_loss: 1.56   conf_loss: 138.02   prob_loss: 0.69   total_loss: 140.28\n",
      "=> STEP 2718/6250   lr: 0.000656   giou_loss: 1.58   conf_loss: 137.96   prob_loss: 0.70   total_loss: 140.24\n",
      "=> STEP 2719/6250   lr: 0.000655   giou_loss: 1.56   conf_loss: 137.88   prob_loss: 0.70   total_loss: 140.14\n",
      "=> STEP 2720/6250   lr: 0.000655   giou_loss: 1.65   conf_loss: 137.82   prob_loss: 0.70   total_loss: 140.16\n",
      "=> STEP 2721/6250   lr: 0.000655   giou_loss: 1.59   conf_loss: 137.75   prob_loss: 0.70   total_loss: 140.04\n",
      "=> STEP 2722/6250   lr: 0.000655   giou_loss: 1.64   conf_loss: 137.69   prob_loss: 0.70   total_loss: 140.03\n",
      "=> STEP 2723/6250   lr: 0.000654   giou_loss: 1.67   conf_loss: 137.62   prob_loss: 0.70   total_loss: 139.99\n",
      "=> STEP 2724/6250   lr: 0.000654   giou_loss: 1.57   conf_loss: 137.55   prob_loss: 0.70   total_loss: 139.81\n",
      "=> STEP 2725/6250   lr: 0.000654   giou_loss: 1.69   conf_loss: 137.48   prob_loss: 0.70   total_loss: 139.87\n",
      "=> STEP 2726/6250   lr: 0.000654   giou_loss: 1.62   conf_loss: 137.42   prob_loss: 0.69   total_loss: 139.74\n",
      "=> STEP 2727/6250   lr: 0.000653   giou_loss: 1.57   conf_loss: 137.36   prob_loss: 0.69   total_loss: 139.62\n",
      "=> STEP 2728/6250   lr: 0.000653   giou_loss: 1.67   conf_loss: 137.29   prob_loss: 0.69   total_loss: 139.65\n",
      "=> STEP 2729/6250   lr: 0.000653   giou_loss: 1.65   conf_loss: 137.24   prob_loss: 0.69   total_loss: 139.58\n",
      "=> STEP 2730/6250   lr: 0.000653   giou_loss: 1.56   conf_loss: 137.17   prob_loss: 0.69   total_loss: 139.43\n",
      "=> STEP 2731/6250   lr: 0.000652   giou_loss: 1.57   conf_loss: 137.09   prob_loss: 0.69   total_loss: 139.35\n",
      "=> STEP 2732/6250   lr: 0.000652   giou_loss: 1.64   conf_loss: 137.04   prob_loss: 0.69   total_loss: 139.37\n",
      "=> STEP 2733/6250   lr: 0.000652   giou_loss: 1.63   conf_loss: 136.97   prob_loss: 0.69   total_loss: 139.29\n",
      "=> STEP 2734/6250   lr: 0.000652   giou_loss: 1.56   conf_loss: 136.89   prob_loss: 0.69   total_loss: 139.15\n",
      "=> STEP 2735/6250   lr: 0.000652   giou_loss: 1.69   conf_loss: 136.83   prob_loss: 0.70   total_loss: 139.22\n",
      "=> STEP 2736/6250   lr: 0.000651   giou_loss: 1.71   conf_loss: 136.77   prob_loss: 0.70   total_loss: 139.17\n",
      "=> STEP 2737/6250   lr: 0.000651   giou_loss: 1.59   conf_loss: 136.70   prob_loss: 0.69   total_loss: 138.98\n",
      "=> STEP 2738/6250   lr: 0.000651   giou_loss: 1.63   conf_loss: 136.63   prob_loss: 0.69   total_loss: 138.95\n",
      "=> STEP 2739/6250   lr: 0.000651   giou_loss: 1.69   conf_loss: 136.57   prob_loss: 0.69   total_loss: 138.95\n",
      "=> STEP 2740/6250   lr: 0.000650   giou_loss: 1.63   conf_loss: 136.52   prob_loss: 0.69   total_loss: 138.84\n",
      "=> STEP 2741/6250   lr: 0.000650   giou_loss: 1.60   conf_loss: 136.44   prob_loss: 0.70   total_loss: 138.73\n",
      "=> STEP 2742/6250   lr: 0.000650   giou_loss: 1.66   conf_loss: 136.36   prob_loss: 0.70   total_loss: 138.72\n",
      "=> STEP 2743/6250   lr: 0.000650   giou_loss: 1.73   conf_loss: 136.31   prob_loss: 0.70   total_loss: 138.74\n",
      "=> STEP 2744/6250   lr: 0.000649   giou_loss: 1.62   conf_loss: 136.26   prob_loss: 0.69   total_loss: 138.57\n",
      "=> STEP 2745/6250   lr: 0.000649   giou_loss: 1.58   conf_loss: 136.18   prob_loss: 0.69   total_loss: 138.45\n",
      "=> STEP 2746/6250   lr: 0.000649   giou_loss: 1.59   conf_loss: 136.13   prob_loss: 0.69   total_loss: 138.40\n",
      "=> STEP 2747/6250   lr: 0.000649   giou_loss: 1.61   conf_loss: 136.07   prob_loss: 0.69   total_loss: 138.36\n",
      "=> STEP 2748/6250   lr: 0.000648   giou_loss: 1.58   conf_loss: 135.98   prob_loss: 0.69   total_loss: 138.25\n",
      "=> STEP 2749/6250   lr: 0.000648   giou_loss: 1.64   conf_loss: 135.93   prob_loss: 0.69   total_loss: 138.25\n",
      "=> STEP 2750/6250   lr: 0.000648   giou_loss: 1.63   conf_loss: 135.87   prob_loss: 0.69   total_loss: 138.19\n",
      "=> STEP 2751/6250   lr: 0.000648   giou_loss: 1.58   conf_loss: 135.78   prob_loss: 0.69   total_loss: 138.06\n",
      "=> STEP 2752/6250   lr: 0.000647   giou_loss: 1.68   conf_loss: 135.74   prob_loss: 0.69   total_loss: 138.11\n",
      "=> STEP 2753/6250   lr: 0.000647   giou_loss: 1.64   conf_loss: 135.66   prob_loss: 0.69   total_loss: 138.00\n",
      "=> STEP 2754/6250   lr: 0.000647   giou_loss: 1.61   conf_loss: 135.60   prob_loss: 0.69   total_loss: 137.90\n",
      "=> STEP 2755/6250   lr: 0.000647   giou_loss: 1.57   conf_loss: 135.55   prob_loss: 0.69   total_loss: 137.81\n",
      "=> STEP 2756/6250   lr: 0.000646   giou_loss: 1.63   conf_loss: 135.46   prob_loss: 0.69   total_loss: 137.79\n",
      "=> STEP 2757/6250   lr: 0.000646   giou_loss: 1.65   conf_loss: 135.41   prob_loss: 0.69   total_loss: 137.75\n",
      "=> STEP 2758/6250   lr: 0.000646   giou_loss: 1.57   conf_loss: 135.36   prob_loss: 0.69   total_loss: 137.61\n",
      "=> STEP 2759/6250   lr: 0.000646   giou_loss: 1.56   conf_loss: 135.27   prob_loss: 0.69   total_loss: 137.53\n",
      "=> STEP 2760/6250   lr: 0.000646   giou_loss: 1.60   conf_loss: 135.22   prob_loss: 0.69   total_loss: 137.52\n",
      "=> STEP 2761/6250   lr: 0.000645   giou_loss: 1.56   conf_loss: 135.15   prob_loss: 0.69   total_loss: 137.41\n",
      "=> STEP 2762/6250   lr: 0.000645   giou_loss: 1.57   conf_loss: 135.08   prob_loss: 0.69   total_loss: 137.34\n",
      "=> STEP 2763/6250   lr: 0.000645   giou_loss: 1.57   conf_loss: 135.03   prob_loss: 0.69   total_loss: 137.29\n",
      "=> STEP 2764/6250   lr: 0.000645   giou_loss: 1.56   conf_loss: 134.96   prob_loss: 0.69   total_loss: 137.21\n",
      "=> STEP 2765/6250   lr: 0.000644   giou_loss: 1.58   conf_loss: 134.89   prob_loss: 0.69   total_loss: 137.16\n",
      "=> STEP 2766/6250   lr: 0.000644   giou_loss: 1.57   conf_loss: 134.85   prob_loss: 0.69   total_loss: 137.11\n",
      "=> STEP 2767/6250   lr: 0.000644   giou_loss: 1.62   conf_loss: 134.77   prob_loss: 0.69   total_loss: 137.08\n",
      "=> STEP 2768/6250   lr: 0.000644   giou_loss: 1.61   conf_loss: 134.71   prob_loss: 0.69   total_loss: 137.00\n",
      "=> STEP 2769/6250   lr: 0.000643   giou_loss: 1.60   conf_loss: 134.66   prob_loss: 0.69   total_loss: 136.96\n",
      "=> STEP 2770/6250   lr: 0.000643   giou_loss: 1.62   conf_loss: 134.57   prob_loss: 0.69   total_loss: 136.87\n",
      "=> STEP 2771/6250   lr: 0.000643   giou_loss: 1.59   conf_loss: 134.54   prob_loss: 0.69   total_loss: 136.81\n",
      "=> STEP 2772/6250   lr: 0.000643   giou_loss: 1.57   conf_loss: 134.47   prob_loss: 0.68   total_loss: 136.72\n",
      "=> STEP 2773/6250   lr: 0.000642   giou_loss: 1.66   conf_loss: 134.38   prob_loss: 0.68   total_loss: 136.72\n",
      "=> STEP 2774/6250   lr: 0.000642   giou_loss: 1.65   conf_loss: 134.36   prob_loss: 0.68   total_loss: 136.70\n",
      "=> STEP 2775/6250   lr: 0.000642   giou_loss: 1.61   conf_loss: 134.26   prob_loss: 0.69   total_loss: 136.56\n",
      "=> STEP 2776/6250   lr: 0.000642   giou_loss: 1.63   conf_loss: 134.21   prob_loss: 0.69   total_loss: 136.53\n",
      "=> STEP 2777/6250   lr: 0.000641   giou_loss: 1.59   conf_loss: 134.17   prob_loss: 0.69   total_loss: 136.46\n",
      "=> STEP 2778/6250   lr: 0.000641   giou_loss: 1.64   conf_loss: 134.05   prob_loss: 0.69   total_loss: 136.38\n",
      "=> STEP 2779/6250   lr: 0.000641   giou_loss: 1.67   conf_loss: 134.02   prob_loss: 0.69   total_loss: 136.38\n",
      "=> STEP 2780/6250   lr: 0.000641   giou_loss: 1.56   conf_loss: 133.98   prob_loss: 0.69   total_loss: 136.23\n",
      "=> STEP 2781/6250   lr: 0.000640   giou_loss: 1.65   conf_loss: 133.87   prob_loss: 0.68   total_loss: 136.20\n",
      "=> STEP 2782/6250   lr: 0.000640   giou_loss: 1.56   conf_loss: 133.87   prob_loss: 0.68   total_loss: 136.12\n",
      "=> STEP 2783/6250   lr: 0.000640   giou_loss: 1.57   conf_loss: 133.78   prob_loss: 0.68   total_loss: 136.03\n",
      "=> STEP 2784/6250   lr: 0.000640   giou_loss: 1.58   conf_loss: 133.69   prob_loss: 0.68   total_loss: 135.95\n",
      "=> STEP 2785/6250   lr: 0.000640   giou_loss: 1.58   conf_loss: 133.68   prob_loss: 0.68   total_loss: 135.94\n",
      "=> STEP 2786/6250   lr: 0.000639   giou_loss: 1.59   conf_loss: 133.57   prob_loss: 0.68   total_loss: 135.84\n",
      "=> STEP 2787/6250   lr: 0.000639   giou_loss: 1.56   conf_loss: 133.50   prob_loss: 0.68   total_loss: 135.75\n",
      "=> STEP 2788/6250   lr: 0.000639   giou_loss: 1.61   conf_loss: 133.45   prob_loss: 0.69   total_loss: 135.75\n",
      "=> STEP 2789/6250   lr: 0.000639   giou_loss: 1.57   conf_loss: 133.38   prob_loss: 0.68   total_loss: 135.64\n",
      "=> STEP 2790/6250   lr: 0.000638   giou_loss: 1.56   conf_loss: 133.31   prob_loss: 0.68   total_loss: 135.56\n",
      "=> STEP 2791/6250   lr: 0.000638   giou_loss: 1.57   conf_loss: 133.26   prob_loss: 0.68   total_loss: 135.51\n",
      "=> STEP 2792/6250   lr: 0.000638   giou_loss: 1.59   conf_loss: 133.19   prob_loss: 0.68   total_loss: 135.46\n",
      "=> STEP 2793/6250   lr: 0.000638   giou_loss: 1.57   conf_loss: 133.12   prob_loss: 0.68   total_loss: 135.37\n",
      "=> STEP 2794/6250   lr: 0.000637   giou_loss: 1.57   conf_loss: 133.07   prob_loss: 0.69   total_loss: 135.32\n",
      "=> STEP 2795/6250   lr: 0.000637   giou_loss: 1.60   conf_loss: 132.99   prob_loss: 0.69   total_loss: 135.28\n",
      "=> STEP 2796/6250   lr: 0.000637   giou_loss: 1.57   conf_loss: 132.94   prob_loss: 0.68   total_loss: 135.19\n",
      "=> STEP 2797/6250   lr: 0.000637   giou_loss: 1.65   conf_loss: 132.88   prob_loss: 0.68   total_loss: 135.22\n",
      "=> STEP 2798/6250   lr: 0.000636   giou_loss: 1.60   conf_loss: 132.81   prob_loss: 0.68   total_loss: 135.09\n",
      "=> STEP 2799/6250   lr: 0.000636   giou_loss: 1.64   conf_loss: 132.74   prob_loss: 0.69   total_loss: 135.08\n",
      "=> STEP 2800/6250   lr: 0.000636   giou_loss: 1.68   conf_loss: 132.68   prob_loss: 0.69   total_loss: 135.05\n",
      "=> STEP 2801/6250   lr: 0.000636   giou_loss: 1.58   conf_loss: 132.62   prob_loss: 0.69   total_loss: 134.88\n",
      "=> STEP 2802/6250   lr: 0.000635   giou_loss: 1.71   conf_loss: 132.57   prob_loss: 0.68   total_loss: 134.95\n",
      "=> STEP 2803/6250   lr: 0.000635   giou_loss: 1.77   conf_loss: 132.50   prob_loss: 0.68   total_loss: 134.96\n",
      "=> STEP 2804/6250   lr: 0.000635   giou_loss: 1.72   conf_loss: 132.43   prob_loss: 0.68   total_loss: 134.84\n",
      "=> STEP 2805/6250   lr: 0.000635   giou_loss: 1.56   conf_loss: 132.37   prob_loss: 0.69   total_loss: 134.62\n",
      "=> STEP 2806/6250   lr: 0.000634   giou_loss: 1.66   conf_loss: 132.30   prob_loss: 0.69   total_loss: 134.65\n",
      "=> STEP 2807/6250   lr: 0.000634   giou_loss: 1.66   conf_loss: 132.24   prob_loss: 0.69   total_loss: 134.59\n",
      "=> STEP 2808/6250   lr: 0.000634   giou_loss: 1.59   conf_loss: 132.19   prob_loss: 0.68   total_loss: 134.46\n",
      "=> STEP 2809/6250   lr: 0.000634   giou_loss: 1.62   conf_loss: 132.13   prob_loss: 0.68   total_loss: 134.43\n",
      "=> STEP 2810/6250   lr: 0.000633   giou_loss: 1.64   conf_loss: 132.07   prob_loss: 0.68   total_loss: 134.39\n",
      "=> STEP 2811/6250   lr: 0.000633   giou_loss: 1.68   conf_loss: 132.00   prob_loss: 0.68   total_loss: 134.37\n",
      "=> STEP 2812/6250   lr: 0.000633   giou_loss: 1.66   conf_loss: 131.95   prob_loss: 0.69   total_loss: 134.30\n",
      "=> STEP 2813/6250   lr: 0.000633   giou_loss: 1.58   conf_loss: 131.87   prob_loss: 0.69   total_loss: 134.14\n",
      "=> STEP 2814/6250   lr: 0.000632   giou_loss: 1.60   conf_loss: 131.84   prob_loss: 0.69   total_loss: 134.12\n",
      "=> STEP 2815/6250   lr: 0.000632   giou_loss: 1.66   conf_loss: 131.78   prob_loss: 0.68   total_loss: 134.12\n",
      "=> STEP 2816/6250   lr: 0.000632   giou_loss: 1.62   conf_loss: 131.69   prob_loss: 0.68   total_loss: 134.00\n",
      "=> STEP 2817/6250   lr: 0.000632   giou_loss: 1.58   conf_loss: 131.67   prob_loss: 0.69   total_loss: 133.94\n",
      "=> STEP 2818/6250   lr: 0.000632   giou_loss: 1.71   conf_loss: 131.59   prob_loss: 0.69   total_loss: 133.98\n",
      "=> STEP 2819/6250   lr: 0.000631   giou_loss: 1.66   conf_loss: 131.52   prob_loss: 0.69   total_loss: 133.87\n",
      "=> STEP 2820/6250   lr: 0.000631   giou_loss: 1.56   conf_loss: 131.51   prob_loss: 0.68   total_loss: 133.75\n",
      "=> STEP 2821/6250   lr: 0.000631   giou_loss: 1.59   conf_loss: 131.41   prob_loss: 0.68   total_loss: 133.68\n",
      "=> STEP 2822/6250   lr: 0.000631   giou_loss: 1.59   conf_loss: 131.36   prob_loss: 0.68   total_loss: 133.62\n",
      "=> STEP 2823/6250   lr: 0.000630   giou_loss: 1.56   conf_loss: 131.32   prob_loss: 0.68   total_loss: 133.56\n",
      "=> STEP 2824/6250   lr: 0.000630   giou_loss: 1.56   conf_loss: 131.22   prob_loss: 0.68   total_loss: 133.46\n",
      "=> STEP 2825/6250   lr: 0.000630   giou_loss: 1.57   conf_loss: 131.17   prob_loss: 0.68   total_loss: 133.41\n",
      "=> STEP 2826/6250   lr: 0.000630   giou_loss: 1.59   conf_loss: 131.13   prob_loss: 0.68   total_loss: 133.40\n",
      "=> STEP 2827/6250   lr: 0.000629   giou_loss: 1.63   conf_loss: 131.04   prob_loss: 0.67   total_loss: 133.35\n",
      "=> STEP 2828/6250   lr: 0.000629   giou_loss: 1.57   conf_loss: 131.00   prob_loss: 0.68   total_loss: 133.24\n",
      "=> STEP 2829/6250   lr: 0.000629   giou_loss: 1.56   conf_loss: 130.95   prob_loss: 0.68   total_loss: 133.19\n",
      "=> STEP 2830/6250   lr: 0.000629   giou_loss: 1.56   conf_loss: 130.85   prob_loss: 0.68   total_loss: 133.10\n",
      "=> STEP 2831/6250   lr: 0.000628   giou_loss: 1.57   conf_loss: 130.80   prob_loss: 0.68   total_loss: 133.06\n",
      "=> STEP 2832/6250   lr: 0.000628   giou_loss: 1.63   conf_loss: 130.75   prob_loss: 0.68   total_loss: 133.06\n",
      "=> STEP 2833/6250   lr: 0.000628   giou_loss: 1.57   conf_loss: 130.68   prob_loss: 0.68   total_loss: 132.93\n",
      "=> STEP 2834/6250   lr: 0.000628   giou_loss: 1.68   conf_loss: 130.62   prob_loss: 0.68   total_loss: 132.98\n",
      "=> STEP 2835/6250   lr: 0.000627   giou_loss: 1.68   conf_loss: 130.57   prob_loss: 0.68   total_loss: 132.93\n",
      "=> STEP 2836/6250   lr: 0.000627   giou_loss: 1.56   conf_loss: 130.49   prob_loss: 0.68   total_loss: 132.74\n",
      "=> STEP 2837/6250   lr: 0.000627   giou_loss: 1.74   conf_loss: 130.44   prob_loss: 0.68   total_loss: 132.86\n",
      "=> STEP 2838/6250   lr: 0.000627   giou_loss: 1.79   conf_loss: 130.40   prob_loss: 0.68   total_loss: 132.87\n",
      "=> STEP 2839/6250   lr: 0.000626   giou_loss: 1.75   conf_loss: 130.32   prob_loss: 0.68   total_loss: 132.74\n",
      "=> STEP 2840/6250   lr: 0.000626   giou_loss: 1.63   conf_loss: 130.26   prob_loss: 0.68   total_loss: 132.57\n",
      "=> STEP 2841/6250   lr: 0.000626   giou_loss: 1.73   conf_loss: 130.21   prob_loss: 0.68   total_loss: 132.62\n",
      "=> STEP 2842/6250   lr: 0.000626   giou_loss: 1.83   conf_loss: 130.13   prob_loss: 0.68   total_loss: 132.65\n",
      "=> STEP 2843/6250   lr: 0.000625   giou_loss: 1.83   conf_loss: 130.07   prob_loss: 0.68   total_loss: 132.58\n",
      "=> STEP 2844/6250   lr: 0.000625   giou_loss: 1.76   conf_loss: 130.02   prob_loss: 0.68   total_loss: 132.47\n",
      "=> STEP 2845/6250   lr: 0.000625   giou_loss: 1.56   conf_loss: 129.97   prob_loss: 0.68   total_loss: 132.20\n",
      "=> STEP 2846/6250   lr: 0.000625   giou_loss: 1.71   conf_loss: 129.91   prob_loss: 0.67   total_loss: 132.29\n",
      "=> STEP 2847/6250   lr: 0.000624   giou_loss: 1.75   conf_loss: 129.85   prob_loss: 0.67   total_loss: 132.27\n",
      "=> STEP 2848/6250   lr: 0.000624   giou_loss: 1.67   conf_loss: 129.78   prob_loss: 0.67   total_loss: 132.13\n",
      "=> STEP 2849/6250   lr: 0.000624   giou_loss: 1.62   conf_loss: 129.72   prob_loss: 0.68   total_loss: 132.02\n",
      "=> STEP 2850/6250   lr: 0.000624   giou_loss: 1.62   conf_loss: 129.65   prob_loss: 0.68   total_loss: 131.95\n",
      "=> STEP 2851/6250   lr: 0.000624   giou_loss: 1.57   conf_loss: 129.59   prob_loss: 0.68   total_loss: 131.85\n",
      "=> STEP 2852/6250   lr: 0.000623   giou_loss: 1.60   conf_loss: 129.54   prob_loss: 0.68   total_loss: 131.81\n",
      "=> STEP 2853/6250   lr: 0.000623   giou_loss: 1.56   conf_loss: 129.47   prob_loss: 0.68   total_loss: 131.72\n",
      "=> STEP 2854/6250   lr: 0.000623   giou_loss: 1.58   conf_loss: 129.41   prob_loss: 0.68   total_loss: 131.68\n",
      "=> STEP 2855/6250   lr: 0.000623   giou_loss: 1.57   conf_loss: 129.36   prob_loss: 0.68   total_loss: 131.61\n",
      "=> STEP 2856/6250   lr: 0.000622   giou_loss: 1.63   conf_loss: 129.30   prob_loss: 0.68   total_loss: 131.60\n",
      "=> STEP 2857/6250   lr: 0.000622   giou_loss: 1.64   conf_loss: 129.24   prob_loss: 0.68   total_loss: 131.55\n",
      "=> STEP 2858/6250   lr: 0.000622   giou_loss: 1.66   conf_loss: 129.17   prob_loss: 0.68   total_loss: 131.51\n",
      "=> STEP 2859/6250   lr: 0.000622   giou_loss: 1.70   conf_loss: 129.12   prob_loss: 0.68   total_loss: 131.50\n",
      "=> STEP 2860/6250   lr: 0.000621   giou_loss: 1.62   conf_loss: 129.06   prob_loss: 0.68   total_loss: 131.36\n",
      "=> STEP 2861/6250   lr: 0.000621   giou_loss: 1.63   conf_loss: 129.00   prob_loss: 0.67   total_loss: 131.31\n",
      "=> STEP 2862/6250   lr: 0.000621   giou_loss: 1.68   conf_loss: 128.95   prob_loss: 0.67   total_loss: 131.30\n",
      "=> STEP 2863/6250   lr: 0.000621   giou_loss: 1.60   conf_loss: 128.88   prob_loss: 0.68   total_loss: 131.16\n",
      "=> STEP 2864/6250   lr: 0.000620   giou_loss: 1.65   conf_loss: 128.82   prob_loss: 0.68   total_loss: 131.15\n",
      "=> STEP 2865/6250   lr: 0.000620   giou_loss: 1.71   conf_loss: 128.76   prob_loss: 0.68   total_loss: 131.15\n",
      "=> STEP 2866/6250   lr: 0.000620   giou_loss: 1.63   conf_loss: 128.70   prob_loss: 0.68   total_loss: 131.01\n",
      "=> STEP 2867/6250   lr: 0.000620   giou_loss: 1.72   conf_loss: 128.65   prob_loss: 0.68   total_loss: 131.04\n",
      "=> STEP 2868/6250   lr: 0.000619   giou_loss: 1.74   conf_loss: 128.59   prob_loss: 0.68   total_loss: 131.00\n",
      "=> STEP 2869/6250   lr: 0.000619   giou_loss: 1.58   conf_loss: 128.53   prob_loss: 0.68   total_loss: 130.79\n",
      "=> STEP 2870/6250   lr: 0.000619   giou_loss: 1.75   conf_loss: 128.47   prob_loss: 0.68   total_loss: 130.89\n",
      "=> STEP 2871/6250   lr: 0.000619   giou_loss: 1.79   conf_loss: 128.41   prob_loss: 0.68   total_loss: 130.88\n",
      "=> STEP 2872/6250   lr: 0.000618   giou_loss: 1.68   conf_loss: 128.36   prob_loss: 0.68   total_loss: 130.72\n",
      "=> STEP 2873/6250   lr: 0.000618   giou_loss: 1.60   conf_loss: 128.29   prob_loss: 0.68   total_loss: 130.57\n",
      "=> STEP 2874/6250   lr: 0.000618   giou_loss: 1.69   conf_loss: 128.25   prob_loss: 0.67   total_loss: 130.61\n",
      "=> STEP 2875/6250   lr: 0.000618   giou_loss: 1.72   conf_loss: 128.21   prob_loss: 0.67   total_loss: 130.60\n",
      "=> STEP 2876/6250   lr: 0.000617   giou_loss: 1.62   conf_loss: 128.12   prob_loss: 0.67   total_loss: 130.42\n",
      "=> STEP 2877/6250   lr: 0.000617   giou_loss: 1.67   conf_loss: 128.08   prob_loss: 0.68   total_loss: 130.44\n",
      "=> STEP 2878/6250   lr: 0.000617   giou_loss: 1.77   conf_loss: 128.03   prob_loss: 0.68   total_loss: 130.48\n",
      "=> STEP 2879/6250   lr: 0.000617   giou_loss: 1.70   conf_loss: 127.94   prob_loss: 0.68   total_loss: 130.32\n",
      "=> STEP 2880/6250   lr: 0.000616   giou_loss: 1.57   conf_loss: 127.93   prob_loss: 0.68   total_loss: 130.19\n",
      "=> STEP 2881/6250   lr: 0.000616   giou_loss: 1.63   conf_loss: 127.86   prob_loss: 0.68   total_loss: 130.17\n",
      "=> STEP 2882/6250   lr: 0.000616   giou_loss: 1.63   conf_loss: 127.79   prob_loss: 0.68   total_loss: 130.09\n",
      "=> STEP 2883/6250   lr: 0.000616   giou_loss: 1.57   conf_loss: 127.74   prob_loss: 0.67   total_loss: 129.99\n",
      "=> STEP 2884/6250   lr: 0.000615   giou_loss: 1.66   conf_loss: 127.67   prob_loss: 0.68   total_loss: 130.01\n",
      "=> STEP 2885/6250   lr: 0.000615   giou_loss: 1.64   conf_loss: 127.61   prob_loss: 0.68   total_loss: 129.92\n",
      "=> STEP 2886/6250   lr: 0.000615   giou_loss: 1.56   conf_loss: 127.55   prob_loss: 0.68   total_loss: 129.80\n",
      "=> STEP 2887/6250   lr: 0.000615   giou_loss: 1.67   conf_loss: 127.49   prob_loss: 0.68   total_loss: 129.84\n",
      "=> STEP 2888/6250   lr: 0.000614   giou_loss: 1.66   conf_loss: 127.43   prob_loss: 0.68   total_loss: 129.77\n",
      "=> STEP 2889/6250   lr: 0.000614   giou_loss: 1.56   conf_loss: 127.38   prob_loss: 0.68   total_loss: 129.62\n",
      "=> STEP 2890/6250   lr: 0.000614   giou_loss: 1.62   conf_loss: 127.31   prob_loss: 0.68   total_loss: 129.61\n",
      "=> STEP 2891/6250   lr: 0.000614   giou_loss: 1.60   conf_loss: 127.25   prob_loss: 0.68   total_loss: 129.53\n",
      "=> STEP 2892/6250   lr: 0.000614   giou_loss: 1.62   conf_loss: 127.20   prob_loss: 0.68   total_loss: 129.50\n",
      "=> STEP 2893/6250   lr: 0.000613   giou_loss: 1.59   conf_loss: 127.14   prob_loss: 0.67   total_loss: 129.41\n",
      "=> STEP 2894/6250   lr: 0.000613   giou_loss: 1.65   conf_loss: 127.08   prob_loss: 0.67   total_loss: 129.40\n",
      "=> STEP 2895/6250   lr: 0.000613   giou_loss: 1.59   conf_loss: 127.03   prob_loss: 0.67   total_loss: 129.29\n",
      "=> STEP 2896/6250   lr: 0.000613   giou_loss: 1.66   conf_loss: 126.98   prob_loss: 0.68   total_loss: 129.32\n",
      "=> STEP 2897/6250   lr: 0.000612   giou_loss: 1.67   conf_loss: 126.91   prob_loss: 0.68   total_loss: 129.26\n",
      "=> STEP 2898/6250   lr: 0.000612   giou_loss: 1.56   conf_loss: 126.87   prob_loss: 0.68   total_loss: 129.11\n",
      "=> STEP 2899/6250   lr: 0.000612   giou_loss: 1.70   conf_loss: 126.81   prob_loss: 0.68   total_loss: 129.19\n",
      "=> STEP 2900/6250   lr: 0.000612   giou_loss: 1.65   conf_loss: 126.74   prob_loss: 0.68   total_loss: 129.07\n",
      "=> STEP 2901/6250   lr: 0.000611   giou_loss: 1.56   conf_loss: 126.70   prob_loss: 0.67   total_loss: 128.94\n",
      "=> STEP 2902/6250   lr: 0.000611   giou_loss: 1.66   conf_loss: 126.64   prob_loss: 0.67   total_loss: 128.97\n",
      "=> STEP 2903/6250   lr: 0.000611   giou_loss: 1.66   conf_loss: 126.57   prob_loss: 0.68   total_loss: 128.90\n",
      "=> STEP 2904/6250   lr: 0.000611   giou_loss: 1.57   conf_loss: 126.53   prob_loss: 0.68   total_loss: 128.77\n",
      "=> STEP 2905/6250   lr: 0.000610   giou_loss: 1.71   conf_loss: 126.46   prob_loss: 0.68   total_loss: 128.84\n",
      "=> STEP 2906/6250   lr: 0.000610   giou_loss: 1.64   conf_loss: 126.40   prob_loss: 0.68   total_loss: 128.72\n",
      "=> STEP 2907/6250   lr: 0.000610   giou_loss: 1.56   conf_loss: 126.36   prob_loss: 0.67   total_loss: 128.60\n",
      "=> STEP 2908/6250   lr: 0.000610   giou_loss: 1.66   conf_loss: 126.29   prob_loss: 0.67   total_loss: 128.62\n",
      "=> STEP 2909/6250   lr: 0.000609   giou_loss: 1.65   conf_loss: 126.23   prob_loss: 0.67   total_loss: 128.55\n",
      "=> STEP 2910/6250   lr: 0.000609   giou_loss: 1.57   conf_loss: 126.19   prob_loss: 0.67   total_loss: 128.43\n",
      "=> STEP 2911/6250   lr: 0.000609   giou_loss: 1.73   conf_loss: 126.12   prob_loss: 0.67   total_loss: 128.52\n",
      "=> STEP 2912/6250   lr: 0.000609   giou_loss: 1.67   conf_loss: 126.06   prob_loss: 0.67   total_loss: 128.40\n",
      "=> STEP 2913/6250   lr: 0.000608   giou_loss: 1.56   conf_loss: 126.02   prob_loss: 0.67   total_loss: 128.26\n",
      "=> STEP 2914/6250   lr: 0.000608   giou_loss: 1.61   conf_loss: 125.95   prob_loss: 0.67   total_loss: 128.23\n",
      "=> STEP 2915/6250   lr: 0.000608   giou_loss: 1.59   conf_loss: 125.89   prob_loss: 0.67   total_loss: 128.15\n",
      "=> STEP 2916/6250   lr: 0.000608   giou_loss: 1.63   conf_loss: 125.84   prob_loss: 0.67   total_loss: 128.14\n",
      "=> STEP 2917/6250   lr: 0.000607   giou_loss: 1.63   conf_loss: 125.78   prob_loss: 0.67   total_loss: 128.09\n",
      "=> STEP 2918/6250   lr: 0.000607   giou_loss: 1.58   conf_loss: 125.73   prob_loss: 0.67   total_loss: 127.98\n",
      "=> STEP 2919/6250   lr: 0.000607   giou_loss: 1.63   conf_loss: 125.68   prob_loss: 0.67   total_loss: 127.98\n",
      "=> STEP 2920/6250   lr: 0.000607   giou_loss: 1.59   conf_loss: 125.62   prob_loss: 0.67   total_loss: 127.88\n",
      "=> STEP 2921/6250   lr: 0.000606   giou_loss: 1.63   conf_loss: 125.55   prob_loss: 0.68   total_loss: 127.86\n",
      "=> STEP 2922/6250   lr: 0.000606   giou_loss: 1.64   conf_loss: 125.50   prob_loss: 0.68   total_loss: 127.82\n",
      "=> STEP 2923/6250   lr: 0.000606   giou_loss: 1.57   conf_loss: 125.45   prob_loss: 0.67   total_loss: 127.69\n",
      "=> STEP 2924/6250   lr: 0.000606   giou_loss: 1.65   conf_loss: 125.39   prob_loss: 0.67   total_loss: 127.71\n",
      "=> STEP 2925/6250   lr: 0.000605   giou_loss: 1.57   conf_loss: 125.34   prob_loss: 0.67   total_loss: 127.58\n",
      "=> STEP 2926/6250   lr: 0.000605   giou_loss: 1.65   conf_loss: 125.29   prob_loss: 0.67   total_loss: 127.61\n",
      "=> STEP 2927/6250   lr: 0.000605   giou_loss: 1.67   conf_loss: 125.22   prob_loss: 0.68   total_loss: 127.56\n",
      "=> STEP 2928/6250   lr: 0.000605   giou_loss: 1.57   conf_loss: 125.18   prob_loss: 0.67   total_loss: 127.42\n",
      "=> STEP 2929/6250   lr: 0.000604   giou_loss: 1.69   conf_loss: 125.13   prob_loss: 0.67   total_loss: 127.49\n",
      "=> STEP 2930/6250   lr: 0.000604   giou_loss: 1.75   conf_loss: 125.06   prob_loss: 0.66   total_loss: 127.48\n",
      "=> STEP 2931/6250   lr: 0.000604   giou_loss: 1.70   conf_loss: 125.02   prob_loss: 0.67   total_loss: 127.39\n",
      "=> STEP 2932/6250   lr: 0.000604   giou_loss: 1.56   conf_loss: 124.96   prob_loss: 0.67   total_loss: 127.19\n",
      "=> STEP 2933/6250   lr: 0.000603   giou_loss: 1.68   conf_loss: 124.89   prob_loss: 0.68   total_loss: 127.24\n",
      "=> STEP 2934/6250   lr: 0.000603   giou_loss: 1.65   conf_loss: 124.84   prob_loss: 0.67   total_loss: 127.17\n",
      "=> STEP 2935/6250   lr: 0.000603   giou_loss: 1.57   conf_loss: 124.80   prob_loss: 0.67   total_loss: 127.03\n",
      "=> STEP 2936/6250   lr: 0.000603   giou_loss: 1.60   conf_loss: 124.73   prob_loss: 0.67   total_loss: 127.00\n",
      "=> STEP 2937/6250   lr: 0.000602   giou_loss: 1.57   conf_loss: 124.68   prob_loss: 0.66   total_loss: 126.92\n",
      "=> STEP 2938/6250   lr: 0.000602   giou_loss: 1.56   conf_loss: 124.64   prob_loss: 0.67   total_loss: 126.87\n",
      "=> STEP 2939/6250   lr: 0.000602   giou_loss: 1.60   conf_loss: 124.56   prob_loss: 0.67   total_loss: 126.83\n",
      "=> STEP 2940/6250   lr: 0.000602   giou_loss: 1.56   conf_loss: 124.51   prob_loss: 0.67   total_loss: 126.75\n",
      "=> STEP 2941/6250   lr: 0.000602   giou_loss: 1.58   conf_loss: 124.47   prob_loss: 0.67   total_loss: 126.72\n",
      "=> STEP 2942/6250   lr: 0.000601   giou_loss: 1.58   conf_loss: 124.40   prob_loss: 0.67   total_loss: 126.64\n",
      "=> STEP 2943/6250   lr: 0.000601   giou_loss: 1.57   conf_loss: 124.34   prob_loss: 0.67   total_loss: 126.58\n",
      "=> STEP 2944/6250   lr: 0.000601   giou_loss: 1.57   conf_loss: 124.30   prob_loss: 0.66   total_loss: 126.53\n",
      "=> STEP 2945/6250   lr: 0.000601   giou_loss: 1.56   conf_loss: 124.23   prob_loss: 0.67   total_loss: 126.46\n",
      "=> STEP 2946/6250   lr: 0.000600   giou_loss: 1.62   conf_loss: 124.17   prob_loss: 0.67   total_loss: 126.46\n",
      "=> STEP 2947/6250   lr: 0.000600   giou_loss: 1.59   conf_loss: 124.13   prob_loss: 0.67   total_loss: 126.39\n",
      "=> STEP 2948/6250   lr: 0.000600   giou_loss: 1.59   conf_loss: 124.09   prob_loss: 0.67   total_loss: 126.34\n",
      "=> STEP 2949/6250   lr: 0.000600   giou_loss: 1.59   conf_loss: 124.01   prob_loss: 0.66   total_loss: 126.26\n",
      "=> STEP 2950/6250   lr: 0.000599   giou_loss: 1.56   conf_loss: 123.98   prob_loss: 0.67   total_loss: 126.21\n",
      "=> STEP 2951/6250   lr: 0.000599   giou_loss: 1.64   conf_loss: 123.92   prob_loss: 0.67   total_loss: 126.23\n",
      "=> STEP 2952/6250   lr: 0.000599   giou_loss: 1.59   conf_loss: 123.84   prob_loss: 0.67   total_loss: 126.10\n",
      "=> STEP 2953/6250   lr: 0.000599   giou_loss: 1.59   conf_loss: 123.82   prob_loss: 0.67   total_loss: 126.08\n",
      "=> STEP 2954/6250   lr: 0.000598   giou_loss: 1.60   conf_loss: 123.75   prob_loss: 0.66   total_loss: 126.02\n",
      "=> STEP 2955/6250   lr: 0.000598   giou_loss: 1.57   conf_loss: 123.69   prob_loss: 0.66   total_loss: 125.92\n",
      "=> STEP 2956/6250   lr: 0.000598   giou_loss: 1.70   conf_loss: 123.66   prob_loss: 0.67   total_loss: 126.02\n",
      "=> STEP 2957/6250   lr: 0.000598   giou_loss: 1.68   conf_loss: 123.57   prob_loss: 0.67   total_loss: 125.92\n",
      "=> STEP 2958/6250   lr: 0.000597   giou_loss: 1.57   conf_loss: 123.53   prob_loss: 0.67   total_loss: 125.76\n",
      "=> STEP 2959/6250   lr: 0.000597   giou_loss: 1.68   conf_loss: 123.49   prob_loss: 0.67   total_loss: 125.83\n",
      "=> STEP 2960/6250   lr: 0.000597   giou_loss: 1.64   conf_loss: 123.41   prob_loss: 0.67   total_loss: 125.71\n",
      "=> STEP 2961/6250   lr: 0.000597   giou_loss: 1.56   conf_loss: 123.37   prob_loss: 0.67   total_loss: 125.60\n",
      "=> STEP 2962/6250   lr: 0.000596   giou_loss: 1.60   conf_loss: 123.33   prob_loss: 0.67   total_loss: 125.60\n",
      "=> STEP 2963/6250   lr: 0.000596   giou_loss: 1.58   conf_loss: 123.25   prob_loss: 0.67   total_loss: 125.49\n",
      "=> STEP 2964/6250   lr: 0.000596   giou_loss: 1.58   conf_loss: 123.22   prob_loss: 0.67   total_loss: 125.47\n",
      "=> STEP 2965/6250   lr: 0.000596   giou_loss: 1.58   conf_loss: 123.16   prob_loss: 0.67   total_loss: 125.41\n",
      "=> STEP 2966/6250   lr: 0.000595   giou_loss: 1.56   conf_loss: 123.09   prob_loss: 0.67   total_loss: 125.31\n",
      "=> STEP 2967/6250   lr: 0.000595   giou_loss: 1.66   conf_loss: 123.05   prob_loss: 0.67   total_loss: 125.37\n",
      "=> STEP 2968/6250   lr: 0.000595   giou_loss: 1.65   conf_loss: 122.99   prob_loss: 0.67   total_loss: 125.30\n",
      "=> STEP 2969/6250   lr: 0.000595   giou_loss: 1.56   conf_loss: 122.92   prob_loss: 0.67   total_loss: 125.15\n",
      "=> STEP 2970/6250   lr: 0.000594   giou_loss: 1.69   conf_loss: 122.88   prob_loss: 0.66   total_loss: 125.24\n",
      "=> STEP 2971/6250   lr: 0.000594   giou_loss: 1.64   conf_loss: 122.82   prob_loss: 0.66   total_loss: 125.12\n",
      "=> STEP 2972/6250   lr: 0.000594   giou_loss: 1.60   conf_loss: 122.76   prob_loss: 0.67   total_loss: 125.02\n",
      "=> STEP 2973/6250   lr: 0.000594   giou_loss: 1.66   conf_loss: 122.72   prob_loss: 0.67   total_loss: 125.05\n",
      "=> STEP 2974/6250   lr: 0.000593   giou_loss: 1.62   conf_loss: 122.65   prob_loss: 0.67   total_loss: 124.94\n",
      "=> STEP 2975/6250   lr: 0.000593   giou_loss: 1.63   conf_loss: 122.60   prob_loss: 0.66   total_loss: 124.90\n",
      "=> STEP 2976/6250   lr: 0.000593   giou_loss: 1.64   conf_loss: 122.56   prob_loss: 0.66   total_loss: 124.87\n",
      "=> STEP 2977/6250   lr: 0.000593   giou_loss: 1.70   conf_loss: 122.49   prob_loss: 0.66   total_loss: 124.86\n",
      "=> STEP 2978/6250   lr: 0.000592   giou_loss: 1.65   conf_loss: 122.44   prob_loss: 0.66   total_loss: 124.76\n",
      "=> STEP 2979/6250   lr: 0.000592   giou_loss: 1.62   conf_loss: 122.40   prob_loss: 0.66   total_loss: 124.68\n",
      "=> STEP 2980/6250   lr: 0.000592   giou_loss: 1.64   conf_loss: 122.33   prob_loss: 0.66   total_loss: 124.63\n",
      "=> STEP 2981/6250   lr: 0.000592   giou_loss: 1.57   conf_loss: 122.29   prob_loss: 0.66   total_loss: 124.51\n",
      "=> STEP 2982/6250   lr: 0.000591   giou_loss: 1.67   conf_loss: 122.24   prob_loss: 0.66   total_loss: 124.57\n",
      "=> STEP 2983/6250   lr: 0.000591   giou_loss: 1.64   conf_loss: 122.17   prob_loss: 0.66   total_loss: 124.47\n",
      "=> STEP 2984/6250   lr: 0.000591   giou_loss: 1.56   conf_loss: 122.13   prob_loss: 0.66   total_loss: 124.35\n",
      "=> STEP 2985/6250   lr: 0.000591   giou_loss: 1.60   conf_loss: 122.08   prob_loss: 0.66   total_loss: 124.34\n",
      "=> STEP 2986/6250   lr: 0.000590   giou_loss: 1.60   conf_loss: 122.01   prob_loss: 0.66   total_loss: 124.27\n",
      "=> STEP 2987/6250   lr: 0.000590   giou_loss: 1.57   conf_loss: 121.97   prob_loss: 0.66   total_loss: 124.19\n",
      "=> STEP 2988/6250   lr: 0.000590   giou_loss: 1.71   conf_loss: 121.93   prob_loss: 0.66   total_loss: 124.29\n",
      "=> STEP 2989/6250   lr: 0.000590   giou_loss: 1.67   conf_loss: 121.85   prob_loss: 0.66   total_loss: 124.19\n",
      "=> STEP 2990/6250   lr: 0.000589   giou_loss: 1.57   conf_loss: 121.81   prob_loss: 0.66   total_loss: 124.03\n",
      "=> STEP 2991/6250   lr: 0.000589   giou_loss: 1.56   conf_loss: 121.76   prob_loss: 0.67   total_loss: 123.99\n",
      "=> STEP 2992/6250   lr: 0.000589   giou_loss: 1.65   conf_loss: 121.69   prob_loss: 0.67   total_loss: 124.00\n",
      "=> STEP 2993/6250   lr: 0.000589   giou_loss: 1.60   conf_loss: 121.64   prob_loss: 0.67   total_loss: 123.91\n",
      "=> STEP 2994/6250   lr: 0.000588   giou_loss: 1.63   conf_loss: 121.61   prob_loss: 0.66   total_loss: 123.90\n",
      "=> STEP 2995/6250   lr: 0.000588   giou_loss: 1.66   conf_loss: 121.55   prob_loss: 0.66   total_loss: 123.87\n",
      "=> STEP 2996/6250   lr: 0.000588   giou_loss: 1.75   conf_loss: 121.49   prob_loss: 0.66   total_loss: 123.89\n",
      "=> STEP 2997/6250   lr: 0.000588   giou_loss: 1.68   conf_loss: 121.45   prob_loss: 0.66   total_loss: 123.79\n",
      "=> STEP 2998/6250   lr: 0.000587   giou_loss: 1.58   conf_loss: 121.39   prob_loss: 0.66   total_loss: 123.63\n",
      "=> STEP 2999/6250   lr: 0.000587   giou_loss: 1.75   conf_loss: 121.32   prob_loss: 0.66   total_loss: 123.74\n",
      "=> STEP 3000/6250   lr: 0.000587   giou_loss: 1.78   conf_loss: 121.29   prob_loss: 0.66   total_loss: 123.74\n",
      "=> STEP 3001/6250   lr: 0.000587   giou_loss: 1.68   conf_loss: 121.24   prob_loss: 0.66   total_loss: 123.58\n",
      "=> STEP 3002/6250   lr: 0.000587   giou_loss: 1.63   conf_loss: 121.17   prob_loss: 0.66   total_loss: 123.46\n",
      "=> STEP 3003/6250   lr: 0.000586   giou_loss: 1.74   conf_loss: 121.12   prob_loss: 0.66   total_loss: 123.53\n",
      "=> STEP 3004/6250   lr: 0.000586   giou_loss: 1.70   conf_loss: 121.09   prob_loss: 0.66   total_loss: 123.45\n",
      "=> STEP 3005/6250   lr: 0.000586   giou_loss: 1.64   conf_loss: 121.02   prob_loss: 0.66   total_loss: 123.32\n",
      "=> STEP 3006/6250   lr: 0.000586   giou_loss: 1.71   conf_loss: 120.97   prob_loss: 0.66   total_loss: 123.34\n",
      "=> STEP 3007/6250   lr: 0.000585   giou_loss: 1.72   conf_loss: 120.94   prob_loss: 0.66   total_loss: 123.33\n",
      "=> STEP 3008/6250   lr: 0.000585   giou_loss: 1.73   conf_loss: 120.88   prob_loss: 0.66   total_loss: 123.26\n",
      "=> STEP 3009/6250   lr: 0.000585   giou_loss: 1.62   conf_loss: 120.81   prob_loss: 0.66   total_loss: 123.08\n",
      "=> STEP 3010/6250   lr: 0.000585   giou_loss: 1.67   conf_loss: 120.77   prob_loss: 0.67   total_loss: 123.11\n",
      "=> STEP 3011/6250   lr: 0.000584   giou_loss: 1.72   conf_loss: 120.72   prob_loss: 0.67   total_loss: 123.11\n",
      "=> STEP 3012/6250   lr: 0.000584   giou_loss: 1.62   conf_loss: 120.64   prob_loss: 0.67   total_loss: 122.93\n",
      "=> STEP 3013/6250   lr: 0.000584   giou_loss: 1.67   conf_loss: 120.61   prob_loss: 0.66   total_loss: 122.95\n",
      "=> STEP 3014/6250   lr: 0.000584   giou_loss: 1.76   conf_loss: 120.58   prob_loss: 0.66   total_loss: 122.99\n",
      "=> STEP 3015/6250   lr: 0.000583   giou_loss: 1.71   conf_loss: 120.50   prob_loss: 0.66   total_loss: 122.86\n",
      "=> STEP 3016/6250   lr: 0.000583   giou_loss: 1.56   conf_loss: 120.45   prob_loss: 0.66   total_loss: 122.67\n",
      "=> STEP 3017/6250   lr: 0.000583   giou_loss: 1.69   conf_loss: 120.41   prob_loss: 0.66   total_loss: 122.76\n",
      "=> STEP 3018/6250   lr: 0.000583   giou_loss: 1.69   conf_loss: 120.34   prob_loss: 0.67   total_loss: 122.69\n",
      "=> STEP 3019/6250   lr: 0.000582   giou_loss: 1.56   conf_loss: 120.29   prob_loss: 0.67   total_loss: 122.52\n",
      "=> STEP 3020/6250   lr: 0.000582   giou_loss: 1.62   conf_loss: 120.25   prob_loss: 0.66   total_loss: 122.53\n",
      "=> STEP 3021/6250   lr: 0.000582   giou_loss: 1.58   conf_loss: 120.19   prob_loss: 0.66   total_loss: 122.43\n",
      "=> STEP 3022/6250   lr: 0.000582   giou_loss: 1.56   conf_loss: 120.13   prob_loss: 0.66   total_loss: 122.36\n",
      "=> STEP 3023/6250   lr: 0.000581   giou_loss: 1.68   conf_loss: 120.09   prob_loss: 0.66   total_loss: 122.43\n",
      "=> STEP 3024/6250   lr: 0.000581   giou_loss: 1.67   conf_loss: 120.03   prob_loss: 0.66   total_loss: 122.36\n",
      "=> STEP 3025/6250   lr: 0.000581   giou_loss: 1.60   conf_loss: 119.98   prob_loss: 0.66   total_loss: 122.24\n",
      "=> STEP 3026/6250   lr: 0.000581   giou_loss: 1.57   conf_loss: 119.94   prob_loss: 0.65   total_loss: 122.16\n",
      "=> STEP 3027/6250   lr: 0.000580   giou_loss: 1.68   conf_loss: 119.89   prob_loss: 0.65   total_loss: 122.22\n",
      "=> STEP 3028/6250   lr: 0.000580   giou_loss: 1.62   conf_loss: 119.83   prob_loss: 0.65   total_loss: 122.10\n",
      "=> STEP 3029/6250   lr: 0.000580   giou_loss: 1.59   conf_loss: 119.79   prob_loss: 0.66   total_loss: 122.03\n",
      "=> STEP 3030/6250   lr: 0.000580   giou_loss: 1.69   conf_loss: 119.74   prob_loss: 0.66   total_loss: 122.09\n",
      "=> STEP 3031/6250   lr: 0.000579   giou_loss: 1.69   conf_loss: 119.69   prob_loss: 0.66   total_loss: 122.04\n",
      "=> STEP 3032/6250   lr: 0.000579   giou_loss: 1.57   conf_loss: 119.65   prob_loss: 0.66   total_loss: 121.88\n",
      "=> STEP 3033/6250   lr: 0.000579   giou_loss: 1.65   conf_loss: 119.61   prob_loss: 0.66   total_loss: 121.92\n",
      "=> STEP 3034/6250   lr: 0.000579   giou_loss: 1.74   conf_loss: 119.55   prob_loss: 0.66   total_loss: 121.94\n",
      "=> STEP 3035/6250   lr: 0.000578   giou_loss: 1.67   conf_loss: 119.50   prob_loss: 0.65   total_loss: 121.82\n",
      "=> STEP 3036/6250   lr: 0.000578   giou_loss: 1.56   conf_loss: 119.46   prob_loss: 0.65   total_loss: 121.67\n",
      "=> STEP 3037/6250   lr: 0.000578   giou_loss: 1.61   conf_loss: 119.39   prob_loss: 0.66   total_loss: 121.66\n",
      "=> STEP 3038/6250   lr: 0.000578   giou_loss: 1.61   conf_loss: 119.33   prob_loss: 0.66   total_loss: 121.60\n",
      "=> STEP 3039/6250   lr: 0.000577   giou_loss: 1.59   conf_loss: 119.29   prob_loss: 0.66   total_loss: 121.54\n",
      "=> STEP 3040/6250   lr: 0.000577   giou_loss: 1.62   conf_loss: 119.24   prob_loss: 0.65   total_loss: 121.51\n",
      "=> STEP 3041/6250   lr: 0.000577   giou_loss: 1.66   conf_loss: 119.18   prob_loss: 0.65   total_loss: 121.50\n",
      "=> STEP 3042/6250   lr: 0.000577   giou_loss: 1.64   conf_loss: 119.14   prob_loss: 0.65   total_loss: 121.43\n",
      "=> STEP 3043/6250   lr: 0.000576   giou_loss: 1.57   conf_loss: 119.08   prob_loss: 0.66   total_loss: 121.30\n",
      "=> STEP 3044/6250   lr: 0.000576   giou_loss: 1.58   conf_loss: 119.02   prob_loss: 0.66   total_loss: 121.26\n",
      "=> STEP 3045/6250   lr: 0.000576   giou_loss: 1.60   conf_loss: 118.97   prob_loss: 0.66   total_loss: 121.23\n",
      "=> STEP 3046/6250   lr: 0.000576   giou_loss: 1.56   conf_loss: 118.92   prob_loss: 0.65   total_loss: 121.14\n",
      "=> STEP 3047/6250   lr: 0.000575   giou_loss: 1.62   conf_loss: 118.87   prob_loss: 0.65   total_loss: 121.14\n",
      "=> STEP 3048/6250   lr: 0.000575   giou_loss: 1.59   conf_loss: 118.82   prob_loss: 0.65   total_loss: 121.05\n",
      "=> STEP 3049/6250   lr: 0.000575   giou_loss: 1.56   conf_loss: 118.77   prob_loss: 0.65   total_loss: 120.99\n",
      "=> STEP 3050/6250   lr: 0.000575   giou_loss: 1.68   conf_loss: 118.71   prob_loss: 0.66   total_loss: 121.05\n",
      "=> STEP 3051/6250   lr: 0.000574   giou_loss: 1.67   conf_loss: 118.66   prob_loss: 0.66   total_loss: 120.99\n",
      "=> STEP 3052/6250   lr: 0.000574   giou_loss: 1.57   conf_loss: 118.62   prob_loss: 0.66   total_loss: 120.85\n",
      "=> STEP 3053/6250   lr: 0.000574   giou_loss: 1.66   conf_loss: 118.58   prob_loss: 0.65   total_loss: 120.88\n",
      "=> STEP 3054/6250   lr: 0.000574   giou_loss: 1.71   conf_loss: 118.52   prob_loss: 0.65   total_loss: 120.88\n",
      "=> STEP 3055/6250   lr: 0.000573   giou_loss: 1.66   conf_loss: 118.48   prob_loss: 0.65   total_loss: 120.78\n",
      "=> STEP 3056/6250   lr: 0.000573   giou_loss: 1.61   conf_loss: 118.42   prob_loss: 0.66   total_loss: 120.69\n",
      "=> STEP 3057/6250   lr: 0.000573   giou_loss: 1.63   conf_loss: 118.36   prob_loss: 0.66   total_loss: 120.64\n",
      "=> STEP 3058/6250   lr: 0.000573   giou_loss: 1.63   conf_loss: 118.31   prob_loss: 0.66   total_loss: 120.60\n",
      "=> STEP 3059/6250   lr: 0.000572   giou_loss: 1.56   conf_loss: 118.28   prob_loss: 0.66   total_loss: 120.50\n",
      "=> STEP 3060/6250   lr: 0.000572   giou_loss: 1.60   conf_loss: 118.22   prob_loss: 0.65   total_loss: 120.47\n",
      "=> STEP 3061/6250   lr: 0.000572   giou_loss: 1.57   conf_loss: 118.16   prob_loss: 0.65   total_loss: 120.38\n",
      "=> STEP 3062/6250   lr: 0.000572   giou_loss: 1.58   conf_loss: 118.13   prob_loss: 0.65   total_loss: 120.36\n",
      "=> STEP 3063/6250   lr: 0.000571   giou_loss: 1.59   conf_loss: 118.06   prob_loss: 0.65   total_loss: 120.30\n",
      "=> STEP 3064/6250   lr: 0.000571   giou_loss: 1.57   conf_loss: 118.00   prob_loss: 0.65   total_loss: 120.22\n",
      "=> STEP 3065/6250   lr: 0.000571   giou_loss: 1.62   conf_loss: 117.97   prob_loss: 0.65   total_loss: 120.24\n",
      "=> STEP 3066/6250   lr: 0.000571   giou_loss: 1.59   conf_loss: 117.91   prob_loss: 0.65   total_loss: 120.15\n",
      "=> STEP 3067/6250   lr: 0.000570   giou_loss: 1.56   conf_loss: 117.85   prob_loss: 0.65   total_loss: 120.07\n",
      "=> STEP 3068/6250   lr: 0.000570   giou_loss: 1.68   conf_loss: 117.81   prob_loss: 0.65   total_loss: 120.14\n",
      "=> STEP 3069/6250   lr: 0.000570   giou_loss: 1.68   conf_loss: 117.76   prob_loss: 0.65   total_loss: 120.09\n",
      "=> STEP 3070/6250   lr: 0.000570   giou_loss: 1.57   conf_loss: 117.70   prob_loss: 0.65   total_loss: 119.93\n",
      "=> STEP 3071/6250   lr: 0.000569   giou_loss: 1.64   conf_loss: 117.67   prob_loss: 0.65   total_loss: 119.95\n",
      "=> STEP 3072/6250   lr: 0.000569   giou_loss: 1.71   conf_loss: 117.61   prob_loss: 0.65   total_loss: 119.97\n",
      "=> STEP 3073/6250   lr: 0.000569   giou_loss: 1.66   conf_loss: 117.55   prob_loss: 0.65   total_loss: 119.86\n",
      "=> STEP 3074/6250   lr: 0.000569   giou_loss: 1.56   conf_loss: 117.51   prob_loss: 0.65   total_loss: 119.73\n",
      "=> STEP 3075/6250   lr: 0.000568   giou_loss: 1.62   conf_loss: 117.46   prob_loss: 0.65   total_loss: 119.73\n",
      "=> STEP 3076/6250   lr: 0.000568   giou_loss: 1.61   conf_loss: 117.40   prob_loss: 0.65   total_loss: 119.66\n",
      "=> STEP 3077/6250   lr: 0.000568   giou_loss: 1.56   conf_loss: 117.36   prob_loss: 0.65   total_loss: 119.58\n",
      "=> STEP 3078/6250   lr: 0.000568   giou_loss: 1.61   conf_loss: 117.31   prob_loss: 0.65   total_loss: 119.57\n",
      "=> STEP 3079/6250   lr: 0.000567   giou_loss: 1.58   conf_loss: 117.25   prob_loss: 0.65   total_loss: 119.48\n",
      "=> STEP 3080/6250   lr: 0.000567   giou_loss: 1.59   conf_loss: 117.21   prob_loss: 0.65   total_loss: 119.45\n",
      "=> STEP 3081/6250   lr: 0.000567   giou_loss: 1.60   conf_loss: 117.16   prob_loss: 0.65   total_loss: 119.42\n",
      "=> STEP 3082/6250   lr: 0.000567   giou_loss: 1.59   conf_loss: 117.10   prob_loss: 0.65   total_loss: 119.35\n",
      "=> STEP 3083/6250   lr: 0.000566   giou_loss: 1.65   conf_loss: 117.06   prob_loss: 0.65   total_loss: 119.36\n",
      "=> STEP 3084/6250   lr: 0.000566   giou_loss: 1.62   conf_loss: 117.01   prob_loss: 0.65   total_loss: 119.28\n",
      "=> STEP 3085/6250   lr: 0.000566   giou_loss: 1.56   conf_loss: 116.95   prob_loss: 0.65   total_loss: 119.17\n",
      "=> STEP 3086/6250   lr: 0.000566   giou_loss: 1.73   conf_loss: 116.91   prob_loss: 0.65   total_loss: 119.29\n",
      "=> STEP 3087/6250   lr: 0.000565   giou_loss: 1.71   conf_loss: 116.86   prob_loss: 0.65   total_loss: 119.22\n",
      "=> STEP 3088/6250   lr: 0.000565   giou_loss: 1.56   conf_loss: 116.81   prob_loss: 0.64   total_loss: 119.02\n",
      "=> STEP 3089/6250   lr: 0.000565   giou_loss: 1.66   conf_loss: 116.77   prob_loss: 0.64   total_loss: 119.07\n",
      "=> STEP 3090/6250   lr: 0.000565   giou_loss: 1.68   conf_loss: 116.72   prob_loss: 0.64   total_loss: 119.04\n",
      "=> STEP 3091/6250   lr: 0.000564   giou_loss: 1.58   conf_loss: 116.66   prob_loss: 0.64   total_loss: 118.89\n",
      "=> STEP 3092/6250   lr: 0.000564   giou_loss: 1.65   conf_loss: 116.61   prob_loss: 0.65   total_loss: 118.91\n",
      "=> STEP 3093/6250   lr: 0.000564   giou_loss: 1.75   conf_loss: 116.57   prob_loss: 0.65   total_loss: 118.97\n",
      "=> STEP 3094/6250   lr: 0.000564   giou_loss: 1.73   conf_loss: 116.51   prob_loss: 0.65   total_loss: 118.89\n",
      "=> STEP 3095/6250   lr: 0.000564   giou_loss: 1.65   conf_loss: 116.46   prob_loss: 0.65   total_loss: 118.76\n",
      "=> STEP 3096/6250   lr: 0.000563   giou_loss: 1.57   conf_loss: 116.42   prob_loss: 0.65   total_loss: 118.64\n",
      "=> STEP 3097/6250   lr: 0.000563   giou_loss: 1.69   conf_loss: 116.37   prob_loss: 0.64   total_loss: 118.71\n",
      "=> STEP 3098/6250   lr: 0.000563   giou_loss: 1.73   conf_loss: 116.32   prob_loss: 0.64   total_loss: 118.69\n",
      "=> STEP 3099/6250   lr: 0.000563   giou_loss: 1.66   conf_loss: 116.28   prob_loss: 0.64   total_loss: 118.58\n",
      "=> STEP 3100/6250   lr: 0.000562   giou_loss: 1.56   conf_loss: 116.23   prob_loss: 0.64   total_loss: 118.44\n",
      "=> STEP 3101/6250   lr: 0.000562   giou_loss: 1.65   conf_loss: 116.17   prob_loss: 0.65   total_loss: 118.46\n",
      "=> STEP 3102/6250   lr: 0.000562   giou_loss: 1.67   conf_loss: 116.12   prob_loss: 0.65   total_loss: 118.44\n",
      "=> STEP 3103/6250   lr: 0.000562   giou_loss: 1.58   conf_loss: 116.08   prob_loss: 0.65   total_loss: 118.31\n",
      "=> STEP 3104/6250   lr: 0.000561   giou_loss: 1.61   conf_loss: 116.03   prob_loss: 0.64   total_loss: 118.29\n",
      "=> STEP 3105/6250   lr: 0.000561   giou_loss: 1.67   conf_loss: 115.98   prob_loss: 0.64   total_loss: 118.29\n",
      "=> STEP 3106/6250   lr: 0.000561   giou_loss: 1.61   conf_loss: 115.94   prob_loss: 0.64   total_loss: 118.19\n",
      "=> STEP 3107/6250   lr: 0.000561   giou_loss: 1.56   conf_loss: 115.89   prob_loss: 0.64   total_loss: 118.09\n",
      "=> STEP 3108/6250   lr: 0.000560   giou_loss: 1.71   conf_loss: 115.83   prob_loss: 0.65   total_loss: 118.19\n",
      "=> STEP 3109/6250   lr: 0.000560   giou_loss: 1.71   conf_loss: 115.79   prob_loss: 0.65   total_loss: 118.14\n",
      "=> STEP 3110/6250   lr: 0.000560   giou_loss: 1.63   conf_loss: 115.74   prob_loss: 0.65   total_loss: 118.01\n",
      "=> STEP 3111/6250   lr: 0.000560   giou_loss: 1.59   conf_loss: 115.70   prob_loss: 0.64   total_loss: 117.93\n",
      "=> STEP 3112/6250   lr: 0.000559   giou_loss: 1.71   conf_loss: 115.65   prob_loss: 0.64   total_loss: 118.00\n",
      "=> STEP 3113/6250   lr: 0.000559   giou_loss: 1.74   conf_loss: 115.60   prob_loss: 0.64   total_loss: 117.98\n",
      "=> STEP 3114/6250   lr: 0.000559   giou_loss: 1.67   conf_loss: 115.55   prob_loss: 0.64   total_loss: 117.87\n",
      "=> STEP 3115/6250   lr: 0.000559   giou_loss: 1.57   conf_loss: 115.50   prob_loss: 0.65   total_loss: 117.71\n",
      "=> STEP 3116/6250   lr: 0.000558   giou_loss: 1.65   conf_loss: 115.45   prob_loss: 0.65   total_loss: 117.75\n",
      "=> STEP 3117/6250   lr: 0.000558   giou_loss: 1.66   conf_loss: 115.41   prob_loss: 0.65   total_loss: 117.71\n",
      "=> STEP 3118/6250   lr: 0.000558   giou_loss: 1.56   conf_loss: 115.37   prob_loss: 0.65   total_loss: 117.57\n",
      "=> STEP 3119/6250   lr: 0.000558   giou_loss: 1.57   conf_loss: 115.32   prob_loss: 0.64   total_loss: 117.53\n",
      "=> STEP 3120/6250   lr: 0.000557   giou_loss: 1.63   conf_loss: 115.27   prob_loss: 0.64   total_loss: 117.54\n",
      "=> STEP 3121/6250   lr: 0.000557   giou_loss: 1.57   conf_loss: 115.22   prob_loss: 0.64   total_loss: 117.43\n",
      "=> STEP 3122/6250   lr: 0.000557   giou_loss: 1.61   conf_loss: 115.17   prob_loss: 0.64   total_loss: 117.42\n",
      "=> STEP 3123/6250   lr: 0.000557   giou_loss: 1.64   conf_loss: 115.12   prob_loss: 0.64   total_loss: 117.40\n",
      "=> STEP 3124/6250   lr: 0.000556   giou_loss: 1.57   conf_loss: 115.07   prob_loss: 0.64   total_loss: 117.29\n",
      "=> STEP 3125/6250   lr: 0.000556   giou_loss: 1.59   conf_loss: 115.03   prob_loss: 0.64   total_loss: 117.26\n",
      "=> STEP 3126/6250   lr: 0.000556   giou_loss: 1.57   conf_loss: 114.98   prob_loss: 0.64   total_loss: 117.18\n",
      "=> STEP 3127/6250   lr: 0.000556   giou_loss: 1.70   conf_loss: 114.93   prob_loss: 0.64   total_loss: 117.27\n",
      "=> STEP 3128/6250   lr: 0.000555   giou_loss: 1.65   conf_loss: 114.88   prob_loss: 0.64   total_loss: 117.17\n",
      "=> STEP 3129/6250   lr: 0.000555   giou_loss: 1.56   conf_loss: 114.83   prob_loss: 0.64   total_loss: 117.03\n",
      "=> STEP 3130/6250   lr: 0.000555   giou_loss: 1.62   conf_loss: 114.78   prob_loss: 0.64   total_loss: 117.04\n",
      "=> STEP 3131/6250   lr: 0.000555   giou_loss: 1.63   conf_loss: 114.73   prob_loss: 0.64   total_loss: 117.00\n",
      "=> STEP 3132/6250   lr: 0.000554   giou_loss: 1.62   conf_loss: 114.68   prob_loss: 0.64   total_loss: 116.95\n",
      "=> STEP 3133/6250   lr: 0.000554   giou_loss: 1.65   conf_loss: 114.63   prob_loss: 0.64   total_loss: 116.93\n",
      "=> STEP 3134/6250   lr: 0.000554   giou_loss: 1.57   conf_loss: 114.59   prob_loss: 0.64   total_loss: 116.80\n",
      "=> STEP 3135/6250   lr: 0.000554   giou_loss: 1.60   conf_loss: 114.55   prob_loss: 0.64   total_loss: 116.79\n",
      "=> STEP 3136/6250   lr: 0.000553   giou_loss: 1.56   conf_loss: 114.49   prob_loss: 0.64   total_loss: 116.70\n",
      "=> STEP 3137/6250   lr: 0.000553   giou_loss: 1.67   conf_loss: 114.44   prob_loss: 0.65   total_loss: 116.76\n",
      "=> STEP 3138/6250   lr: 0.000553   giou_loss: 1.68   conf_loss: 114.40   prob_loss: 0.65   total_loss: 116.72\n",
      "=> STEP 3139/6250   lr: 0.000553   giou_loss: 1.60   conf_loss: 114.35   prob_loss: 0.64   total_loss: 116.59\n",
      "=> STEP 3140/6250   lr: 0.000552   giou_loss: 1.65   conf_loss: 114.31   prob_loss: 0.64   total_loss: 116.60\n",
      "=> STEP 3141/6250   lr: 0.000552   giou_loss: 1.71   conf_loss: 114.27   prob_loss: 0.64   total_loss: 116.61\n",
      "=> STEP 3142/6250   lr: 0.000552   giou_loss: 1.64   conf_loss: 114.21   prob_loss: 0.64   total_loss: 116.50\n",
      "=> STEP 3143/6250   lr: 0.000552   giou_loss: 1.58   conf_loss: 114.16   prob_loss: 0.64   total_loss: 116.38\n",
      "=> STEP 3144/6250   lr: 0.000551   giou_loss: 1.60   conf_loss: 114.11   prob_loss: 0.65   total_loss: 116.36\n",
      "=> STEP 3145/6250   lr: 0.000551   giou_loss: 1.64   conf_loss: 114.07   prob_loss: 0.65   total_loss: 116.35\n",
      "=> STEP 3146/6250   lr: 0.000551   giou_loss: 1.66   conf_loss: 114.02   prob_loss: 0.64   total_loss: 116.33\n",
      "=> STEP 3147/6250   lr: 0.000551   giou_loss: 1.56   conf_loss: 113.98   prob_loss: 0.64   total_loss: 116.18\n",
      "=> STEP 3148/6250   lr: 0.000550   giou_loss: 1.66   conf_loss: 113.93   prob_loss: 0.64   total_loss: 116.23\n",
      "=> STEP 3149/6250   lr: 0.000550   giou_loss: 1.69   conf_loss: 113.88   prob_loss: 0.64   total_loss: 116.21\n",
      "=> STEP 3150/6250   lr: 0.000550   giou_loss: 1.57   conf_loss: 113.84   prob_loss: 0.64   total_loss: 116.04\n",
      "=> STEP 3151/6250   lr: 0.000550   giou_loss: 1.65   conf_loss: 113.79   prob_loss: 0.64   total_loss: 116.08\n",
      "=> STEP 3152/6250   lr: 0.000549   giou_loss: 1.65   conf_loss: 113.74   prob_loss: 0.64   total_loss: 116.03\n",
      "=> STEP 3153/6250   lr: 0.000549   giou_loss: 1.61   conf_loss: 113.69   prob_loss: 0.65   total_loss: 115.95\n",
      "=> STEP 3154/6250   lr: 0.000549   giou_loss: 1.64   conf_loss: 113.65   prob_loss: 0.64   total_loss: 115.93\n",
      "=> STEP 3155/6250   lr: 0.000549   giou_loss: 1.56   conf_loss: 113.60   prob_loss: 0.64   total_loss: 115.81\n",
      "=> STEP 3156/6250   lr: 0.000548   giou_loss: 1.58   conf_loss: 113.56   prob_loss: 0.64   total_loss: 115.78\n",
      "=> STEP 3157/6250   lr: 0.000548   giou_loss: 1.56   conf_loss: 113.51   prob_loss: 0.64   total_loss: 115.72\n",
      "=> STEP 3158/6250   lr: 0.000548   giou_loss: 1.57   conf_loss: 113.46   prob_loss: 0.64   total_loss: 115.67\n",
      "=> STEP 3159/6250   lr: 0.000548   giou_loss: 1.60   conf_loss: 113.42   prob_loss: 0.64   total_loss: 115.66\n",
      "=> STEP 3160/6250   lr: 0.000547   giou_loss: 1.57   conf_loss: 113.37   prob_loss: 0.64   total_loss: 115.58\n",
      "=> STEP 3161/6250   lr: 0.000547   giou_loss: 1.62   conf_loss: 113.33   prob_loss: 0.64   total_loss: 115.58\n",
      "=> STEP 3162/6250   lr: 0.000547   giou_loss: 1.57   conf_loss: 113.28   prob_loss: 0.64   total_loss: 115.49\n",
      "=> STEP 3163/6250   lr: 0.000547   giou_loss: 1.65   conf_loss: 113.23   prob_loss: 0.64   total_loss: 115.52\n",
      "=> STEP 3164/6250   lr: 0.000546   giou_loss: 1.68   conf_loss: 113.18   prob_loss: 0.64   total_loss: 115.51\n",
      "=> STEP 3165/6250   lr: 0.000546   giou_loss: 1.61   conf_loss: 113.14   prob_loss: 0.64   total_loss: 115.38\n",
      "=> STEP 3166/6250   lr: 0.000546   giou_loss: 1.64   conf_loss: 113.09   prob_loss: 0.64   total_loss: 115.37\n",
      "=> STEP 3167/6250   lr: 0.000546   giou_loss: 1.69   conf_loss: 113.05   prob_loss: 0.64   total_loss: 115.38\n",
      "=> STEP 3168/6250   lr: 0.000545   giou_loss: 1.63   conf_loss: 113.00   prob_loss: 0.64   total_loss: 115.27\n",
      "=> STEP 3169/6250   lr: 0.000545   giou_loss: 1.60   conf_loss: 112.95   prob_loss: 0.64   total_loss: 115.19\n",
      "=> STEP 3170/6250   lr: 0.000545   giou_loss: 1.64   conf_loss: 112.91   prob_loss: 0.64   total_loss: 115.19\n",
      "=> STEP 3171/6250   lr: 0.000545   giou_loss: 1.56   conf_loss: 112.86   prob_loss: 0.64   total_loss: 115.07\n",
      "=> STEP 3172/6250   lr: 0.000544   giou_loss: 1.57   conf_loss: 112.82   prob_loss: 0.64   total_loss: 115.03\n",
      "=> STEP 3173/6250   lr: 0.000544   giou_loss: 1.56   conf_loss: 112.77   prob_loss: 0.64   total_loss: 114.98\n",
      "=> STEP 3174/6250   lr: 0.000544   giou_loss: 1.59   conf_loss: 112.72   prob_loss: 0.64   total_loss: 114.96\n",
      "=> STEP 3175/6250   lr: 0.000544   giou_loss: 1.56   conf_loss: 112.68   prob_loss: 0.64   total_loss: 114.89\n",
      "=> STEP 3176/6250   lr: 0.000543   giou_loss: 1.61   conf_loss: 112.64   prob_loss: 0.64   total_loss: 114.89\n",
      "=> STEP 3177/6250   lr: 0.000543   giou_loss: 1.57   conf_loss: 112.59   prob_loss: 0.64   total_loss: 114.79\n",
      "=> STEP 3178/6250   lr: 0.000543   giou_loss: 1.57   conf_loss: 112.54   prob_loss: 0.64   total_loss: 114.75\n",
      "=> STEP 3179/6250   lr: 0.000543   giou_loss: 1.56   conf_loss: 112.50   prob_loss: 0.64   total_loss: 114.70\n",
      "=> STEP 3180/6250   lr: 0.000542   giou_loss: 1.63   conf_loss: 112.45   prob_loss: 0.64   total_loss: 114.73\n",
      "=> STEP 3181/6250   lr: 0.000542   giou_loss: 1.58   conf_loss: 112.41   prob_loss: 0.64   total_loss: 114.63\n",
      "=> STEP 3182/6250   lr: 0.000542   giou_loss: 1.66   conf_loss: 112.37   prob_loss: 0.64   total_loss: 114.66\n",
      "=> STEP 3183/6250   lr: 0.000542   giou_loss: 1.77   conf_loss: 112.32   prob_loss: 0.63   total_loss: 114.72\n",
      "=> STEP 3184/6250   lr: 0.000541   giou_loss: 1.67   conf_loss: 112.28   prob_loss: 0.63   total_loss: 114.58\n",
      "=> STEP 3185/6250   lr: 0.000541   giou_loss: 1.60   conf_loss: 112.24   prob_loss: 0.63   total_loss: 114.47\n",
      "=> STEP 3186/6250   lr: 0.000541   giou_loss: 1.63   conf_loss: 112.19   prob_loss: 0.64   total_loss: 114.46\n",
      "=> STEP 3187/6250   lr: 0.000541   giou_loss: 1.57   conf_loss: 112.15   prob_loss: 0.64   total_loss: 114.35\n",
      "=> STEP 3188/6250   lr: 0.000540   giou_loss: 1.70   conf_loss: 112.10   prob_loss: 0.64   total_loss: 114.44\n",
      "=> STEP 3189/6250   lr: 0.000540   giou_loss: 1.63   conf_loss: 112.05   prob_loss: 0.64   total_loss: 114.32\n",
      "=> STEP 3190/6250   lr: 0.000540   giou_loss: 1.64   conf_loss: 112.01   prob_loss: 0.64   total_loss: 114.29\n",
      "=> STEP 3191/6250   lr: 0.000540   giou_loss: 1.74   conf_loss: 111.97   prob_loss: 0.64   total_loss: 114.35\n",
      "=> STEP 3192/6250   lr: 0.000539   giou_loss: 1.65   conf_loss: 111.92   prob_loss: 0.64   total_loss: 114.20\n",
      "=> STEP 3193/6250   lr: 0.000539   giou_loss: 1.69   conf_loss: 111.88   prob_loss: 0.64   total_loss: 114.21\n",
      "=> STEP 3194/6250   lr: 0.000539   giou_loss: 1.79   conf_loss: 111.84   prob_loss: 0.64   total_loss: 114.27\n",
      "=> STEP 3195/6250   lr: 0.000539   giou_loss: 1.73   conf_loss: 111.79   prob_loss: 0.64   total_loss: 114.15\n",
      "=> STEP 3196/6250   lr: 0.000538   giou_loss: 1.56   conf_loss: 111.75   prob_loss: 0.63   total_loss: 113.95\n",
      "=> STEP 3197/6250   lr: 0.000538   giou_loss: 1.72   conf_loss: 111.71   prob_loss: 0.63   total_loss: 114.07\n",
      "=> STEP 3198/6250   lr: 0.000538   giou_loss: 1.72   conf_loss: 111.66   prob_loss: 0.63   total_loss: 114.01\n",
      "=> STEP 3199/6250   lr: 0.000538   giou_loss: 1.56   conf_loss: 111.61   prob_loss: 0.64   total_loss: 113.82\n",
      "=> STEP 3200/6250   lr: 0.000537   giou_loss: 1.64   conf_loss: 111.57   prob_loss: 0.64   total_loss: 113.85\n",
      "=> STEP 3201/6250   lr: 0.000537   giou_loss: 1.58   conf_loss: 111.52   prob_loss: 0.64   total_loss: 113.74\n",
      "=> STEP 3202/6250   lr: 0.000537   giou_loss: 1.65   conf_loss: 111.48   prob_loss: 0.64   total_loss: 113.76\n",
      "=> STEP 3203/6250   lr: 0.000537   giou_loss: 1.78   conf_loss: 111.44   prob_loss: 0.64   total_loss: 113.86\n",
      "=> STEP 3204/6250   lr: 0.000536   giou_loss: 1.70   conf_loss: 111.39   prob_loss: 0.64   total_loss: 113.73\n",
      "=> STEP 3205/6250   lr: 0.000536   giou_loss: 1.65   conf_loss: 111.34   prob_loss: 0.64   total_loss: 113.63\n",
      "=> STEP 3206/6250   lr: 0.000536   giou_loss: 1.77   conf_loss: 111.31   prob_loss: 0.64   total_loss: 113.72\n",
      "=> STEP 3207/6250   lr: 0.000536   giou_loss: 1.79   conf_loss: 111.26   prob_loss: 0.63   total_loss: 113.68\n",
      "=> STEP 3208/6250   lr: 0.000535   giou_loss: 1.65   conf_loss: 111.22   prob_loss: 0.63   total_loss: 113.50\n",
      "=> STEP 3209/6250   lr: 0.000535   giou_loss: 1.68   conf_loss: 111.18   prob_loss: 0.63   total_loss: 113.49\n",
      "=> STEP 3210/6250   lr: 0.000535   giou_loss: 1.72   conf_loss: 111.13   prob_loss: 0.64   total_loss: 113.49\n",
      "=> STEP 3211/6250   lr: 0.000535   giou_loss: 1.62   conf_loss: 111.08   prob_loss: 0.64   total_loss: 113.34\n",
      "=> STEP 3212/6250   lr: 0.000534   giou_loss: 1.66   conf_loss: 111.03   prob_loss: 0.64   total_loss: 113.34\n",
      "=> STEP 3213/6250   lr: 0.000534   giou_loss: 1.78   conf_loss: 110.99   prob_loss: 0.64   total_loss: 113.42\n",
      "=> STEP 3214/6250   lr: 0.000534   giou_loss: 1.74   conf_loss: 110.95   prob_loss: 0.64   total_loss: 113.32\n",
      "=> STEP 3215/6250   lr: 0.000534   giou_loss: 1.57   conf_loss: 110.91   prob_loss: 0.64   total_loss: 113.11\n",
      "=> STEP 3216/6250   lr: 0.000533   giou_loss: 1.78   conf_loss: 110.87   prob_loss: 0.63   total_loss: 113.28\n",
      "=> STEP 3217/6250   lr: 0.000533   giou_loss: 1.87   conf_loss: 110.83   prob_loss: 0.63   total_loss: 113.33\n",
      "=> STEP 3218/6250   lr: 0.000533   giou_loss: 1.81   conf_loss: 110.78   prob_loss: 0.63   total_loss: 113.23\n",
      "=> STEP 3219/6250   lr: 0.000533   giou_loss: 1.73   conf_loss: 110.73   prob_loss: 0.64   total_loss: 113.10\n",
      "=> STEP 3220/6250   lr: 0.000532   giou_loss: 1.62   conf_loss: 110.68   prob_loss: 0.64   total_loss: 112.94\n",
      "=> STEP 3221/6250   lr: 0.000532   giou_loss: 1.69   conf_loss: 110.64   prob_loss: 0.64   total_loss: 112.97\n",
      "=> STEP 3222/6250   lr: 0.000532   giou_loss: 1.70   conf_loss: 110.59   prob_loss: 0.64   total_loss: 112.94\n",
      "=> STEP 3223/6250   lr: 0.000532   giou_loss: 1.73   conf_loss: 110.55   prob_loss: 0.64   total_loss: 112.92\n",
      "=> STEP 3224/6250   lr: 0.000531   giou_loss: 1.72   conf_loss: 110.51   prob_loss: 0.64   total_loss: 112.86\n",
      "=> STEP 3225/6250   lr: 0.000531   giou_loss: 1.67   conf_loss: 110.47   prob_loss: 0.64   total_loss: 112.78\n",
      "=> STEP 3226/6250   lr: 0.000531   giou_loss: 1.57   conf_loss: 110.43   prob_loss: 0.63   total_loss: 112.63\n",
      "=> STEP 3227/6250   lr: 0.000531   giou_loss: 1.74   conf_loss: 110.39   prob_loss: 0.63   total_loss: 112.76\n",
      "=> STEP 3228/6250   lr: 0.000530   giou_loss: 1.75   conf_loss: 110.35   prob_loss: 0.63   total_loss: 112.72\n",
      "=> STEP 3229/6250   lr: 0.000530   giou_loss: 1.69   conf_loss: 110.30   prob_loss: 0.63   total_loss: 112.62\n",
      "=> STEP 3230/6250   lr: 0.000530   giou_loss: 1.60   conf_loss: 110.25   prob_loss: 0.63   total_loss: 112.48\n",
      "=> STEP 3231/6250   lr: 0.000530   giou_loss: 1.58   conf_loss: 110.20   prob_loss: 0.64   total_loss: 112.42\n",
      "=> STEP 3232/6250   lr: 0.000529   giou_loss: 1.61   conf_loss: 110.16   prob_loss: 0.64   total_loss: 112.42\n",
      "=> STEP 3233/6250   lr: 0.000529   giou_loss: 1.57   conf_loss: 110.13   prob_loss: 0.64   total_loss: 112.33\n",
      "=> STEP 3234/6250   lr: 0.000529   giou_loss: 1.59   conf_loss: 110.08   prob_loss: 0.63   total_loss: 112.30\n",
      "=> STEP 3235/6250   lr: 0.000529   giou_loss: 1.57   conf_loss: 110.04   prob_loss: 0.63   total_loss: 112.23\n",
      "=> STEP 3236/6250   lr: 0.000528   giou_loss: 1.60   conf_loss: 110.01   prob_loss: 0.63   total_loss: 112.24\n",
      "=> STEP 3237/6250   lr: 0.000528   giou_loss: 1.56   conf_loss: 109.96   prob_loss: 0.63   total_loss: 112.15\n",
      "=> STEP 3238/6250   lr: 0.000528   giou_loss: 1.60   conf_loss: 109.91   prob_loss: 0.63   total_loss: 112.15\n",
      "=> STEP 3239/6250   lr: 0.000528   giou_loss: 1.56   conf_loss: 109.87   prob_loss: 0.63   total_loss: 112.07\n",
      "=> STEP 3240/6250   lr: 0.000527   giou_loss: 1.58   conf_loss: 109.83   prob_loss: 0.63   total_loss: 112.04\n",
      "=> STEP 3241/6250   lr: 0.000527   giou_loss: 1.58   conf_loss: 109.79   prob_loss: 0.63   total_loss: 111.99\n",
      "=> STEP 3242/6250   lr: 0.000527   giou_loss: 1.57   conf_loss: 109.74   prob_loss: 0.63   total_loss: 111.94\n",
      "=> STEP 3243/6250   lr: 0.000527   giou_loss: 1.56   conf_loss: 109.70   prob_loss: 0.63   total_loss: 111.89\n",
      "=> STEP 3244/6250   lr: 0.000526   giou_loss: 1.58   conf_loss: 109.65   prob_loss: 0.63   total_loss: 111.86\n",
      "=> STEP 3245/6250   lr: 0.000526   giou_loss: 1.56   conf_loss: 109.60   prob_loss: 0.63   total_loss: 111.80\n",
      "=> STEP 3246/6250   lr: 0.000526   giou_loss: 1.64   conf_loss: 109.56   prob_loss: 0.63   total_loss: 111.83\n",
      "=> STEP 3247/6250   lr: 0.000526   giou_loss: 1.58   conf_loss: 109.52   prob_loss: 0.63   total_loss: 111.73\n",
      "=> STEP 3248/6250   lr: 0.000525   giou_loss: 1.63   conf_loss: 109.48   prob_loss: 0.63   total_loss: 111.74\n",
      "=> STEP 3249/6250   lr: 0.000525   giou_loss: 1.66   conf_loss: 109.44   prob_loss: 0.63   total_loss: 111.72\n",
      "=> STEP 3250/6250   lr: 0.000525   giou_loss: 1.56   conf_loss: 109.39   prob_loss: 0.63   total_loss: 111.59\n",
      "=> STEP 3251/6250   lr: 0.000525   giou_loss: 1.68   conf_loss: 109.35   prob_loss: 0.63   total_loss: 111.66\n",
      "=> STEP 3252/6250   lr: 0.000525   giou_loss: 1.73   conf_loss: 109.30   prob_loss: 0.64   total_loss: 111.66\n",
      "=> STEP 3253/6250   lr: 0.000524   giou_loss: 1.70   conf_loss: 109.26   prob_loss: 0.64   total_loss: 111.59\n",
      "=> STEP 3254/6250   lr: 0.000524   giou_loss: 1.56   conf_loss: 109.22   prob_loss: 0.63   total_loss: 111.42\n",
      "=> STEP 3255/6250   lr: 0.000524   giou_loss: 1.64   conf_loss: 109.18   prob_loss: 0.63   total_loss: 111.45\n",
      "=> STEP 3256/6250   lr: 0.000524   giou_loss: 1.64   conf_loss: 109.14   prob_loss: 0.63   total_loss: 111.40\n",
      "=> STEP 3257/6250   lr: 0.000523   giou_loss: 1.58   conf_loss: 109.09   prob_loss: 0.63   total_loss: 111.30\n",
      "=> STEP 3258/6250   lr: 0.000523   giou_loss: 1.61   conf_loss: 109.05   prob_loss: 0.63   total_loss: 111.29\n",
      "=> STEP 3259/6250   lr: 0.000523   giou_loss: 1.57   conf_loss: 109.00   prob_loss: 0.63   total_loss: 111.21\n",
      "=> STEP 3260/6250   lr: 0.000523   giou_loss: 1.68   conf_loss: 108.96   prob_loss: 0.63   total_loss: 111.27\n",
      "=> STEP 3261/6250   lr: 0.000522   giou_loss: 1.65   conf_loss: 108.92   prob_loss: 0.63   total_loss: 111.20\n",
      "=> STEP 3262/6250   lr: 0.000522   giou_loss: 1.57   conf_loss: 108.88   prob_loss: 0.63   total_loss: 111.08\n",
      "=> STEP 3263/6250   lr: 0.000522   giou_loss: 1.63   conf_loss: 108.83   prob_loss: 0.63   total_loss: 111.09\n",
      "=> STEP 3264/6250   lr: 0.000522   giou_loss: 1.56   conf_loss: 108.80   prob_loss: 0.63   total_loss: 110.99\n",
      "=> STEP 3265/6250   lr: 0.000521   giou_loss: 1.58   conf_loss: 108.76   prob_loss: 0.63   total_loss: 110.97\n",
      "=> STEP 3266/6250   lr: 0.000521   giou_loss: 1.56   conf_loss: 108.71   prob_loss: 0.63   total_loss: 110.90\n",
      "=> STEP 3267/6250   lr: 0.000521   giou_loss: 1.59   conf_loss: 108.67   prob_loss: 0.63   total_loss: 110.89\n",
      "=> STEP 3268/6250   lr: 0.000521   giou_loss: 1.57   conf_loss: 108.63   prob_loss: 0.63   total_loss: 110.83\n",
      "=> STEP 3269/6250   lr: 0.000520   giou_loss: 1.64   conf_loss: 108.58   prob_loss: 0.63   total_loss: 110.85\n",
      "=> STEP 3270/6250   lr: 0.000520   giou_loss: 1.60   conf_loss: 108.54   prob_loss: 0.63   total_loss: 110.77\n",
      "=> STEP 3271/6250   lr: 0.000520   giou_loss: 1.60   conf_loss: 108.50   prob_loss: 0.63   total_loss: 110.73\n",
      "=> STEP 3272/6250   lr: 0.000520   giou_loss: 1.61   conf_loss: 108.46   prob_loss: 0.63   total_loss: 110.70\n",
      "=> STEP 3273/6250   lr: 0.000519   giou_loss: 1.57   conf_loss: 108.41   prob_loss: 0.63   total_loss: 110.61\n",
      "=> STEP 3274/6250   lr: 0.000519   giou_loss: 1.62   conf_loss: 108.37   prob_loss: 0.63   total_loss: 110.62\n",
      "=> STEP 3275/6250   lr: 0.000519   giou_loss: 1.63   conf_loss: 108.33   prob_loss: 0.63   total_loss: 110.59\n",
      "=> STEP 3276/6250   lr: 0.000519   giou_loss: 1.65   conf_loss: 108.29   prob_loss: 0.63   total_loss: 110.57\n",
      "=> STEP 3277/6250   lr: 0.000518   giou_loss: 1.61   conf_loss: 108.25   prob_loss: 0.63   total_loss: 110.49\n",
      "=> STEP 3278/6250   lr: 0.000518   giou_loss: 1.62   conf_loss: 108.20   prob_loss: 0.63   total_loss: 110.45\n",
      "=> STEP 3279/6250   lr: 0.000518   giou_loss: 1.68   conf_loss: 108.16   prob_loss: 0.63   total_loss: 110.47\n",
      "=> STEP 3280/6250   lr: 0.000518   giou_loss: 1.60   conf_loss: 108.12   prob_loss: 0.63   total_loss: 110.35\n",
      "=> STEP 3281/6250   lr: 0.000517   giou_loss: 1.64   conf_loss: 108.08   prob_loss: 0.63   total_loss: 110.35\n",
      "=> STEP 3282/6250   lr: 0.000517   giou_loss: 1.64   conf_loss: 108.03   prob_loss: 0.63   total_loss: 110.30\n",
      "=> STEP 3283/6250   lr: 0.000517   giou_loss: 1.57   conf_loss: 108.00   prob_loss: 0.63   total_loss: 110.19\n",
      "=> STEP 3284/6250   lr: 0.000517   giou_loss: 1.71   conf_loss: 107.96   prob_loss: 0.63   total_loss: 110.29\n",
      "=> STEP 3285/6250   lr: 0.000516   giou_loss: 1.68   conf_loss: 107.91   prob_loss: 0.63   total_loss: 110.21\n",
      "=> STEP 3286/6250   lr: 0.000516   giou_loss: 1.57   conf_loss: 107.87   prob_loss: 0.63   total_loss: 110.07\n",
      "=> STEP 3287/6250   lr: 0.000516   giou_loss: 1.59   conf_loss: 107.83   prob_loss: 0.63   total_loss: 110.05\n",
      "=> STEP 3288/6250   lr: 0.000516   giou_loss: 1.60   conf_loss: 107.78   prob_loss: 0.63   total_loss: 110.01\n",
      "=> STEP 3289/6250   lr: 0.000515   giou_loss: 1.59   conf_loss: 107.74   prob_loss: 0.63   total_loss: 109.96\n",
      "=> STEP 3290/6250   lr: 0.000515   giou_loss: 1.58   conf_loss: 107.70   prob_loss: 0.63   total_loss: 109.91\n",
      "=> STEP 3291/6250   lr: 0.000515   giou_loss: 1.57   conf_loss: 107.66   prob_loss: 0.63   total_loss: 109.86\n",
      "=> STEP 3292/6250   lr: 0.000515   giou_loss: 1.60   conf_loss: 107.61   prob_loss: 0.63   total_loss: 109.85\n",
      "=> STEP 3293/6250   lr: 0.000514   giou_loss: 1.60   conf_loss: 107.58   prob_loss: 0.63   total_loss: 109.81\n",
      "=> STEP 3294/6250   lr: 0.000514   giou_loss: 1.57   conf_loss: 107.53   prob_loss: 0.63   total_loss: 109.73\n",
      "=> STEP 3295/6250   lr: 0.000514   giou_loss: 1.56   conf_loss: 107.49   prob_loss: 0.63   total_loss: 109.68\n",
      "=> STEP 3296/6250   lr: 0.000514   giou_loss: 1.56   conf_loss: 107.45   prob_loss: 0.63   total_loss: 109.64\n",
      "=> STEP 3297/6250   lr: 0.000513   giou_loss: 1.56   conf_loss: 107.41   prob_loss: 0.63   total_loss: 109.60\n",
      "=> STEP 3298/6250   lr: 0.000513   giou_loss: 1.56   conf_loss: 107.36   prob_loss: 0.63   total_loss: 109.56\n",
      "=> STEP 3299/6250   lr: 0.000513   giou_loss: 1.56   conf_loss: 107.32   prob_loss: 0.63   total_loss: 109.52\n",
      "=> STEP 3300/6250   lr: 0.000513   giou_loss: 1.56   conf_loss: 107.28   prob_loss: 0.63   total_loss: 109.47\n",
      "=> STEP 3301/6250   lr: 0.000512   giou_loss: 1.56   conf_loss: 107.24   prob_loss: 0.63   total_loss: 109.43\n",
      "=> STEP 3302/6250   lr: 0.000512   giou_loss: 1.56   conf_loss: 107.20   prob_loss: 0.63   total_loss: 109.39\n",
      "=> STEP 3303/6250   lr: 0.000512   giou_loss: 1.57   conf_loss: 107.16   prob_loss: 0.63   total_loss: 109.36\n",
      "=> STEP 3304/6250   lr: 0.000512   giou_loss: 1.57   conf_loss: 107.12   prob_loss: 0.62   total_loss: 109.31\n",
      "=> STEP 3305/6250   lr: 0.000511   giou_loss: 1.61   conf_loss: 107.08   prob_loss: 0.62   total_loss: 109.31\n",
      "=> STEP 3306/6250   lr: 0.000511   giou_loss: 1.57   conf_loss: 107.04   prob_loss: 0.62   total_loss: 109.24\n",
      "=> STEP 3307/6250   lr: 0.000511   giou_loss: 1.56   conf_loss: 107.00   prob_loss: 0.63   total_loss: 109.19\n",
      "=> STEP 3308/6250   lr: 0.000511   giou_loss: 1.68   conf_loss: 106.96   prob_loss: 0.63   total_loss: 109.27\n",
      "=> STEP 3309/6250   lr: 0.000510   giou_loss: 1.68   conf_loss: 106.92   prob_loss: 0.63   total_loss: 109.22\n",
      "=> STEP 3310/6250   lr: 0.000510   giou_loss: 1.58   conf_loss: 106.89   prob_loss: 0.62   total_loss: 109.09\n",
      "=> STEP 3311/6250   lr: 0.000510   giou_loss: 1.60   conf_loss: 106.86   prob_loss: 0.62   total_loss: 109.08\n",
      "=> STEP 3312/6250   lr: 0.000510   giou_loss: 1.66   conf_loss: 106.81   prob_loss: 0.61   total_loss: 109.09\n",
      "=> STEP 3313/6250   lr: 0.000509   giou_loss: 1.61   conf_loss: 106.77   prob_loss: 0.62   total_loss: 109.00\n",
      "=> STEP 3314/6250   lr: 0.000509   giou_loss: 1.56   conf_loss: 106.74   prob_loss: 0.62   total_loss: 108.92\n",
      "=> STEP 3315/6250   lr: 0.000509   giou_loss: 1.65   conf_loss: 106.69   prob_loss: 0.62   total_loss: 108.96\n",
      "=> STEP 3316/6250   lr: 0.000509   giou_loss: 1.66   conf_loss: 106.64   prob_loss: 0.62   total_loss: 108.93\n",
      "=> STEP 3317/6250   lr: 0.000508   giou_loss: 1.60   conf_loss: 106.61   prob_loss: 0.62   total_loss: 108.83\n",
      "=> STEP 3318/6250   lr: 0.000508   giou_loss: 1.59   conf_loss: 106.57   prob_loss: 0.62   total_loss: 108.78\n",
      "=> STEP 3319/6250   lr: 0.000508   giou_loss: 1.60   conf_loss: 106.53   prob_loss: 0.62   total_loss: 108.74\n",
      "=> STEP 3320/6250   lr: 0.000508   giou_loss: 1.64   conf_loss: 106.49   prob_loss: 0.62   total_loss: 108.74\n",
      "=> STEP 3321/6250   lr: 0.000507   giou_loss: 1.57   conf_loss: 106.45   prob_loss: 0.62   total_loss: 108.65\n",
      "=> STEP 3322/6250   lr: 0.000507   giou_loss: 1.57   conf_loss: 106.40   prob_loss: 0.62   total_loss: 108.59\n",
      "=> STEP 3323/6250   lr: 0.000507   giou_loss: 1.56   conf_loss: 106.36   prob_loss: 0.62   total_loss: 108.54\n",
      "=> STEP 3324/6250   lr: 0.000507   giou_loss: 1.56   conf_loss: 106.32   prob_loss: 0.62   total_loss: 108.51\n",
      "=> STEP 3325/6250   lr: 0.000506   giou_loss: 1.56   conf_loss: 106.28   prob_loss: 0.62   total_loss: 108.47\n",
      "=> STEP 3326/6250   lr: 0.000506   giou_loss: 1.57   conf_loss: 106.24   prob_loss: 0.62   total_loss: 108.43\n",
      "=> STEP 3327/6250   lr: 0.000506   giou_loss: 1.64   conf_loss: 106.20   prob_loss: 0.62   total_loss: 108.46\n",
      "=> STEP 3328/6250   lr: 0.000506   giou_loss: 1.58   conf_loss: 106.16   prob_loss: 0.62   total_loss: 108.36\n",
      "=> STEP 3329/6250   lr: 0.000505   giou_loss: 1.61   conf_loss: 106.12   prob_loss: 0.62   total_loss: 108.35\n",
      "=> STEP 3330/6250   lr: 0.000505   giou_loss: 1.62   conf_loss: 106.08   prob_loss: 0.62   total_loss: 108.32\n",
      "=> STEP 3331/6250   lr: 0.000505   giou_loss: 1.58   conf_loss: 106.04   prob_loss: 0.62   total_loss: 108.24\n",
      "=> STEP 3332/6250   lr: 0.000505   giou_loss: 1.64   conf_loss: 106.00   prob_loss: 0.63   total_loss: 108.27\n",
      "=> STEP 3333/6250   lr: 0.000504   giou_loss: 1.63   conf_loss: 105.96   prob_loss: 0.62   total_loss: 108.21\n",
      "=> STEP 3334/6250   lr: 0.000504   giou_loss: 1.57   conf_loss: 105.92   prob_loss: 0.62   total_loss: 108.11\n",
      "=> STEP 3335/6250   lr: 0.000504   giou_loss: 1.67   conf_loss: 105.88   prob_loss: 0.62   total_loss: 108.17\n",
      "=> STEP 3336/6250   lr: 0.000504   giou_loss: 1.67   conf_loss: 105.85   prob_loss: 0.62   total_loss: 108.13\n",
      "=> STEP 3337/6250   lr: 0.000503   giou_loss: 1.60   conf_loss: 105.80   prob_loss: 0.62   total_loss: 108.02\n",
      "=> STEP 3338/6250   lr: 0.000503   giou_loss: 1.59   conf_loss: 105.76   prob_loss: 0.62   total_loss: 107.97\n",
      "=> STEP 3339/6250   lr: 0.000503   giou_loss: 1.57   conf_loss: 105.72   prob_loss: 0.62   total_loss: 107.92\n",
      "=> STEP 3340/6250   lr: 0.000503   giou_loss: 1.61   conf_loss: 105.68   prob_loss: 0.62   total_loss: 107.91\n",
      "=> STEP 3341/6250   lr: 0.000502   giou_loss: 1.62   conf_loss: 105.64   prob_loss: 0.62   total_loss: 107.88\n",
      "=> STEP 3342/6250   lr: 0.000502   giou_loss: 1.56   conf_loss: 105.60   prob_loss: 0.62   total_loss: 107.79\n",
      "=> STEP 3343/6250   lr: 0.000502   giou_loss: 1.62   conf_loss: 105.56   prob_loss: 0.62   total_loss: 107.81\n",
      "=> STEP 3344/6250   lr: 0.000502   giou_loss: 1.63   conf_loss: 105.52   prob_loss: 0.63   total_loss: 107.78\n",
      "=> STEP 3345/6250   lr: 0.000501   giou_loss: 1.61   conf_loss: 105.48   prob_loss: 0.62   total_loss: 107.72\n",
      "=> STEP 3346/6250   lr: 0.000501   giou_loss: 1.60   conf_loss: 105.45   prob_loss: 0.62   total_loss: 107.67\n",
      "=> STEP 3347/6250   lr: 0.000501   giou_loss: 1.71   conf_loss: 105.41   prob_loss: 0.62   total_loss: 107.74\n",
      "=> STEP 3348/6250   lr: 0.000501   giou_loss: 1.72   conf_loss: 105.36   prob_loss: 0.62   total_loss: 107.70\n",
      "=> STEP 3349/6250   lr: 0.000500   giou_loss: 1.65   conf_loss: 105.33   prob_loss: 0.62   total_loss: 107.60\n",
      "=> STEP 3350/6250   lr: 0.000500   giou_loss: 1.60   conf_loss: 105.30   prob_loss: 0.62   total_loss: 107.52\n",
      "=> STEP 3351/6250   lr: 0.000500   giou_loss: 1.68   conf_loss: 105.24   prob_loss: 0.63   total_loss: 107.55\n",
      "=> STEP 3352/6250   lr: 0.000500   giou_loss: 1.79   conf_loss: 105.21   prob_loss: 0.63   total_loss: 107.62\n",
      "=> STEP 3353/6250   lr: 0.000499   giou_loss: 1.73   conf_loss: 105.18   prob_loss: 0.62   total_loss: 107.54\n",
      "=> STEP 3354/6250   lr: 0.000499   giou_loss: 1.58   conf_loss: 105.15   prob_loss: 0.62   total_loss: 107.34\n",
      "=> STEP 3355/6250   lr: 0.000499   giou_loss: 1.70   conf_loss: 105.11   prob_loss: 0.61   total_loss: 107.42\n",
      "=> STEP 3356/6250   lr: 0.000499   giou_loss: 1.77   conf_loss: 105.07   prob_loss: 0.61   total_loss: 107.45\n",
      "=> STEP 3357/6250   lr: 0.000498   giou_loss: 1.71   conf_loss: 105.04   prob_loss: 0.61   total_loss: 107.36\n",
      "=> STEP 3358/6250   lr: 0.000498   giou_loss: 1.59   conf_loss: 104.98   prob_loss: 0.62   total_loss: 107.19\n",
      "=> STEP 3359/6250   lr: 0.000498   giou_loss: 1.70   conf_loss: 104.93   prob_loss: 0.63   total_loss: 107.26\n",
      "=> STEP 3360/6250   lr: 0.000498   giou_loss: 1.78   conf_loss: 104.90   prob_loss: 0.63   total_loss: 107.30\n",
      "=> STEP 3361/6250   lr: 0.000497   giou_loss: 1.77   conf_loss: 104.86   prob_loss: 0.63   total_loss: 107.26\n",
      "=> STEP 3362/6250   lr: 0.000497   giou_loss: 1.69   conf_loss: 104.82   prob_loss: 0.62   total_loss: 107.13\n",
      "=> STEP 3363/6250   lr: 0.000497   giou_loss: 1.56   conf_loss: 104.78   prob_loss: 0.62   total_loss: 106.97\n",
      "=> STEP 3364/6250   lr: 0.000497   giou_loss: 1.71   conf_loss: 104.75   prob_loss: 0.62   total_loss: 107.08\n",
      "=> STEP 3365/6250   lr: 0.000496   giou_loss: 1.76   conf_loss: 104.71   prob_loss: 0.62   total_loss: 107.08\n",
      "=> STEP 3366/6250   lr: 0.000496   giou_loss: 1.71   conf_loss: 104.67   prob_loss: 0.62   total_loss: 106.99\n",
      "=> STEP 3367/6250   lr: 0.000496   giou_loss: 1.57   conf_loss: 104.62   prob_loss: 0.62   total_loss: 106.81\n",
      "=> STEP 3368/6250   lr: 0.000496   giou_loss: 1.64   conf_loss: 104.58   prob_loss: 0.62   total_loss: 106.84\n",
      "=> STEP 3369/6250   lr: 0.000495   giou_loss: 1.65   conf_loss: 104.54   prob_loss: 0.62   total_loss: 106.81\n",
      "=> STEP 3370/6250   lr: 0.000495   giou_loss: 1.57   conf_loss: 104.50   prob_loss: 0.62   total_loss: 106.69\n",
      "=> STEP 3371/6250   lr: 0.000495   giou_loss: 1.58   conf_loss: 104.46   prob_loss: 0.62   total_loss: 106.66\n",
      "=> STEP 3372/6250   lr: 0.000495   giou_loss: 1.57   conf_loss: 104.42   prob_loss: 0.62   total_loss: 106.61\n",
      "=> STEP 3373/6250   lr: 0.000494   giou_loss: 1.57   conf_loss: 104.38   prob_loss: 0.62   total_loss: 106.57\n",
      "=> STEP 3374/6250   lr: 0.000494   giou_loss: 1.57   conf_loss: 104.34   prob_loss: 0.62   total_loss: 106.53\n",
      "=> STEP 3375/6250   lr: 0.000494   giou_loss: 1.57   conf_loss: 104.30   prob_loss: 0.62   total_loss: 106.50\n",
      "=> STEP 3376/6250   lr: 0.000494   giou_loss: 1.56   conf_loss: 104.26   prob_loss: 0.62   total_loss: 106.45\n",
      "=> STEP 3377/6250   lr: 0.000493   giou_loss: 1.56   conf_loss: 104.22   prob_loss: 0.62   total_loss: 106.41\n",
      "=> STEP 3378/6250   lr: 0.000493   giou_loss: 1.57   conf_loss: 104.19   prob_loss: 0.62   total_loss: 106.37\n",
      "=> STEP 3379/6250   lr: 0.000493   giou_loss: 1.57   conf_loss: 104.15   prob_loss: 0.62   total_loss: 106.34\n",
      "=> STEP 3380/6250   lr: 0.000493   giou_loss: 1.56   conf_loss: 104.11   prob_loss: 0.62   total_loss: 106.29\n",
      "=> STEP 3381/6250   lr: 0.000492   giou_loss: 1.56   conf_loss: 104.07   prob_loss: 0.62   total_loss: 106.25\n",
      "=> STEP 3382/6250   lr: 0.000492   giou_loss: 1.57   conf_loss: 104.03   prob_loss: 0.62   total_loss: 106.21\n",
      "=> STEP 3383/6250   lr: 0.000492   giou_loss: 1.60   conf_loss: 103.99   prob_loss: 0.62   total_loss: 106.20\n",
      "=> STEP 3384/6250   lr: 0.000492   giou_loss: 1.60   conf_loss: 103.95   prob_loss: 0.62   total_loss: 106.17\n",
      "=> STEP 3385/6250   lr: 0.000491   giou_loss: 1.63   conf_loss: 103.92   prob_loss: 0.62   total_loss: 106.16\n",
      "=> STEP 3386/6250   lr: 0.000491   giou_loss: 1.59   conf_loss: 103.88   prob_loss: 0.62   total_loss: 106.09\n",
      "=> STEP 3387/6250   lr: 0.000491   giou_loss: 1.60   conf_loss: 103.84   prob_loss: 0.62   total_loss: 106.06\n",
      "=> STEP 3388/6250   lr: 0.000491   giou_loss: 1.62   conf_loss: 103.79   prob_loss: 0.62   total_loss: 106.03\n",
      "=> STEP 3389/6250   lr: 0.000490   giou_loss: 1.56   conf_loss: 103.76   prob_loss: 0.62   total_loss: 105.94\n",
      "=> STEP 3390/6250   lr: 0.000490   giou_loss: 1.64   conf_loss: 103.72   prob_loss: 0.62   total_loss: 105.98\n",
      "=> STEP 3391/6250   lr: 0.000490   giou_loss: 1.57   conf_loss: 103.68   prob_loss: 0.62   total_loss: 105.87\n",
      "=> STEP 3392/6250   lr: 0.000490   giou_loss: 1.63   conf_loss: 103.64   prob_loss: 0.62   total_loss: 105.89\n",
      "=> STEP 3393/6250   lr: 0.000489   giou_loss: 1.65   conf_loss: 103.61   prob_loss: 0.62   total_loss: 105.87\n",
      "=> STEP 3394/6250   lr: 0.000489   giou_loss: 1.56   conf_loss: 103.57   prob_loss: 0.62   total_loss: 105.75\n",
      "=> STEP 3395/6250   lr: 0.000489   giou_loss: 1.59   conf_loss: 103.53   prob_loss: 0.62   total_loss: 105.73\n",
      "=> STEP 3396/6250   lr: 0.000489   giou_loss: 1.56   conf_loss: 103.49   prob_loss: 0.62   total_loss: 105.67\n",
      "=> STEP 3397/6250   lr: 0.000488   giou_loss: 1.58   conf_loss: 103.45   prob_loss: 0.62   total_loss: 105.65\n",
      "=> STEP 3398/6250   lr: 0.000488   giou_loss: 1.59   conf_loss: 103.41   prob_loss: 0.62   total_loss: 105.62\n",
      "=> STEP 3399/6250   lr: 0.000488   giou_loss: 1.56   conf_loss: 103.38   prob_loss: 0.62   total_loss: 105.56\n",
      "=> STEP 3400/6250   lr: 0.000488   giou_loss: 1.65   conf_loss: 103.34   prob_loss: 0.62   total_loss: 105.61\n",
      "=> STEP 3401/6250   lr: 0.000487   giou_loss: 1.59   conf_loss: 103.30   prob_loss: 0.62   total_loss: 105.51\n",
      "=> STEP 3402/6250   lr: 0.000487   giou_loss: 1.62   conf_loss: 103.26   prob_loss: 0.62   total_loss: 105.50\n",
      "=> STEP 3403/6250   lr: 0.000487   giou_loss: 1.68   conf_loss: 103.23   prob_loss: 0.62   total_loss: 105.52\n",
      "=> STEP 3404/6250   lr: 0.000487   giou_loss: 1.57   conf_loss: 103.19   prob_loss: 0.62   total_loss: 105.37\n",
      "=> STEP 3405/6250   lr: 0.000486   giou_loss: 1.57   conf_loss: 103.15   prob_loss: 0.62   total_loss: 105.34\n",
      "=> STEP 3406/6250   lr: 0.000486   giou_loss: 1.57   conf_loss: 103.12   prob_loss: 0.62   total_loss: 105.30\n",
      "=> STEP 3407/6250   lr: 0.000486   giou_loss: 1.57   conf_loss: 103.08   prob_loss: 0.62   total_loss: 105.26\n",
      "=> STEP 3408/6250   lr: 0.000486   giou_loss: 1.57   conf_loss: 103.04   prob_loss: 0.61   total_loss: 105.22\n",
      "=> STEP 3409/6250   lr: 0.000486   giou_loss: 1.64   conf_loss: 103.01   prob_loss: 0.61   total_loss: 105.26\n",
      "=> STEP 3410/6250   lr: 0.000485   giou_loss: 1.58   conf_loss: 102.97   prob_loss: 0.61   total_loss: 105.16\n",
      "=> STEP 3411/6250   lr: 0.000485   giou_loss: 1.63   conf_loss: 102.93   prob_loss: 0.62   total_loss: 105.17\n",
      "=> STEP 3412/6250   lr: 0.000485   giou_loss: 1.65   conf_loss: 102.89   prob_loss: 0.62   total_loss: 105.16\n",
      "=> STEP 3413/6250   lr: 0.000485   giou_loss: 1.62   conf_loss: 102.85   prob_loss: 0.62   total_loss: 105.09\n",
      "=> STEP 3414/6250   lr: 0.000484   giou_loss: 1.65   conf_loss: 102.82   prob_loss: 0.62   total_loss: 105.09\n",
      "=> STEP 3415/6250   lr: 0.000484   giou_loss: 1.70   conf_loss: 102.79   prob_loss: 0.61   total_loss: 105.11\n",
      "=> STEP 3416/6250   lr: 0.000484   giou_loss: 1.68   conf_loss: 102.75   prob_loss: 0.61   total_loss: 105.04\n",
      "=> STEP 3417/6250   lr: 0.000484   giou_loss: 1.57   conf_loss: 102.71   prob_loss: 0.62   total_loss: 104.89\n",
      "=> STEP 3418/6250   lr: 0.000483   giou_loss: 1.59   conf_loss: 102.67   prob_loss: 0.62   total_loss: 104.88\n",
      "=> STEP 3419/6250   lr: 0.000483   giou_loss: 1.56   conf_loss: 102.63   prob_loss: 0.61   total_loss: 104.81\n",
      "=> STEP 3420/6250   lr: 0.000483   giou_loss: 1.67   conf_loss: 102.60   prob_loss: 0.61   total_loss: 104.88\n",
      "=> STEP 3421/6250   lr: 0.000483   giou_loss: 1.59   conf_loss: 102.57   prob_loss: 0.61   total_loss: 104.77\n",
      "=> STEP 3422/6250   lr: 0.000482   giou_loss: 1.60   conf_loss: 102.53   prob_loss: 0.61   total_loss: 104.74\n",
      "=> STEP 3423/6250   lr: 0.000482   giou_loss: 1.63   conf_loss: 102.49   prob_loss: 0.61   total_loss: 104.72\n",
      "=> STEP 3424/6250   lr: 0.000482   giou_loss: 1.58   conf_loss: 102.45   prob_loss: 0.61   total_loss: 104.64\n",
      "=> STEP 3425/6250   lr: 0.000482   giou_loss: 1.60   conf_loss: 102.42   prob_loss: 0.61   total_loss: 104.63\n",
      "=> STEP 3426/6250   lr: 0.000481   giou_loss: 1.58   conf_loss: 102.38   prob_loss: 0.61   total_loss: 104.57\n",
      "=> STEP 3427/6250   lr: 0.000481   giou_loss: 1.63   conf_loss: 102.33   prob_loss: 0.61   total_loss: 104.57\n",
      "=> STEP 3428/6250   lr: 0.000481   giou_loss: 1.65   conf_loss: 102.30   prob_loss: 0.61   total_loss: 104.56\n",
      "=> STEP 3429/6250   lr: 0.000481   giou_loss: 1.57   conf_loss: 102.26   prob_loss: 0.61   total_loss: 104.45\n",
      "=> STEP 3430/6250   lr: 0.000480   giou_loss: 1.66   conf_loss: 102.23   prob_loss: 0.61   total_loss: 104.50\n",
      "=> STEP 3431/6250   lr: 0.000480   giou_loss: 1.72   conf_loss: 102.19   prob_loss: 0.61   total_loss: 104.52\n",
      "=> STEP 3432/6250   lr: 0.000480   giou_loss: 1.67   conf_loss: 102.16   prob_loss: 0.61   total_loss: 104.44\n",
      "=> STEP 3433/6250   lr: 0.000480   giou_loss: 1.56   conf_loss: 102.12   prob_loss: 0.61   total_loss: 104.29\n",
      "=> STEP 3434/6250   lr: 0.000479   giou_loss: 1.67   conf_loss: 102.08   prob_loss: 0.62   total_loss: 104.37\n",
      "=> STEP 3435/6250   lr: 0.000479   giou_loss: 1.65   conf_loss: 102.04   prob_loss: 0.62   total_loss: 104.30\n",
      "=> STEP 3436/6250   lr: 0.000479   giou_loss: 1.56   conf_loss: 102.01   prob_loss: 0.61   total_loss: 104.19\n",
      "=> STEP 3437/6250   lr: 0.000479   giou_loss: 1.57   conf_loss: 101.97   prob_loss: 0.61   total_loss: 104.16\n",
      "=> STEP 3438/6250   lr: 0.000478   giou_loss: 1.56   conf_loss: 101.93   prob_loss: 0.61   total_loss: 104.11\n",
      "=> STEP 3439/6250   lr: 0.000478   giou_loss: 1.57   conf_loss: 101.90   prob_loss: 0.61   total_loss: 104.07\n",
      "=> STEP 3440/6250   lr: 0.000478   giou_loss: 1.58   conf_loss: 101.86   prob_loss: 0.61   total_loss: 104.06\n",
      "=> STEP 3441/6250   lr: 0.000478   giou_loss: 1.56   conf_loss: 101.82   prob_loss: 0.61   total_loss: 103.99\n",
      "=> STEP 3442/6250   lr: 0.000477   giou_loss: 1.63   conf_loss: 101.78   prob_loss: 0.61   total_loss: 104.02\n",
      "=> STEP 3443/6250   lr: 0.000477   giou_loss: 1.59   conf_loss: 101.75   prob_loss: 0.61   total_loss: 103.95\n",
      "=> STEP 3444/6250   lr: 0.000477   giou_loss: 1.61   conf_loss: 101.71   prob_loss: 0.61   total_loss: 103.93\n",
      "=> STEP 3445/6250   lr: 0.000477   giou_loss: 1.63   conf_loss: 101.67   prob_loss: 0.61   total_loss: 103.91\n",
      "=> STEP 3446/6250   lr: 0.000476   giou_loss: 1.57   conf_loss: 101.63   prob_loss: 0.61   total_loss: 103.81\n",
      "=> STEP 3447/6250   lr: 0.000476   giou_loss: 1.58   conf_loss: 101.60   prob_loss: 0.61   total_loss: 103.79\n",
      "=> STEP 3448/6250   lr: 0.000476   giou_loss: 1.57   conf_loss: 101.56   prob_loss: 0.61   total_loss: 103.73\n",
      "=> STEP 3449/6250   lr: 0.000476   giou_loss: 1.57   conf_loss: 101.52   prob_loss: 0.61   total_loss: 103.70\n",
      "=> STEP 3450/6250   lr: 0.000475   giou_loss: 1.58   conf_loss: 101.48   prob_loss: 0.61   total_loss: 103.67\n",
      "=> STEP 3451/6250   lr: 0.000475   giou_loss: 1.56   conf_loss: 101.45   prob_loss: 0.61   total_loss: 103.62\n",
      "=> STEP 3452/6250   lr: 0.000475   giou_loss: 1.63   conf_loss: 101.41   prob_loss: 0.61   total_loss: 103.65\n",
      "=> STEP 3453/6250   lr: 0.000475   giou_loss: 1.59   conf_loss: 101.37   prob_loss: 0.61   total_loss: 103.57\n",
      "=> STEP 3454/6250   lr: 0.000474   giou_loss: 1.61   conf_loss: 101.34   prob_loss: 0.61   total_loss: 103.56\n",
      "=> STEP 3455/6250   lr: 0.000474   giou_loss: 1.63   conf_loss: 101.30   prob_loss: 0.61   total_loss: 103.54\n",
      "=> STEP 3456/6250   lr: 0.000474   giou_loss: 1.56   conf_loss: 101.27   prob_loss: 0.61   total_loss: 103.44\n",
      "=> STEP 3457/6250   lr: 0.000474   giou_loss: 1.60   conf_loss: 101.23   prob_loss: 0.61   total_loss: 103.44\n",
      "=> STEP 3458/6250   lr: 0.000473   giou_loss: 1.56   conf_loss: 101.19   prob_loss: 0.61   total_loss: 103.37\n",
      "=> STEP 3459/6250   lr: 0.000473   giou_loss: 1.60   conf_loss: 101.16   prob_loss: 0.61   total_loss: 103.36\n",
      "=> STEP 3460/6250   lr: 0.000473   giou_loss: 1.59   conf_loss: 101.12   prob_loss: 0.61   total_loss: 103.32\n",
      "=> STEP 3461/6250   lr: 0.000473   giou_loss: 1.56   conf_loss: 101.09   prob_loss: 0.61   total_loss: 103.26\n",
      "=> STEP 3462/6250   lr: 0.000472   giou_loss: 1.63   conf_loss: 101.06   prob_loss: 0.61   total_loss: 103.30\n",
      "=> STEP 3463/6250   lr: 0.000472   giou_loss: 1.59   conf_loss: 101.02   prob_loss: 0.61   total_loss: 103.22\n",
      "=> STEP 3464/6250   lr: 0.000472   giou_loss: 1.60   conf_loss: 100.98   prob_loss: 0.61   total_loss: 103.20\n",
      "=> STEP 3465/6250   lr: 0.000472   giou_loss: 1.63   conf_loss: 100.95   prob_loss: 0.61   total_loss: 103.18\n",
      "=> STEP 3466/6250   lr: 0.000471   giou_loss: 1.56   conf_loss: 100.91   prob_loss: 0.61   total_loss: 103.09\n",
      "=> STEP 3467/6250   lr: 0.000471   giou_loss: 1.58   conf_loss: 100.88   prob_loss: 0.61   total_loss: 103.06\n",
      "=> STEP 3468/6250   lr: 0.000471   giou_loss: 1.62   conf_loss: 100.84   prob_loss: 0.61   total_loss: 103.06\n",
      "=> STEP 3469/6250   lr: 0.000471   giou_loss: 1.57   conf_loss: 100.81   prob_loss: 0.61   total_loss: 102.98\n",
      "=> STEP 3470/6250   lr: 0.000470   giou_loss: 1.59   conf_loss: 100.77   prob_loss: 0.61   total_loss: 102.96\n",
      "=> STEP 3471/6250   lr: 0.000470   giou_loss: 1.56   conf_loss: 100.73   prob_loss: 0.61   total_loss: 102.90\n",
      "=> STEP 3472/6250   lr: 0.000470   giou_loss: 1.64   conf_loss: 100.70   prob_loss: 0.61   total_loss: 102.94\n",
      "=> STEP 3473/6250   lr: 0.000470   giou_loss: 1.58   conf_loss: 100.66   prob_loss: 0.61   total_loss: 102.85\n",
      "=> STEP 3474/6250   lr: 0.000469   giou_loss: 1.63   conf_loss: 100.62   prob_loss: 0.61   total_loss: 102.86\n",
      "=> STEP 3475/6250   lr: 0.000469   giou_loss: 1.70   conf_loss: 100.59   prob_loss: 0.61   total_loss: 102.89\n",
      "=> STEP 3476/6250   lr: 0.000469   giou_loss: 1.58   conf_loss: 100.55   prob_loss: 0.61   total_loss: 102.74\n",
      "=> STEP 3477/6250   lr: 0.000469   giou_loss: 1.65   conf_loss: 100.52   prob_loss: 0.61   total_loss: 102.78\n",
      "=> STEP 3478/6250   lr: 0.000468   giou_loss: 1.72   conf_loss: 100.48   prob_loss: 0.61   total_loss: 102.81\n",
      "=> STEP 3479/6250   lr: 0.000468   giou_loss: 1.66   conf_loss: 100.44   prob_loss: 0.61   total_loss: 102.71\n",
      "=> STEP 3480/6250   lr: 0.000468   giou_loss: 1.57   conf_loss: 100.41   prob_loss: 0.61   total_loss: 102.58\n",
      "=> STEP 3481/6250   lr: 0.000468   giou_loss: 1.70   conf_loss: 100.37   prob_loss: 0.61   total_loss: 102.68\n",
      "=> STEP 3482/6250   lr: 0.000467   giou_loss: 1.69   conf_loss: 100.33   prob_loss: 0.61   total_loss: 102.64\n",
      "=> STEP 3483/6250   lr: 0.000467   giou_loss: 1.61   conf_loss: 100.30   prob_loss: 0.61   total_loss: 102.52\n",
      "=> STEP 3484/6250   lr: 0.000467   giou_loss: 1.67   conf_loss: 100.27   prob_loss: 0.61   total_loss: 102.55\n",
      "=> STEP 3485/6250   lr: 0.000467   giou_loss: 1.68   conf_loss: 100.23   prob_loss: 0.61   total_loss: 102.52\n",
      "=> STEP 3486/6250   lr: 0.000466   giou_loss: 1.63   conf_loss: 100.20   prob_loss: 0.61   total_loss: 102.44\n",
      "=> STEP 3487/6250   lr: 0.000466   giou_loss: 1.57   conf_loss: 100.16   prob_loss: 0.61   total_loss: 102.34\n",
      "=> STEP 3488/6250   lr: 0.000466   giou_loss: 1.60   conf_loss: 100.12   prob_loss: 0.61   total_loss: 102.33\n",
      "=> STEP 3489/6250   lr: 0.000466   giou_loss: 1.56   conf_loss: 100.09   prob_loss: 0.61   total_loss: 102.26\n",
      "=> STEP 3490/6250   lr: 0.000465   giou_loss: 1.65   conf_loss: 100.06   prob_loss: 0.61   total_loss: 102.31\n",
      "=> STEP 3491/6250   lr: 0.000465   giou_loss: 1.59   conf_loss: 100.02   prob_loss: 0.61   total_loss: 102.21\n",
      "=> STEP 3492/6250   lr: 0.000465   giou_loss: 1.60   conf_loss: 99.98   prob_loss: 0.61   total_loss: 102.19\n",
      "=> STEP 3493/6250   lr: 0.000465   giou_loss: 1.62   conf_loss: 99.95   prob_loss: 0.61   total_loss: 102.18\n",
      "=> STEP 3494/6250   lr: 0.000464   giou_loss: 1.57   conf_loss: 99.91   prob_loss: 0.61   total_loss: 102.08\n",
      "=> STEP 3495/6250   lr: 0.000464   giou_loss: 1.59   conf_loss: 99.88   prob_loss: 0.60   total_loss: 102.07\n",
      "=> STEP 3496/6250   lr: 0.000464   giou_loss: 1.57   conf_loss: 99.84   prob_loss: 0.60   total_loss: 102.01\n",
      "=> STEP 3497/6250   lr: 0.000464   giou_loss: 1.57   conf_loss: 99.81   prob_loss: 0.60   total_loss: 101.99\n",
      "=> STEP 3498/6250   lr: 0.000463   giou_loss: 1.56   conf_loss: 99.77   prob_loss: 0.61   total_loss: 101.94\n",
      "=> STEP 3499/6250   lr: 0.000463   giou_loss: 1.61   conf_loss: 99.73   prob_loss: 0.61   total_loss: 101.95\n",
      "=> STEP 3500/6250   lr: 0.000463   giou_loss: 1.58   conf_loss: 99.70   prob_loss: 0.61   total_loss: 101.89\n",
      "=> STEP 3501/6250   lr: 0.000463   giou_loss: 1.63   conf_loss: 99.67   prob_loss: 0.60   total_loss: 101.90\n",
      "=> STEP 3502/6250   lr: 0.000463   giou_loss: 1.66   conf_loss: 99.64   prob_loss: 0.60   total_loss: 101.90\n",
      "=> STEP 3503/6250   lr: 0.000462   giou_loss: 1.70   conf_loss: 99.60   prob_loss: 0.60   total_loss: 101.90\n",
      "=> STEP 3504/6250   lr: 0.000462   giou_loss: 1.63   conf_loss: 99.57   prob_loss: 0.61   total_loss: 101.81\n",
      "=> STEP 3505/6250   lr: 0.000462   giou_loss: 1.56   conf_loss: 99.53   prob_loss: 0.61   total_loss: 101.70\n",
      "=> STEP 3506/6250   lr: 0.000462   giou_loss: 1.64   conf_loss: 99.49   prob_loss: 0.61   total_loss: 101.74\n",
      "=> STEP 3507/6250   lr: 0.000461   giou_loss: 1.58   conf_loss: 99.46   prob_loss: 0.61   total_loss: 101.65\n",
      "=> STEP 3508/6250   lr: 0.000461   giou_loss: 1.57   conf_loss: 99.44   prob_loss: 0.61   total_loss: 101.61\n",
      "=> STEP 3509/6250   lr: 0.000461   giou_loss: 1.69   conf_loss: 99.40   prob_loss: 0.60   total_loss: 101.69\n",
      "=> STEP 3510/6250   lr: 0.000461   giou_loss: 1.66   conf_loss: 99.36   prob_loss: 0.60   total_loss: 101.62\n",
      "=> STEP 3511/6250   lr: 0.000460   giou_loss: 1.61   conf_loss: 99.33   prob_loss: 0.60   total_loss: 101.54\n",
      "=> STEP 3512/6250   lr: 0.000460   giou_loss: 1.56   conf_loss: 99.30   prob_loss: 0.61   total_loss: 101.47\n",
      "=> STEP 3513/6250   lr: 0.000460   giou_loss: 1.58   conf_loss: 99.25   prob_loss: 0.61   total_loss: 101.44\n",
      "=> STEP 3514/6250   lr: 0.000460   giou_loss: 1.56   conf_loss: 99.22   prob_loss: 0.61   total_loss: 101.39\n",
      "=> STEP 3515/6250   lr: 0.000459   giou_loss: 1.60   conf_loss: 99.19   prob_loss: 0.60   total_loss: 101.39\n",
      "=> STEP 3516/6250   lr: 0.000459   giou_loss: 1.56   conf_loss: 99.16   prob_loss: 0.60   total_loss: 101.32\n",
      "=> STEP 3517/6250   lr: 0.000459   giou_loss: 1.60   conf_loss: 99.12   prob_loss: 0.60   total_loss: 101.32\n",
      "=> STEP 3518/6250   lr: 0.000459   giou_loss: 1.57   conf_loss: 99.09   prob_loss: 0.60   total_loss: 101.25\n",
      "=> STEP 3519/6250   lr: 0.000458   giou_loss: 1.57   conf_loss: 99.05   prob_loss: 0.60   total_loss: 101.22\n",
      "=> STEP 3520/6250   lr: 0.000458   giou_loss: 1.58   conf_loss: 99.01   prob_loss: 0.60   total_loss: 101.20\n",
      "=> STEP 3521/6250   lr: 0.000458   giou_loss: 1.57   conf_loss: 98.98   prob_loss: 0.60   total_loss: 101.14\n",
      "=> STEP 3522/6250   lr: 0.000458   giou_loss: 1.62   conf_loss: 98.94   prob_loss: 0.60   total_loss: 101.17\n",
      "=> STEP 3523/6250   lr: 0.000457   giou_loss: 1.58   conf_loss: 98.91   prob_loss: 0.60   total_loss: 101.09\n",
      "=> STEP 3524/6250   lr: 0.000457   giou_loss: 1.63   conf_loss: 98.87   prob_loss: 0.60   total_loss: 101.11\n",
      "=> STEP 3525/6250   lr: 0.000457   giou_loss: 1.65   conf_loss: 98.84   prob_loss: 0.61   total_loss: 101.09\n",
      "=> STEP 3526/6250   lr: 0.000457   giou_loss: 1.62   conf_loss: 98.80   prob_loss: 0.61   total_loss: 101.03\n",
      "=> STEP 3527/6250   lr: 0.000456   giou_loss: 1.61   conf_loss: 98.77   prob_loss: 0.60   total_loss: 100.98\n",
      "=> STEP 3528/6250   lr: 0.000456   giou_loss: 1.61   conf_loss: 98.74   prob_loss: 0.60   total_loss: 100.95\n",
      "=> STEP 3529/6250   lr: 0.000456   giou_loss: 1.61   conf_loss: 98.71   prob_loss: 0.60   total_loss: 100.92\n",
      "=> STEP 3530/6250   lr: 0.000456   giou_loss: 1.61   conf_loss: 98.67   prob_loss: 0.60   total_loss: 100.88\n",
      "=> STEP 3531/6250   lr: 0.000455   giou_loss: 1.58   conf_loss: 98.64   prob_loss: 0.60   total_loss: 100.81\n",
      "=> STEP 3532/6250   lr: 0.000455   giou_loss: 1.60   conf_loss: 98.61   prob_loss: 0.60   total_loss: 100.81\n",
      "=> STEP 3533/6250   lr: 0.000455   giou_loss: 1.64   conf_loss: 98.57   prob_loss: 0.60   total_loss: 100.82\n",
      "=> STEP 3534/6250   lr: 0.000455   giou_loss: 1.57   conf_loss: 98.54   prob_loss: 0.60   total_loss: 100.71\n",
      "=> STEP 3535/6250   lr: 0.000454   giou_loss: 1.71   conf_loss: 98.51   prob_loss: 0.60   total_loss: 100.83\n",
      "=> STEP 3536/6250   lr: 0.000454   giou_loss: 1.74   conf_loss: 98.47   prob_loss: 0.60   total_loss: 100.81\n",
      "=> STEP 3537/6250   lr: 0.000454   giou_loss: 1.61   conf_loss: 98.43   prob_loss: 0.60   total_loss: 100.65\n",
      "=> STEP 3538/6250   lr: 0.000454   giou_loss: 1.61   conf_loss: 98.41   prob_loss: 0.60   total_loss: 100.62\n",
      "=> STEP 3539/6250   lr: 0.000453   giou_loss: 1.64   conf_loss: 98.37   prob_loss: 0.60   total_loss: 100.61\n",
      "=> STEP 3540/6250   lr: 0.000453   giou_loss: 1.57   conf_loss: 98.33   prob_loss: 0.60   total_loss: 100.50\n",
      "=> STEP 3541/6250   lr: 0.000453   giou_loss: 1.65   conf_loss: 98.30   prob_loss: 0.60   total_loss: 100.55\n",
      "=> STEP 3542/6250   lr: 0.000453   giou_loss: 1.70   conf_loss: 98.26   prob_loss: 0.60   total_loss: 100.57\n",
      "=> STEP 3543/6250   lr: 0.000452   giou_loss: 1.66   conf_loss: 98.23   prob_loss: 0.60   total_loss: 100.49\n",
      "=> STEP 3544/6250   lr: 0.000452   giou_loss: 1.56   conf_loss: 98.20   prob_loss: 0.60   total_loss: 100.36\n",
      "=> STEP 3545/6250   lr: 0.000452   giou_loss: 1.67   conf_loss: 98.17   prob_loss: 0.60   total_loss: 100.43\n",
      "=> STEP 3546/6250   lr: 0.000452   giou_loss: 1.68   conf_loss: 98.13   prob_loss: 0.60   total_loss: 100.41\n",
      "=> STEP 3547/6250   lr: 0.000451   giou_loss: 1.60   conf_loss: 98.09   prob_loss: 0.60   total_loss: 100.29\n",
      "=> STEP 3548/6250   lr: 0.000451   giou_loss: 1.64   conf_loss: 98.05   prob_loss: 0.60   total_loss: 100.30\n",
      "=> STEP 3549/6250   lr: 0.000451   giou_loss: 1.70   conf_loss: 98.02   prob_loss: 0.60   total_loss: 100.32\n",
      "=> STEP 3550/6250   lr: 0.000451   giou_loss: 1.66   conf_loss: 97.99   prob_loss: 0.60   total_loss: 100.25\n",
      "=> STEP 3551/6250   lr: 0.000450   giou_loss: 1.56   conf_loss: 97.96   prob_loss: 0.60   total_loss: 100.12\n",
      "=> STEP 3552/6250   lr: 0.000450   giou_loss: 1.64   conf_loss: 97.92   prob_loss: 0.60   total_loss: 100.17\n",
      "=> STEP 3553/6250   lr: 0.000450   giou_loss: 1.65   conf_loss: 97.89   prob_loss: 0.60   total_loss: 100.14\n",
      "=> STEP 3554/6250   lr: 0.000450   giou_loss: 1.56   conf_loss: 97.85   prob_loss: 0.60   total_loss: 100.02\n",
      "=> STEP 3555/6250   lr: 0.000449   giou_loss: 1.57   conf_loss: 97.82   prob_loss: 0.60   total_loss: 99.99\n",
      "=> STEP 3556/6250   lr: 0.000449   giou_loss: 1.56   conf_loss: 97.78   prob_loss: 0.60   total_loss: 99.95\n",
      "=> STEP 3557/6250   lr: 0.000449   giou_loss: 1.56   conf_loss: 97.75   prob_loss: 0.60   total_loss: 99.91\n",
      "=> STEP 3558/6250   lr: 0.000449   giou_loss: 1.57   conf_loss: 97.72   prob_loss: 0.60   total_loss: 99.89\n",
      "=> STEP 3559/6250   lr: 0.000448   giou_loss: 1.58   conf_loss: 97.68   prob_loss: 0.60   total_loss: 99.86\n",
      "=> STEP 3560/6250   lr: 0.000448   giou_loss: 1.60   conf_loss: 97.65   prob_loss: 0.60   total_loss: 99.85\n",
      "=> STEP 3561/6250   lr: 0.000448   giou_loss: 1.57   conf_loss: 97.62   prob_loss: 0.60   total_loss: 99.78\n",
      "=> STEP 3562/6250   lr: 0.000448   giou_loss: 1.56   conf_loss: 97.58   prob_loss: 0.60   total_loss: 99.75\n",
      "=> STEP 3563/6250   lr: 0.000448   giou_loss: 1.59   conf_loss: 97.55   prob_loss: 0.60   total_loss: 99.74\n",
      "=> STEP 3564/6250   lr: 0.000447   giou_loss: 1.56   conf_loss: 97.52   prob_loss: 0.60   total_loss: 99.68\n",
      "=> STEP 3565/6250   lr: 0.000447   giou_loss: 1.62   conf_loss: 97.48   prob_loss: 0.60   total_loss: 99.70\n",
      "=> STEP 3566/6250   lr: 0.000447   giou_loss: 1.58   conf_loss: 97.45   prob_loss: 0.60   total_loss: 99.63\n",
      "=> STEP 3567/6250   lr: 0.000447   giou_loss: 1.65   conf_loss: 97.42   prob_loss: 0.60   total_loss: 99.67\n",
      "=> STEP 3568/6250   lr: 0.000446   giou_loss: 1.63   conf_loss: 97.39   prob_loss: 0.60   total_loss: 99.61\n",
      "=> STEP 3569/6250   lr: 0.000446   giou_loss: 1.57   conf_loss: 97.35   prob_loss: 0.60   total_loss: 99.51\n",
      "=> STEP 3570/6250   lr: 0.000446   giou_loss: 1.63   conf_loss: 97.32   prob_loss: 0.60   total_loss: 99.54\n",
      "=> STEP 3571/6250   lr: 0.000446   giou_loss: 1.57   conf_loss: 97.28   prob_loss: 0.60   total_loss: 99.45\n",
      "=> STEP 3572/6250   lr: 0.000445   giou_loss: 1.56   conf_loss: 97.25   prob_loss: 0.60   total_loss: 99.41\n",
      "=> STEP 3573/6250   lr: 0.000445   giou_loss: 1.57   conf_loss: 97.22   prob_loss: 0.60   total_loss: 99.38\n",
      "=> STEP 3574/6250   lr: 0.000445   giou_loss: 1.58   conf_loss: 97.18   prob_loss: 0.60   total_loss: 99.36\n",
      "=> STEP 3575/6250   lr: 0.000445   giou_loss: 1.63   conf_loss: 97.15   prob_loss: 0.60   total_loss: 99.38\n",
      "=> STEP 3576/6250   lr: 0.000444   giou_loss: 1.60   conf_loss: 97.12   prob_loss: 0.60   total_loss: 99.32\n",
      "=> STEP 3577/6250   lr: 0.000444   giou_loss: 1.61   conf_loss: 97.09   prob_loss: 0.60   total_loss: 99.29\n",
      "=> STEP 3578/6250   lr: 0.000444   giou_loss: 1.61   conf_loss: 97.06   prob_loss: 0.60   total_loss: 99.26\n",
      "=> STEP 3579/6250   lr: 0.000444   giou_loss: 1.56   conf_loss: 97.02   prob_loss: 0.60   total_loss: 99.19\n",
      "=> STEP 3580/6250   lr: 0.000443   giou_loss: 1.60   conf_loss: 96.99   prob_loss: 0.60   total_loss: 99.19\n",
      "=> STEP 3581/6250   lr: 0.000443   giou_loss: 1.60   conf_loss: 96.96   prob_loss: 0.60   total_loss: 99.16\n",
      "=> STEP 3582/6250   lr: 0.000443   giou_loss: 1.56   conf_loss: 96.93   prob_loss: 0.60   total_loss: 99.09\n",
      "=> STEP 3583/6250   lr: 0.000443   giou_loss: 1.56   conf_loss: 96.90   prob_loss: 0.60   total_loss: 99.06\n",
      "=> STEP 3584/6250   lr: 0.000442   giou_loss: 1.57   conf_loss: 96.86   prob_loss: 0.60   total_loss: 99.03\n",
      "=> STEP 3585/6250   lr: 0.000442   giou_loss: 1.59   conf_loss: 96.83   prob_loss: 0.60   total_loss: 99.02\n",
      "=> STEP 3586/6250   lr: 0.000442   giou_loss: 1.56   conf_loss: 96.80   prob_loss: 0.60   total_loss: 98.96\n",
      "=> STEP 3587/6250   lr: 0.000442   giou_loss: 1.56   conf_loss: 96.76   prob_loss: 0.60   total_loss: 98.93\n",
      "=> STEP 3588/6250   lr: 0.000441   giou_loss: 1.64   conf_loss: 96.73   prob_loss: 0.60   total_loss: 98.97\n",
      "=> STEP 3589/6250   lr: 0.000441   giou_loss: 1.56   conf_loss: 96.69   prob_loss: 0.60   total_loss: 98.86\n",
      "=> STEP 3590/6250   lr: 0.000441   giou_loss: 1.65   conf_loss: 96.67   prob_loss: 0.60   total_loss: 98.92\n",
      "=> STEP 3591/6250   lr: 0.000441   giou_loss: 1.65   conf_loss: 96.64   prob_loss: 0.59   total_loss: 98.88\n",
      "=> STEP 3592/6250   lr: 0.000440   giou_loss: 1.60   conf_loss: 96.60   prob_loss: 0.59   total_loss: 98.79\n",
      "=> STEP 3593/6250   lr: 0.000440   giou_loss: 1.61   conf_loss: 96.57   prob_loss: 0.59   total_loss: 98.78\n",
      "=> STEP 3594/6250   lr: 0.000440   giou_loss: 1.58   conf_loss: 96.54   prob_loss: 0.60   total_loss: 98.71\n",
      "=> STEP 3595/6250   lr: 0.000440   giou_loss: 1.61   conf_loss: 96.50   prob_loss: 0.60   total_loss: 98.71\n",
      "=> STEP 3596/6250   lr: 0.000439   giou_loss: 1.59   conf_loss: 96.47   prob_loss: 0.60   total_loss: 98.65\n",
      "=> STEP 3597/6250   lr: 0.000439   giou_loss: 1.60   conf_loss: 96.44   prob_loss: 0.60   total_loss: 98.63\n",
      "=> STEP 3598/6250   lr: 0.000439   giou_loss: 1.62   conf_loss: 96.40   prob_loss: 0.60   total_loss: 98.62\n",
      "=> STEP 3599/6250   lr: 0.000439   giou_loss: 1.56   conf_loss: 96.37   prob_loss: 0.60   total_loss: 98.53\n",
      "=> STEP 3600/6250   lr: 0.000438   giou_loss: 1.61   conf_loss: 96.34   prob_loss: 0.60   total_loss: 98.54\n",
      "=> STEP 3601/6250   lr: 0.000438   giou_loss: 1.62   conf_loss: 96.30   prob_loss: 0.60   total_loss: 98.53\n",
      "=> STEP 3602/6250   lr: 0.000438   giou_loss: 1.62   conf_loss: 96.27   prob_loss: 0.60   total_loss: 98.49\n",
      "=> STEP 3603/6250   lr: 0.000438   giou_loss: 1.61   conf_loss: 96.25   prob_loss: 0.60   total_loss: 98.46\n",
      "=> STEP 3604/6250   lr: 0.000437   giou_loss: 1.68   conf_loss: 96.21   prob_loss: 0.59   total_loss: 98.49\n",
      "=> STEP 3605/6250   lr: 0.000437   giou_loss: 1.76   conf_loss: 96.18   prob_loss: 0.60   total_loss: 98.53\n",
      "=> STEP 3606/6250   lr: 0.000437   giou_loss: 1.66   conf_loss: 96.15   prob_loss: 0.60   total_loss: 98.40\n",
      "=> STEP 3607/6250   lr: 0.000437   giou_loss: 1.57   conf_loss: 96.12   prob_loss: 0.60   total_loss: 98.29\n",
      "=> STEP 3608/6250   lr: 0.000436   giou_loss: 1.71   conf_loss: 96.08   prob_loss: 0.60   total_loss: 98.39\n",
      "=> STEP 3609/6250   lr: 0.000436   giou_loss: 1.72   conf_loss: 96.05   prob_loss: 0.60   total_loss: 98.37\n",
      "=> STEP 3610/6250   lr: 0.000436   giou_loss: 1.66   conf_loss: 96.03   prob_loss: 0.60   total_loss: 98.29\n",
      "=> STEP 3611/6250   lr: 0.000436   giou_loss: 1.57   conf_loss: 96.01   prob_loss: 0.59   total_loss: 98.17\n",
      "=> STEP 3612/6250   lr: 0.000436   giou_loss: 1.60   conf_loss: 95.97   prob_loss: 0.59   total_loss: 98.15\n",
      "=> STEP 3613/6250   lr: 0.000435   giou_loss: 1.61   conf_loss: 95.94   prob_loss: 0.59   total_loss: 98.13\n",
      "=> STEP 3614/6250   lr: 0.000435   giou_loss: 1.56   conf_loss: 95.91   prob_loss: 0.59   total_loss: 98.07\n",
      "=> STEP 3615/6250   lr: 0.000435   giou_loss: 1.56   conf_loss: 95.87   prob_loss: 0.59   total_loss: 98.03\n",
      "=> STEP 3616/6250   lr: 0.000435   giou_loss: 1.62   conf_loss: 95.83   prob_loss: 0.59   total_loss: 98.05\n",
      "=> STEP 3617/6250   lr: 0.000434   giou_loss: 1.58   conf_loss: 95.80   prob_loss: 0.60   total_loss: 97.98\n",
      "=> STEP 3618/6250   lr: 0.000434   giou_loss: 1.57   conf_loss: 95.78   prob_loss: 0.59   total_loss: 97.94\n",
      "=> STEP 3619/6250   lr: 0.000434   giou_loss: 1.66   conf_loss: 95.75   prob_loss: 0.59   total_loss: 98.00\n",
      "=> STEP 3620/6250   lr: 0.000434   giou_loss: 1.66   conf_loss: 95.72   prob_loss: 0.59   total_loss: 97.97\n",
      "=> STEP 3621/6250   lr: 0.000433   giou_loss: 1.57   conf_loss: 95.69   prob_loss: 0.59   total_loss: 97.86\n",
      "=> STEP 3622/6250   lr: 0.000433   giou_loss: 1.59   conf_loss: 95.66   prob_loss: 0.60   total_loss: 97.85\n",
      "=> STEP 3623/6250   lr: 0.000433   giou_loss: 1.65   conf_loss: 95.61   prob_loss: 0.60   total_loss: 97.86\n",
      "=> STEP 3624/6250   lr: 0.000433   giou_loss: 1.60   conf_loss: 95.59   prob_loss: 0.60   total_loss: 97.79\n",
      "=> STEP 3625/6250   lr: 0.000432   giou_loss: 1.57   conf_loss: 95.56   prob_loss: 0.60   total_loss: 97.72\n",
      "=> STEP 3626/6250   lr: 0.000432   giou_loss: 1.64   conf_loss: 95.52   prob_loss: 0.59   total_loss: 97.76\n",
      "=> STEP 3627/6250   lr: 0.000432   giou_loss: 1.65   conf_loss: 95.49   prob_loss: 0.59   total_loss: 97.72\n",
      "=> STEP 3628/6250   lr: 0.000432   giou_loss: 1.57   conf_loss: 95.47   prob_loss: 0.59   total_loss: 97.63\n",
      "=> STEP 3629/6250   lr: 0.000431   giou_loss: 1.56   conf_loss: 95.43   prob_loss: 0.59   total_loss: 97.59\n",
      "=> STEP 3630/6250   lr: 0.000431   giou_loss: 1.57   conf_loss: 95.39   prob_loss: 0.59   total_loss: 97.55\n",
      "=> STEP 3631/6250   lr: 0.000431   giou_loss: 1.56   conf_loss: 95.36   prob_loss: 0.59   total_loss: 97.52\n",
      "=> STEP 3632/6250   lr: 0.000431   giou_loss: 1.56   conf_loss: 95.34   prob_loss: 0.59   total_loss: 97.49\n",
      "=> STEP 3633/6250   lr: 0.000430   giou_loss: 1.56   conf_loss: 95.30   prob_loss: 0.59   total_loss: 97.45\n",
      "=> STEP 3634/6250   lr: 0.000430   giou_loss: 1.56   conf_loss: 95.26   prob_loss: 0.59   total_loss: 97.42\n",
      "=> STEP 3635/6250   lr: 0.000430   giou_loss: 1.57   conf_loss: 95.24   prob_loss: 0.59   total_loss: 97.39\n",
      "=> STEP 3636/6250   lr: 0.000430   giou_loss: 1.60   conf_loss: 95.20   prob_loss: 0.59   total_loss: 97.39\n",
      "=> STEP 3637/6250   lr: 0.000429   giou_loss: 1.57   conf_loss: 95.17   prob_loss: 0.59   total_loss: 97.33\n",
      "=> STEP 3638/6250   lr: 0.000429   giou_loss: 1.64   conf_loss: 95.13   prob_loss: 0.59   total_loss: 97.37\n",
      "=> STEP 3639/6250   lr: 0.000429   giou_loss: 1.62   conf_loss: 95.11   prob_loss: 0.59   total_loss: 97.32\n",
      "=> STEP 3640/6250   lr: 0.000429   giou_loss: 1.57   conf_loss: 95.08   prob_loss: 0.59   total_loss: 97.24\n",
      "=> STEP 3641/6250   lr: 0.000428   giou_loss: 1.72   conf_loss: 95.04   prob_loss: 0.59   total_loss: 97.35\n",
      "=> STEP 3642/6250   lr: 0.000428   giou_loss: 1.70   conf_loss: 95.01   prob_loss: 0.59   total_loss: 97.30\n",
      "=> STEP 3643/6250   lr: 0.000428   giou_loss: 1.57   conf_loss: 94.99   prob_loss: 0.59   total_loss: 97.14\n",
      "=> STEP 3644/6250   lr: 0.000428   giou_loss: 1.65   conf_loss: 94.95   prob_loss: 0.59   total_loss: 97.19\n",
      "=> STEP 3645/6250   lr: 0.000427   giou_loss: 1.67   conf_loss: 94.91   prob_loss: 0.59   total_loss: 97.17\n",
      "=> STEP 3646/6250   lr: 0.000427   giou_loss: 1.56   conf_loss: 94.88   prob_loss: 0.59   total_loss: 97.04\n",
      "=> STEP 3647/6250   lr: 0.000427   giou_loss: 1.63   conf_loss: 94.86   prob_loss: 0.59   total_loss: 97.07\n",
      "=> STEP 3648/6250   lr: 0.000427   giou_loss: 1.61   conf_loss: 94.82   prob_loss: 0.59   total_loss: 97.02\n",
      "=> STEP 3649/6250   lr: 0.000426   giou_loss: 1.56   conf_loss: 94.79   prob_loss: 0.59   total_loss: 96.94\n",
      "=> STEP 3650/6250   lr: 0.000426   giou_loss: 1.65   conf_loss: 94.76   prob_loss: 0.59   total_loss: 97.00\n",
      "=> STEP 3651/6250   lr: 0.000426   giou_loss: 1.64   conf_loss: 94.73   prob_loss: 0.59   total_loss: 96.96\n",
      "=> STEP 3652/6250   lr: 0.000426   giou_loss: 1.56   conf_loss: 94.69   prob_loss: 0.59   total_loss: 96.85\n",
      "=> STEP 3653/6250   lr: 0.000426   giou_loss: 1.61   conf_loss: 94.66   prob_loss: 0.59   total_loss: 96.86\n",
      "=> STEP 3654/6250   lr: 0.000425   giou_loss: 1.60   conf_loss: 94.63   prob_loss: 0.59   total_loss: 96.82\n",
      "=> STEP 3655/6250   lr: 0.000425   giou_loss: 1.57   conf_loss: 94.60   prob_loss: 0.59   total_loss: 96.76\n",
      "=> STEP 3656/6250   lr: 0.000425   giou_loss: 1.57   conf_loss: 94.56   prob_loss: 0.59   total_loss: 96.73\n",
      "=> STEP 3657/6250   lr: 0.000425   giou_loss: 1.59   conf_loss: 94.53   prob_loss: 0.59   total_loss: 96.71\n",
      "=> STEP 3658/6250   lr: 0.000424   giou_loss: 1.58   conf_loss: 94.50   prob_loss: 0.59   total_loss: 96.67\n",
      "=> STEP 3659/6250   lr: 0.000424   giou_loss: 1.59   conf_loss: 94.47   prob_loss: 0.59   total_loss: 96.65\n",
      "=> STEP 3660/6250   lr: 0.000424   giou_loss: 1.58   conf_loss: 94.44   prob_loss: 0.59   total_loss: 96.61\n",
      "=> STEP 3661/6250   lr: 0.000424   giou_loss: 1.58   conf_loss: 94.41   prob_loss: 0.59   total_loss: 96.58\n",
      "=> STEP 3662/6250   lr: 0.000423   giou_loss: 1.57   conf_loss: 94.37   prob_loss: 0.59   total_loss: 96.54\n",
      "=> STEP 3663/6250   lr: 0.000423   giou_loss: 1.59   conf_loss: 94.35   prob_loss: 0.59   total_loss: 96.53\n",
      "=> STEP 3664/6250   lr: 0.000423   giou_loss: 1.60   conf_loss: 94.32   prob_loss: 0.59   total_loss: 96.50\n",
      "=> STEP 3665/6250   lr: 0.000423   giou_loss: 1.57   conf_loss: 94.28   prob_loss: 0.59   total_loss: 96.45\n",
      "=> STEP 3666/6250   lr: 0.000422   giou_loss: 1.60   conf_loss: 94.25   prob_loss: 0.59   total_loss: 96.44\n",
      "=> STEP 3667/6250   lr: 0.000422   giou_loss: 1.59   conf_loss: 94.22   prob_loss: 0.59   total_loss: 96.41\n",
      "=> STEP 3668/6250   lr: 0.000422   giou_loss: 1.59   conf_loss: 94.19   prob_loss: 0.59   total_loss: 96.37\n",
      "=> STEP 3669/6250   lr: 0.000422   giou_loss: 1.57   conf_loss: 94.16   prob_loss: 0.59   total_loss: 96.32\n",
      "=> STEP 3670/6250   lr: 0.000421   giou_loss: 1.56   conf_loss: 94.13   prob_loss: 0.59   total_loss: 96.28\n",
      "=> STEP 3671/6250   lr: 0.000421   giou_loss: 1.56   conf_loss: 94.10   prob_loss: 0.59   total_loss: 96.25\n",
      "=> STEP 3672/6250   lr: 0.000421   giou_loss: 1.57   conf_loss: 94.07   prob_loss: 0.59   total_loss: 96.23\n",
      "=> STEP 3673/6250   lr: 0.000421   giou_loss: 1.56   conf_loss: 94.04   prob_loss: 0.59   total_loss: 96.19\n",
      "=> STEP 3674/6250   lr: 0.000420   giou_loss: 1.57   conf_loss: 94.01   prob_loss: 0.59   total_loss: 96.16\n",
      "=> STEP 3675/6250   lr: 0.000420   giou_loss: 1.62   conf_loss: 93.98   prob_loss: 0.59   total_loss: 96.18\n",
      "=> STEP 3676/6250   lr: 0.000420   giou_loss: 1.57   conf_loss: 93.95   prob_loss: 0.59   total_loss: 96.10\n",
      "=> STEP 3677/6250   lr: 0.000420   giou_loss: 1.56   conf_loss: 93.92   prob_loss: 0.59   total_loss: 96.07\n",
      "=> STEP 3678/6250   lr: 0.000419   giou_loss: 1.58   conf_loss: 93.88   prob_loss: 0.59   total_loss: 96.06\n",
      "=> STEP 3679/6250   lr: 0.000419   giou_loss: 1.56   conf_loss: 93.86   prob_loss: 0.59   total_loss: 96.01\n",
      "=> STEP 3680/6250   lr: 0.000419   giou_loss: 1.57   conf_loss: 93.83   prob_loss: 0.59   total_loss: 95.99\n",
      "=> STEP 3681/6250   lr: 0.000419   giou_loss: 1.61   conf_loss: 93.80   prob_loss: 0.59   total_loss: 96.00\n",
      "=> STEP 3682/6250   lr: 0.000418   giou_loss: 1.57   conf_loss: 93.77   prob_loss: 0.59   total_loss: 95.93\n",
      "=> STEP 3683/6250   lr: 0.000418   giou_loss: 1.58   conf_loss: 93.74   prob_loss: 0.59   total_loss: 95.91\n",
      "=> STEP 3684/6250   lr: 0.000418   giou_loss: 1.56   conf_loss: 93.71   prob_loss: 0.59   total_loss: 95.86\n",
      "=> STEP 3685/6250   lr: 0.000418   giou_loss: 1.56   conf_loss: 93.67   prob_loss: 0.59   total_loss: 95.83\n",
      "=> STEP 3686/6250   lr: 0.000418   giou_loss: 1.58   conf_loss: 93.65   prob_loss: 0.59   total_loss: 95.81\n",
      "=> STEP 3687/6250   lr: 0.000417   giou_loss: 1.56   conf_loss: 93.62   prob_loss: 0.59   total_loss: 95.77\n",
      "=> STEP 3688/6250   lr: 0.000417   giou_loss: 1.57   conf_loss: 93.59   prob_loss: 0.59   total_loss: 95.74\n",
      "=> STEP 3689/6250   lr: 0.000417   giou_loss: 1.59   conf_loss: 93.56   prob_loss: 0.59   total_loss: 95.74\n",
      "=> STEP 3690/6250   lr: 0.000417   giou_loss: 1.57   conf_loss: 93.53   prob_loss: 0.59   total_loss: 95.68\n",
      "=> STEP 3691/6250   lr: 0.000416   giou_loss: 1.58   conf_loss: 93.50   prob_loss: 0.59   total_loss: 95.66\n",
      "=> STEP 3692/6250   lr: 0.000416   giou_loss: 1.56   conf_loss: 93.47   prob_loss: 0.59   total_loss: 95.62\n",
      "=> STEP 3693/6250   lr: 0.000416   giou_loss: 1.59   conf_loss: 93.43   prob_loss: 0.59   total_loss: 95.61\n",
      "=> STEP 3694/6250   lr: 0.000416   giou_loss: 1.56   conf_loss: 93.41   prob_loss: 0.59   total_loss: 95.56\n",
      "=> STEP 3695/6250   lr: 0.000415   giou_loss: 1.56   conf_loss: 93.38   prob_loss: 0.59   total_loss: 95.53\n",
      "=> STEP 3696/6250   lr: 0.000415   giou_loss: 1.57   conf_loss: 93.35   prob_loss: 0.59   total_loss: 95.50\n",
      "=> STEP 3697/6250   lr: 0.000415   giou_loss: 1.59   conf_loss: 93.32   prob_loss: 0.58   total_loss: 95.50\n",
      "=> STEP 3698/6250   lr: 0.000415   giou_loss: 1.57   conf_loss: 93.29   prob_loss: 0.59   total_loss: 95.45\n",
      "=> STEP 3699/6250   lr: 0.000414   giou_loss: 1.56   conf_loss: 93.26   prob_loss: 0.59   total_loss: 95.41\n",
      "=> STEP 3700/6250   lr: 0.000414   giou_loss: 1.58   conf_loss: 93.23   prob_loss: 0.59   total_loss: 95.40\n",
      "=> STEP 3701/6250   lr: 0.000414   giou_loss: 1.56   conf_loss: 93.20   prob_loss: 0.59   total_loss: 95.35\n",
      "=> STEP 3702/6250   lr: 0.000414   giou_loss: 1.57   conf_loss: 93.17   prob_loss: 0.59   total_loss: 95.33\n",
      "=> STEP 3703/6250   lr: 0.000413   giou_loss: 1.58   conf_loss: 93.14   prob_loss: 0.59   total_loss: 95.31\n",
      "=> STEP 3704/6250   lr: 0.000413   giou_loss: 1.57   conf_loss: 93.11   prob_loss: 0.59   total_loss: 95.27\n",
      "=> STEP 3705/6250   lr: 0.000413   giou_loss: 1.56   conf_loss: 93.08   prob_loss: 0.59   total_loss: 95.23\n",
      "=> STEP 3706/6250   lr: 0.000413   giou_loss: 1.57   conf_loss: 93.05   prob_loss: 0.59   total_loss: 95.21\n",
      "=> STEP 3707/6250   lr: 0.000412   giou_loss: 1.57   conf_loss: 93.02   prob_loss: 0.59   total_loss: 95.17\n",
      "=> STEP 3708/6250   lr: 0.000412   giou_loss: 1.58   conf_loss: 92.99   prob_loss: 0.59   total_loss: 95.15\n",
      "=> STEP 3709/6250   lr: 0.000412   giou_loss: 1.57   conf_loss: 92.96   prob_loss: 0.59   total_loss: 95.11\n",
      "=> STEP 3710/6250   lr: 0.000412   giou_loss: 1.57   conf_loss: 92.93   prob_loss: 0.58   total_loss: 95.08\n",
      "=> STEP 3711/6250   lr: 0.000411   giou_loss: 1.57   conf_loss: 92.90   prob_loss: 0.58   total_loss: 95.06\n",
      "=> STEP 3712/6250   lr: 0.000411   giou_loss: 1.57   conf_loss: 92.87   prob_loss: 0.58   total_loss: 95.03\n",
      "=> STEP 3713/6250   lr: 0.000411   giou_loss: 1.58   conf_loss: 92.84   prob_loss: 0.59   total_loss: 95.01\n",
      "=> STEP 3714/6250   lr: 0.000411   giou_loss: 1.64   conf_loss: 92.81   prob_loss: 0.59   total_loss: 95.04\n",
      "=> STEP 3715/6250   lr: 0.000410   giou_loss: 1.61   conf_loss: 92.78   prob_loss: 0.59   total_loss: 94.98\n",
      "=> STEP 3716/6250   lr: 0.000410   giou_loss: 1.57   conf_loss: 92.75   prob_loss: 0.59   total_loss: 94.91\n",
      "=> STEP 3717/6250   lr: 0.000410   giou_loss: 1.58   conf_loss: 92.72   prob_loss: 0.59   total_loss: 94.89\n",
      "=> STEP 3718/6250   lr: 0.000410   giou_loss: 1.57   conf_loss: 92.69   prob_loss: 0.59   total_loss: 94.85\n",
      "=> STEP 3719/6250   lr: 0.000410   giou_loss: 1.57   conf_loss: 92.66   prob_loss: 0.58   total_loss: 94.82\n",
      "=> STEP 3720/6250   lr: 0.000409   giou_loss: 1.57   conf_loss: 92.63   prob_loss: 0.58   total_loss: 94.79\n",
      "=> STEP 3721/6250   lr: 0.000409   giou_loss: 1.57   conf_loss: 92.60   prob_loss: 0.58   total_loss: 94.75\n",
      "=> STEP 3722/6250   lr: 0.000409   giou_loss: 1.57   conf_loss: 92.57   prob_loss: 0.58   total_loss: 94.72\n",
      "=> STEP 3723/6250   lr: 0.000409   giou_loss: 1.57   conf_loss: 92.54   prob_loss: 0.58   total_loss: 94.70\n",
      "=> STEP 3724/6250   lr: 0.000408   giou_loss: 1.58   conf_loss: 92.51   prob_loss: 0.59   total_loss: 94.68\n",
      "=> STEP 3725/6250   lr: 0.000408   giou_loss: 1.57   conf_loss: 92.48   prob_loss: 0.59   total_loss: 94.64\n",
      "=> STEP 3726/6250   lr: 0.000408   giou_loss: 1.57   conf_loss: 92.45   prob_loss: 0.59   total_loss: 94.61\n",
      "=> STEP 3727/6250   lr: 0.000408   giou_loss: 1.57   conf_loss: 92.43   prob_loss: 0.59   total_loss: 94.58\n",
      "=> STEP 3728/6250   lr: 0.000407   giou_loss: 1.56   conf_loss: 92.40   prob_loss: 0.59   total_loss: 94.55\n",
      "=> STEP 3729/6250   lr: 0.000407   giou_loss: 1.57   conf_loss: 92.37   prob_loss: 0.58   total_loss: 94.52\n",
      "=> STEP 3730/6250   lr: 0.000407   giou_loss: 1.58   conf_loss: 92.34   prob_loss: 0.59   total_loss: 94.50\n",
      "=> STEP 3731/6250   lr: 0.000407   giou_loss: 1.56   conf_loss: 92.31   prob_loss: 0.59   total_loss: 94.46\n",
      "=> STEP 3732/6250   lr: 0.000406   giou_loss: 1.56   conf_loss: 92.28   prob_loss: 0.58   total_loss: 94.43\n",
      "=> STEP 3733/6250   lr: 0.000406   giou_loss: 1.56   conf_loss: 92.25   prob_loss: 0.58   total_loss: 94.40\n",
      "=> STEP 3734/6250   lr: 0.000406   giou_loss: 1.56   conf_loss: 92.22   prob_loss: 0.58   total_loss: 94.37\n",
      "=> STEP 3735/6250   lr: 0.000406   giou_loss: 1.58   conf_loss: 92.19   prob_loss: 0.58   total_loss: 94.36\n",
      "=> STEP 3736/6250   lr: 0.000405   giou_loss: 1.57   conf_loss: 92.16   prob_loss: 0.58   total_loss: 94.32\n",
      "=> STEP 3737/6250   lr: 0.000405   giou_loss: 1.60   conf_loss: 92.13   prob_loss: 0.58   total_loss: 94.32\n",
      "=> STEP 3738/6250   lr: 0.000405   giou_loss: 1.57   conf_loss: 92.10   prob_loss: 0.58   total_loss: 94.25\n",
      "=> STEP 3739/6250   lr: 0.000405   giou_loss: 1.56   conf_loss: 92.08   prob_loss: 0.58   total_loss: 94.23\n",
      "=> STEP 3740/6250   lr: 0.000404   giou_loss: 1.58   conf_loss: 92.05   prob_loss: 0.58   total_loss: 94.21\n",
      "=> STEP 3741/6250   lr: 0.000404   giou_loss: 1.56   conf_loss: 92.02   prob_loss: 0.58   total_loss: 94.17\n",
      "=> STEP 3742/6250   lr: 0.000404   giou_loss: 1.63   conf_loss: 91.99   prob_loss: 0.58   total_loss: 94.21\n",
      "=> STEP 3743/6250   lr: 0.000404   giou_loss: 1.60   conf_loss: 91.96   prob_loss: 0.58   total_loss: 94.14\n",
      "=> STEP 3744/6250   lr: 0.000404   giou_loss: 1.63   conf_loss: 91.94   prob_loss: 0.58   total_loss: 94.15\n",
      "=> STEP 3745/6250   lr: 0.000403   giou_loss: 1.61   conf_loss: 91.91   prob_loss: 0.58   total_loss: 94.10\n",
      "=> STEP 3746/6250   lr: 0.000403   giou_loss: 1.57   conf_loss: 91.88   prob_loss: 0.58   total_loss: 94.02\n",
      "=> STEP 3747/6250   lr: 0.000403   giou_loss: 1.66   conf_loss: 91.85   prob_loss: 0.58   total_loss: 94.09\n",
      "=> STEP 3748/6250   lr: 0.000403   giou_loss: 1.59   conf_loss: 91.82   prob_loss: 0.58   total_loss: 93.99\n",
      "=> STEP 3749/6250   lr: 0.000402   giou_loss: 1.60   conf_loss: 91.79   prob_loss: 0.58   total_loss: 93.97\n",
      "=> STEP 3750/6250   lr: 0.000402   giou_loss: 1.61   conf_loss: 91.76   prob_loss: 0.58   total_loss: 93.95\n",
      "=> STEP 3751/6250   lr: 0.000402   giou_loss: 1.56   conf_loss: 91.73   prob_loss: 0.58   total_loss: 93.88\n",
      "=> STEP 3752/6250   lr: 0.000402   giou_loss: 1.64   conf_loss: 91.70   prob_loss: 0.59   total_loss: 93.92\n",
      "=> STEP 3753/6250   lr: 0.000401   giou_loss: 1.59   conf_loss: 91.68   prob_loss: 0.58   total_loss: 93.85\n",
      "=> STEP 3754/6250   lr: 0.000401   giou_loss: 1.59   conf_loss: 91.65   prob_loss: 0.58   total_loss: 93.82\n",
      "=> STEP 3755/6250   lr: 0.000401   giou_loss: 1.59   conf_loss: 91.62   prob_loss: 0.58   total_loss: 93.79\n",
      "=> STEP 3756/6250   lr: 0.000401   giou_loss: 1.61   conf_loss: 91.59   prob_loss: 0.58   total_loss: 93.78\n",
      "=> STEP 3757/6250   lr: 0.000400   giou_loss: 1.57   conf_loss: 91.56   prob_loss: 0.58   total_loss: 93.71\n",
      "=> STEP 3758/6250   lr: 0.000400   giou_loss: 1.56   conf_loss: 91.53   prob_loss: 0.58   total_loss: 93.68\n",
      "=> STEP 3759/6250   lr: 0.000400   giou_loss: 1.56   conf_loss: 91.50   prob_loss: 0.58   total_loss: 93.65\n",
      "=> STEP 3760/6250   lr: 0.000400   giou_loss: 1.61   conf_loss: 91.48   prob_loss: 0.58   total_loss: 93.67\n",
      "=> STEP 3761/6250   lr: 0.000399   giou_loss: 1.57   conf_loss: 91.45   prob_loss: 0.58   total_loss: 93.60\n",
      "=> STEP 3762/6250   lr: 0.000399   giou_loss: 1.58   conf_loss: 91.42   prob_loss: 0.58   total_loss: 93.59\n",
      "=> STEP 3763/6250   lr: 0.000399   giou_loss: 1.57   conf_loss: 91.39   prob_loss: 0.58   total_loss: 93.54\n",
      "=> STEP 3764/6250   lr: 0.000399   giou_loss: 1.60   conf_loss: 91.37   prob_loss: 0.58   total_loss: 93.55\n",
      "=> STEP 3765/6250   lr: 0.000398   giou_loss: 1.62   conf_loss: 91.34   prob_loss: 0.58   total_loss: 93.54\n",
      "=> STEP 3766/6250   lr: 0.000398   giou_loss: 1.57   conf_loss: 91.31   prob_loss: 0.58   total_loss: 93.45\n",
      "=> STEP 3767/6250   lr: 0.000398   giou_loss: 1.62   conf_loss: 91.28   prob_loss: 0.58   total_loss: 93.48\n",
      "=> STEP 3768/6250   lr: 0.000398   giou_loss: 1.62   conf_loss: 91.25   prob_loss: 0.58   total_loss: 93.45\n",
      "=> STEP 3769/6250   lr: 0.000398   giou_loss: 1.58   conf_loss: 91.23   prob_loss: 0.58   total_loss: 93.39\n",
      "=> STEP 3770/6250   lr: 0.000397   giou_loss: 1.58   conf_loss: 91.20   prob_loss: 0.58   total_loss: 93.36\n",
      "=> STEP 3771/6250   lr: 0.000397   giou_loss: 1.57   conf_loss: 91.17   prob_loss: 0.58   total_loss: 93.32\n",
      "=> STEP 3772/6250   lr: 0.000397   giou_loss: 1.57   conf_loss: 91.14   prob_loss: 0.58   total_loss: 93.28\n",
      "=> STEP 3773/6250   lr: 0.000397   giou_loss: 1.57   conf_loss: 91.12   prob_loss: 0.58   total_loss: 93.26\n",
      "=> STEP 3774/6250   lr: 0.000396   giou_loss: 1.57   conf_loss: 91.08   prob_loss: 0.58   total_loss: 93.23\n",
      "=> STEP 3775/6250   lr: 0.000396   giou_loss: 1.57   conf_loss: 91.06   prob_loss: 0.58   total_loss: 93.20\n",
      "=> STEP 3776/6250   lr: 0.000396   giou_loss: 1.56   conf_loss: 91.02   prob_loss: 0.58   total_loss: 93.17\n",
      "=> STEP 3777/6250   lr: 0.000396   giou_loss: 1.61   conf_loss: 91.00   prob_loss: 0.58   total_loss: 93.18\n",
      "=> STEP 3778/6250   lr: 0.000395   giou_loss: 1.58   conf_loss: 90.97   prob_loss: 0.58   total_loss: 93.13\n",
      "=> STEP 3779/6250   lr: 0.000395   giou_loss: 1.57   conf_loss: 90.95   prob_loss: 0.58   total_loss: 93.09\n",
      "=> STEP 3780/6250   lr: 0.000395   giou_loss: 1.67   conf_loss: 90.92   prob_loss: 0.58   total_loss: 93.17\n",
      "=> STEP 3781/6250   lr: 0.000395   giou_loss: 1.68   conf_loss: 90.89   prob_loss: 0.58   total_loss: 93.15\n",
      "=> STEP 3782/6250   lr: 0.000394   giou_loss: 1.57   conf_loss: 90.86   prob_loss: 0.58   total_loss: 93.01\n",
      "=> STEP 3783/6250   lr: 0.000394   giou_loss: 1.60   conf_loss: 90.83   prob_loss: 0.58   total_loss: 93.01\n",
      "=> STEP 3784/6250   lr: 0.000394   giou_loss: 1.59   conf_loss: 90.80   prob_loss: 0.58   total_loss: 92.98\n",
      "=> STEP 3785/6250   lr: 0.000394   giou_loss: 1.56   conf_loss: 90.78   prob_loss: 0.58   total_loss: 92.92\n",
      "=> STEP 3786/6250   lr: 0.000393   giou_loss: 1.56   conf_loss: 90.75   prob_loss: 0.58   total_loss: 92.90\n",
      "=> STEP 3787/6250   lr: 0.000393   giou_loss: 1.56   conf_loss: 90.73   prob_loss: 0.58   total_loss: 92.87\n",
      "=> STEP 3788/6250   lr: 0.000393   giou_loss: 1.59   conf_loss: 90.70   prob_loss: 0.58   total_loss: 92.86\n",
      "=> STEP 3789/6250   lr: 0.000393   giou_loss: 1.57   conf_loss: 90.67   prob_loss: 0.58   total_loss: 92.81\n",
      "=> STEP 3790/6250   lr: 0.000392   giou_loss: 1.64   conf_loss: 90.64   prob_loss: 0.58   total_loss: 92.87\n",
      "=> STEP 3791/6250   lr: 0.000392   giou_loss: 1.61   conf_loss: 90.61   prob_loss: 0.58   total_loss: 92.80\n",
      "=> STEP 3792/6250   lr: 0.000392   giou_loss: 1.59   conf_loss: 90.58   prob_loss: 0.58   total_loss: 92.75\n",
      "=> STEP 3793/6250   lr: 0.000392   giou_loss: 1.60   conf_loss: 90.55   prob_loss: 0.58   total_loss: 92.74\n",
      "=> STEP 3794/6250   lr: 0.000392   giou_loss: 1.57   conf_loss: 90.52   prob_loss: 0.58   total_loss: 92.67\n",
      "=> STEP 3795/6250   lr: 0.000391   giou_loss: 1.64   conf_loss: 90.50   prob_loss: 0.58   total_loss: 92.72\n",
      "=> STEP 3796/6250   lr: 0.000391   giou_loss: 1.61   conf_loss: 90.47   prob_loss: 0.58   total_loss: 92.66\n",
      "=> STEP 3797/6250   lr: 0.000391   giou_loss: 1.58   conf_loss: 90.44   prob_loss: 0.58   total_loss: 92.61\n",
      "=> STEP 3798/6250   lr: 0.000391   giou_loss: 1.59   conf_loss: 90.41   prob_loss: 0.58   total_loss: 92.58\n",
      "=> STEP 3799/6250   lr: 0.000390   giou_loss: 1.56   conf_loss: 90.39   prob_loss: 0.58   total_loss: 92.53\n",
      "=> STEP 3800/6250   lr: 0.000390   giou_loss: 1.64   conf_loss: 90.36   prob_loss: 0.58   total_loss: 92.58\n",
      "=> STEP 3801/6250   lr: 0.000390   giou_loss: 1.62   conf_loss: 90.33   prob_loss: 0.58   total_loss: 92.53\n",
      "=> STEP 3802/6250   lr: 0.000390   giou_loss: 1.56   conf_loss: 90.30   prob_loss: 0.58   total_loss: 92.45\n",
      "=> STEP 3803/6250   lr: 0.000389   giou_loss: 1.66   conf_loss: 90.28   prob_loss: 0.58   total_loss: 92.52\n",
      "=> STEP 3804/6250   lr: 0.000389   giou_loss: 1.65   conf_loss: 90.25   prob_loss: 0.58   total_loss: 92.48\n",
      "=> STEP 3805/6250   lr: 0.000389   giou_loss: 1.60   conf_loss: 90.22   prob_loss: 0.58   total_loss: 92.40\n",
      "=> STEP 3806/6250   lr: 0.000389   giou_loss: 1.60   conf_loss: 90.19   prob_loss: 0.58   total_loss: 92.37\n",
      "=> STEP 3807/6250   lr: 0.000388   giou_loss: 1.56   conf_loss: 90.17   prob_loss: 0.58   total_loss: 92.31\n",
      "=> STEP 3808/6250   lr: 0.000388   giou_loss: 1.66   conf_loss: 90.14   prob_loss: 0.58   total_loss: 92.39\n",
      "=> STEP 3809/6250   lr: 0.000388   giou_loss: 1.67   conf_loss: 90.11   prob_loss: 0.58   total_loss: 92.37\n",
      "=> STEP 3810/6250   lr: 0.000388   giou_loss: 1.59   conf_loss: 90.09   prob_loss: 0.58   total_loss: 92.26\n",
      "=> STEP 3811/6250   lr: 0.000387   giou_loss: 1.57   conf_loss: 90.07   prob_loss: 0.58   total_loss: 92.22\n",
      "=> STEP 3812/6250   lr: 0.000387   giou_loss: 1.63   conf_loss: 90.04   prob_loss: 0.58   total_loss: 92.25\n",
      "=> STEP 3813/6250   lr: 0.000387   giou_loss: 1.58   conf_loss: 90.01   prob_loss: 0.58   total_loss: 92.17\n",
      "=> STEP 3814/6250   lr: 0.000387   giou_loss: 1.62   conf_loss: 89.99   prob_loss: 0.58   total_loss: 92.19\n",
      "=> STEP 3815/6250   lr: 0.000387   giou_loss: 1.67   conf_loss: 89.96   prob_loss: 0.58   total_loss: 92.21\n",
      "=> STEP 3816/6250   lr: 0.000386   giou_loss: 1.72   conf_loss: 89.93   prob_loss: 0.58   total_loss: 92.23\n",
      "=> STEP 3817/6250   lr: 0.000386   giou_loss: 1.66   conf_loss: 89.92   prob_loss: 0.58   total_loss: 92.16\n",
      "=> STEP 3818/6250   lr: 0.000386   giou_loss: 1.57   conf_loss: 89.90   prob_loss: 0.58   total_loss: 92.05\n",
      "=> STEP 3819/6250   lr: 0.000386   giou_loss: 1.72   conf_loss: 89.87   prob_loss: 0.57   total_loss: 92.16\n",
      "=> STEP 3820/6250   lr: 0.000385   giou_loss: 1.72   conf_loss: 89.84   prob_loss: 0.57   total_loss: 92.13\n",
      "=> STEP 3821/6250   lr: 0.000385   giou_loss: 1.68   conf_loss: 89.82   prob_loss: 0.57   total_loss: 92.08\n",
      "=> STEP 3822/6250   lr: 0.000385   giou_loss: 1.62   conf_loss: 89.79   prob_loss: 0.58   total_loss: 91.99\n",
      "=> STEP 3823/6250   lr: 0.000385   giou_loss: 1.60   conf_loss: 89.75   prob_loss: 0.58   total_loss: 91.93\n",
      "=> STEP 3824/6250   lr: 0.000384   giou_loss: 1.63   conf_loss: 89.72   prob_loss: 0.58   total_loss: 91.93\n",
      "=> STEP 3825/6250   lr: 0.000384   giou_loss: 1.58   conf_loss: 89.71   prob_loss: 0.58   total_loss: 91.86\n",
      "=> STEP 3826/6250   lr: 0.000384   giou_loss: 1.59   conf_loss: 89.68   prob_loss: 0.58   total_loss: 91.84\n",
      "=> STEP 3827/6250   lr: 0.000384   giou_loss: 1.60   conf_loss: 89.65   prob_loss: 0.57   total_loss: 91.83\n",
      "=> STEP 3828/6250   lr: 0.000383   giou_loss: 1.57   conf_loss: 89.63   prob_loss: 0.57   total_loss: 91.76\n",
      "=> STEP 3829/6250   lr: 0.000383   giou_loss: 1.63   conf_loss: 89.60   prob_loss: 0.57   total_loss: 91.80\n",
      "=> STEP 3830/6250   lr: 0.000383   giou_loss: 1.60   conf_loss: 89.57   prob_loss: 0.58   total_loss: 91.74\n",
      "=> STEP 3831/6250   lr: 0.000383   giou_loss: 1.58   conf_loss: 89.53   prob_loss: 0.58   total_loss: 91.68\n",
      "=> STEP 3832/6250   lr: 0.000382   giou_loss: 1.57   conf_loss: 89.51   prob_loss: 0.58   total_loss: 91.66\n",
      "=> STEP 3833/6250   lr: 0.000382   giou_loss: 1.58   conf_loss: 89.48   prob_loss: 0.58   total_loss: 91.64\n",
      "=> STEP 3834/6250   lr: 0.000382   giou_loss: 1.60   conf_loss: 89.45   prob_loss: 0.58   total_loss: 91.63\n",
      "=> STEP 3835/6250   lr: 0.000382   giou_loss: 1.59   conf_loss: 89.43   prob_loss: 0.58   total_loss: 91.60\n",
      "=> STEP 3836/6250   lr: 0.000382   giou_loss: 1.59   conf_loss: 89.40   prob_loss: 0.58   total_loss: 91.57\n",
      "=> STEP 3837/6250   lr: 0.000381   giou_loss: 1.56   conf_loss: 89.37   prob_loss: 0.58   total_loss: 91.52\n",
      "=> STEP 3838/6250   lr: 0.000381   giou_loss: 1.65   conf_loss: 89.33   prob_loss: 0.58   total_loss: 91.56\n",
      "=> STEP 3839/6250   lr: 0.000381   giou_loss: 1.63   conf_loss: 89.32   prob_loss: 0.58   total_loss: 91.53\n",
      "=> STEP 3840/6250   lr: 0.000381   giou_loss: 1.57   conf_loss: 89.29   prob_loss: 0.58   total_loss: 91.43\n",
      "=> STEP 3841/6250   lr: 0.000380   giou_loss: 1.62   conf_loss: 89.27   prob_loss: 0.57   total_loss: 91.46\n",
      "=> STEP 3842/6250   lr: 0.000380   giou_loss: 1.62   conf_loss: 89.24   prob_loss: 0.57   total_loss: 91.43\n",
      "=> STEP 3843/6250   lr: 0.000380   giou_loss: 1.57   conf_loss: 89.21   prob_loss: 0.58   total_loss: 91.35\n",
      "=> STEP 3844/6250   lr: 0.000380   giou_loss: 1.63   conf_loss: 89.19   prob_loss: 0.58   total_loss: 91.39\n",
      "=> STEP 3845/6250   lr: 0.000379   giou_loss: 1.61   conf_loss: 89.15   prob_loss: 0.58   total_loss: 91.35\n",
      "=> STEP 3846/6250   lr: 0.000379   giou_loss: 1.62   conf_loss: 89.12   prob_loss: 0.58   total_loss: 91.32\n",
      "=> STEP 3847/6250   lr: 0.000379   giou_loss: 1.65   conf_loss: 89.10   prob_loss: 0.58   total_loss: 91.33\n",
      "=> STEP 3848/6250   lr: 0.000379   giou_loss: 1.62   conf_loss: 89.08   prob_loss: 0.58   total_loss: 91.27\n",
      "=> STEP 3849/6250   lr: 0.000378   giou_loss: 1.66   conf_loss: 89.05   prob_loss: 0.58   total_loss: 91.29\n",
      "=> STEP 3850/6250   lr: 0.000378   giou_loss: 1.76   conf_loss: 89.02   prob_loss: 0.58   total_loss: 91.35\n",
      "=> STEP 3851/6250   lr: 0.000378   giou_loss: 1.69   conf_loss: 89.00   prob_loss: 0.58   total_loss: 91.26\n",
      "=> STEP 3852/6250   lr: 0.000378   giou_loss: 1.56   conf_loss: 88.97   prob_loss: 0.58   total_loss: 91.12\n",
      "=> STEP 3853/6250   lr: 0.000378   giou_loss: 1.71   conf_loss: 88.94   prob_loss: 0.58   total_loss: 91.23\n",
      "=> STEP 3854/6250   lr: 0.000377   giou_loss: 1.72   conf_loss: 88.91   prob_loss: 0.58   total_loss: 91.21\n",
      "=> STEP 3855/6250   lr: 0.000377   giou_loss: 1.62   conf_loss: 88.90   prob_loss: 0.58   total_loss: 91.09\n",
      "=> STEP 3856/6250   lr: 0.000377   giou_loss: 1.65   conf_loss: 88.87   prob_loss: 0.58   total_loss: 91.11\n",
      "=> STEP 3857/6250   lr: 0.000377   giou_loss: 1.72   conf_loss: 88.84   prob_loss: 0.58   total_loss: 91.13\n",
      "=> STEP 3858/6250   lr: 0.000376   giou_loss: 1.65   conf_loss: 88.81   prob_loss: 0.57   total_loss: 91.03\n",
      "=> STEP 3859/6250   lr: 0.000376   giou_loss: 1.59   conf_loss: 88.80   prob_loss: 0.57   total_loss: 90.96\n",
      "=> STEP 3860/6250   lr: 0.000376   giou_loss: 1.57   conf_loss: 88.77   prob_loss: 0.58   total_loss: 90.92\n",
      "=> STEP 3861/6250   lr: 0.000376   giou_loss: 1.69   conf_loss: 88.73   prob_loss: 0.58   total_loss: 91.00\n",
      "=> STEP 3862/6250   lr: 0.000375   giou_loss: 1.72   conf_loss: 88.70   prob_loss: 0.59   total_loss: 91.01\n",
      "=> STEP 3863/6250   lr: 0.000375   giou_loss: 1.65   conf_loss: 88.69   prob_loss: 0.58   total_loss: 90.92\n",
      "=> STEP 3864/6250   lr: 0.000375   giou_loss: 1.59   conf_loss: 88.67   prob_loss: 0.58   total_loss: 90.83\n",
      "=> STEP 3865/6250   lr: 0.000375   giou_loss: 1.58   conf_loss: 88.64   prob_loss: 0.57   total_loss: 90.79\n",
      "=> STEP 3866/6250   lr: 0.000374   giou_loss: 1.60   conf_loss: 88.61   prob_loss: 0.57   total_loss: 90.78\n",
      "=> STEP 3867/6250   lr: 0.000374   giou_loss: 1.57   conf_loss: 88.60   prob_loss: 0.57   total_loss: 90.73\n",
      "=> STEP 3868/6250   lr: 0.000374   giou_loss: 1.57   conf_loss: 88.57   prob_loss: 0.58   total_loss: 90.71\n",
      "=> STEP 3869/6250   lr: 0.000374   giou_loss: 1.60   conf_loss: 88.52   prob_loss: 0.58   total_loss: 90.70\n",
      "=> STEP 3870/6250   lr: 0.000373   giou_loss: 1.63   conf_loss: 88.50   prob_loss: 0.58   total_loss: 90.71\n",
      "=> STEP 3871/6250   lr: 0.000373   giou_loss: 1.58   conf_loss: 88.49   prob_loss: 0.58   total_loss: 90.65\n",
      "=> STEP 3872/6250   lr: 0.000373   giou_loss: 1.68   conf_loss: 88.46   prob_loss: 0.58   total_loss: 90.71\n",
      "=> STEP 3873/6250   lr: 0.000373   giou_loss: 1.70   conf_loss: 88.42   prob_loss: 0.58   total_loss: 90.70\n",
      "=> STEP 3874/6250   lr: 0.000373   giou_loss: 1.57   conf_loss: 88.40   prob_loss: 0.58   total_loss: 90.55\n",
      "=> STEP 3875/6250   lr: 0.000372   giou_loss: 1.57   conf_loss: 88.38   prob_loss: 0.58   total_loss: 90.53\n",
      "=> STEP 3876/6250   lr: 0.000372   giou_loss: 1.56   conf_loss: 88.34   prob_loss: 0.58   total_loss: 90.49\n",
      "=> STEP 3877/6250   lr: 0.000372   giou_loss: 1.61   conf_loss: 88.31   prob_loss: 0.58   total_loss: 90.50\n",
      "=> STEP 3878/6250   lr: 0.000372   giou_loss: 1.58   conf_loss: 88.30   prob_loss: 0.58   total_loss: 90.46\n",
      "=> STEP 3879/6250   lr: 0.000371   giou_loss: 1.56   conf_loss: 88.27   prob_loss: 0.58   total_loss: 90.41\n",
      "=> STEP 3880/6250   lr: 0.000371   giou_loss: 1.59   conf_loss: 88.24   prob_loss: 0.57   total_loss: 90.40\n",
      "=> STEP 3881/6250   lr: 0.000371   giou_loss: 1.56   conf_loss: 88.22   prob_loss: 0.57   total_loss: 90.35\n",
      "=> STEP 3882/6250   lr: 0.000371   giou_loss: 1.57   conf_loss: 88.19   prob_loss: 0.57   total_loss: 90.33\n",
      "=> STEP 3883/6250   lr: 0.000370   giou_loss: 1.58   conf_loss: 88.17   prob_loss: 0.57   total_loss: 90.32\n",
      "=> STEP 3884/6250   lr: 0.000370   giou_loss: 1.57   conf_loss: 88.13   prob_loss: 0.57   total_loss: 90.27\n",
      "=> STEP 3885/6250   lr: 0.000370   giou_loss: 1.60   conf_loss: 88.11   prob_loss: 0.57   total_loss: 90.28\n",
      "=> STEP 3886/6250   lr: 0.000370   giou_loss: 1.57   conf_loss: 88.08   prob_loss: 0.58   total_loss: 90.22\n",
      "=> STEP 3887/6250   lr: 0.000369   giou_loss: 1.57   conf_loss: 88.05   prob_loss: 0.58   total_loss: 90.19\n",
      "=> STEP 3888/6250   lr: 0.000369   giou_loss: 1.57   conf_loss: 88.03   prob_loss: 0.57   total_loss: 90.17\n",
      "=> STEP 3889/6250   lr: 0.000369   giou_loss: 1.59   conf_loss: 88.00   prob_loss: 0.57   total_loss: 90.16\n",
      "=> STEP 3890/6250   lr: 0.000369   giou_loss: 1.57   conf_loss: 87.98   prob_loss: 0.57   total_loss: 90.12\n",
      "=> STEP 3891/6250   lr: 0.000369   giou_loss: 1.60   conf_loss: 87.95   prob_loss: 0.57   total_loss: 90.13\n",
      "=> STEP 3892/6250   lr: 0.000368   giou_loss: 1.56   conf_loss: 87.92   prob_loss: 0.57   total_loss: 90.06\n",
      "=> STEP 3893/6250   lr: 0.000368   giou_loss: 1.56   conf_loss: 87.90   prob_loss: 0.57   total_loss: 90.04\n",
      "=> STEP 3894/6250   lr: 0.000368   giou_loss: 1.57   conf_loss: 87.87   prob_loss: 0.58   total_loss: 90.01\n",
      "=> STEP 3895/6250   lr: 0.000368   giou_loss: 1.57   conf_loss: 87.84   prob_loss: 0.58   total_loss: 89.99\n",
      "=> STEP 3896/6250   lr: 0.000367   giou_loss: 1.56   conf_loss: 87.82   prob_loss: 0.57   total_loss: 89.96\n",
      "=> STEP 3897/6250   lr: 0.000367   giou_loss: 1.62   conf_loss: 87.79   prob_loss: 0.57   total_loss: 89.99\n",
      "=> STEP 3898/6250   lr: 0.000367   giou_loss: 1.59   conf_loss: 87.77   prob_loss: 0.57   total_loss: 89.93\n",
      "=> STEP 3899/6250   lr: 0.000367   giou_loss: 1.59   conf_loss: 87.74   prob_loss: 0.58   total_loss: 89.91\n",
      "=> STEP 3900/6250   lr: 0.000366   giou_loss: 1.60   conf_loss: 87.72   prob_loss: 0.58   total_loss: 89.89\n",
      "=> STEP 3901/6250   lr: 0.000366   giou_loss: 1.57   conf_loss: 87.69   prob_loss: 0.57   total_loss: 89.83\n",
      "=> STEP 3902/6250   lr: 0.000366   giou_loss: 1.59   conf_loss: 87.67   prob_loss: 0.57   total_loss: 89.84\n",
      "=> STEP 3903/6250   lr: 0.000366   giou_loss: 1.57   conf_loss: 87.65   prob_loss: 0.57   total_loss: 89.79\n",
      "=> STEP 3904/6250   lr: 0.000366   giou_loss: 1.57   conf_loss: 87.62   prob_loss: 0.57   total_loss: 89.76\n",
      "=> STEP 3905/6250   lr: 0.000365   giou_loss: 1.57   conf_loss: 87.59   prob_loss: 0.57   total_loss: 89.73\n",
      "=> STEP 3906/6250   lr: 0.000365   giou_loss: 1.56   conf_loss: 87.57   prob_loss: 0.57   total_loss: 89.71\n",
      "=> STEP 3907/6250   lr: 0.000365   giou_loss: 1.58   conf_loss: 87.54   prob_loss: 0.58   total_loss: 89.69\n",
      "=> STEP 3908/6250   lr: 0.000365   giou_loss: 1.56   conf_loss: 87.51   prob_loss: 0.57   total_loss: 89.65\n",
      "=> STEP 3909/6250   lr: 0.000364   giou_loss: 1.66   conf_loss: 87.49   prob_loss: 0.57   total_loss: 89.72\n",
      "=> STEP 3910/6250   lr: 0.000364   giou_loss: 1.60   conf_loss: 87.47   prob_loss: 0.57   total_loss: 89.64\n",
      "=> STEP 3911/6250   lr: 0.000364   giou_loss: 1.58   conf_loss: 87.44   prob_loss: 0.57   total_loss: 89.59\n",
      "=> STEP 3912/6250   lr: 0.000364   giou_loss: 1.61   conf_loss: 87.41   prob_loss: 0.57   total_loss: 89.59\n",
      "=> STEP 3913/6250   lr: 0.000363   giou_loss: 1.56   conf_loss: 87.39   prob_loss: 0.57   total_loss: 89.53\n",
      "=> STEP 3914/6250   lr: 0.000363   giou_loss: 1.63   conf_loss: 87.37   prob_loss: 0.57   total_loss: 89.57\n",
      "=> STEP 3915/6250   lr: 0.000363   giou_loss: 1.61   conf_loss: 87.34   prob_loss: 0.57   total_loss: 89.52\n",
      "=> STEP 3916/6250   lr: 0.000363   giou_loss: 1.59   conf_loss: 87.31   prob_loss: 0.57   total_loss: 89.47\n",
      "=> STEP 3917/6250   lr: 0.000362   giou_loss: 1.66   conf_loss: 87.29   prob_loss: 0.57   total_loss: 89.53\n",
      "=> STEP 3918/6250   lr: 0.000362   giou_loss: 1.67   conf_loss: 87.27   prob_loss: 0.57   total_loss: 89.51\n",
      "=> STEP 3919/6250   lr: 0.000362   giou_loss: 1.59   conf_loss: 87.24   prob_loss: 0.57   total_loss: 89.40\n",
      "=> STEP 3920/6250   lr: 0.000362   giou_loss: 1.65   conf_loss: 87.22   prob_loss: 0.57   total_loss: 89.44\n",
      "=> STEP 3921/6250   lr: 0.000362   giou_loss: 1.70   conf_loss: 87.20   prob_loss: 0.57   total_loss: 89.46\n",
      "=> STEP 3922/6250   lr: 0.000361   giou_loss: 1.67   conf_loss: 87.17   prob_loss: 0.57   total_loss: 89.41\n",
      "=> STEP 3923/6250   lr: 0.000361   giou_loss: 1.56   conf_loss: 87.14   prob_loss: 0.57   total_loss: 89.28\n",
      "=> STEP 3924/6250   lr: 0.000361   giou_loss: 1.62   conf_loss: 87.11   prob_loss: 0.58   total_loss: 89.31\n",
      "=> STEP 3925/6250   lr: 0.000361   giou_loss: 1.60   conf_loss: 87.09   prob_loss: 0.58   total_loss: 89.27\n",
      "=> STEP 3926/6250   lr: 0.000360   giou_loss: 1.56   conf_loss: 87.07   prob_loss: 0.57   total_loss: 89.20\n",
      "=> STEP 3927/6250   lr: 0.000360   giou_loss: 1.61   conf_loss: 87.04   prob_loss: 0.57   total_loss: 89.23\n",
      "=> STEP 3928/6250   lr: 0.000360   giou_loss: 1.59   conf_loss: 87.02   prob_loss: 0.57   total_loss: 89.18\n",
      "=> STEP 3929/6250   lr: 0.000360   giou_loss: 1.58   conf_loss: 86.99   prob_loss: 0.57   total_loss: 89.14\n",
      "=> STEP 3930/6250   lr: 0.000359   giou_loss: 1.60   conf_loss: 86.97   prob_loss: 0.57   total_loss: 89.14\n",
      "=> STEP 3931/6250   lr: 0.000359   giou_loss: 1.56   conf_loss: 86.94   prob_loss: 0.57   total_loss: 89.08\n",
      "=> STEP 3932/6250   lr: 0.000359   giou_loss: 1.64   conf_loss: 86.92   prob_loss: 0.57   total_loss: 89.13\n",
      "=> STEP 3933/6250   lr: 0.000359   giou_loss: 1.68   conf_loss: 86.89   prob_loss: 0.57   total_loss: 89.15\n",
      "=> STEP 3934/6250   lr: 0.000358   giou_loss: 1.58   conf_loss: 86.86   prob_loss: 0.57   total_loss: 89.02\n",
      "=> STEP 3935/6250   lr: 0.000358   giou_loss: 1.63   conf_loss: 86.84   prob_loss: 0.57   total_loss: 89.05\n",
      "=> STEP 3936/6250   lr: 0.000358   giou_loss: 1.70   conf_loss: 86.82   prob_loss: 0.57   total_loss: 89.09\n",
      "=> STEP 3937/6250   lr: 0.000358   giou_loss: 1.64   conf_loss: 86.79   prob_loss: 0.57   total_loss: 89.00\n",
      "=> STEP 3938/6250   lr: 0.000358   giou_loss: 1.60   conf_loss: 86.78   prob_loss: 0.57   total_loss: 88.94\n",
      "=> STEP 3939/6250   lr: 0.000357   giou_loss: 1.58   conf_loss: 86.75   prob_loss: 0.57   total_loss: 88.90\n",
      "=> STEP 3940/6250   lr: 0.000357   giou_loss: 1.57   conf_loss: 86.72   prob_loss: 0.57   total_loss: 88.86\n",
      "=> STEP 3941/6250   lr: 0.000357   giou_loss: 1.62   conf_loss: 86.69   prob_loss: 0.58   total_loss: 88.89\n",
      "=> STEP 3942/6250   lr: 0.000357   giou_loss: 1.59   conf_loss: 86.67   prob_loss: 0.57   total_loss: 88.83\n",
      "=> STEP 3943/6250   lr: 0.000356   giou_loss: 1.62   conf_loss: 86.65   prob_loss: 0.57   total_loss: 88.85\n",
      "=> STEP 3944/6250   lr: 0.000356   giou_loss: 1.64   conf_loss: 86.63   prob_loss: 0.57   total_loss: 88.84\n",
      "=> STEP 3945/6250   lr: 0.000356   giou_loss: 1.64   conf_loss: 86.60   prob_loss: 0.57   total_loss: 88.81\n",
      "=> STEP 3946/6250   lr: 0.000356   giou_loss: 1.58   conf_loss: 86.58   prob_loss: 0.57   total_loss: 88.73\n",
      "=> STEP 3947/6250   lr: 0.000355   giou_loss: 1.57   conf_loss: 86.56   prob_loss: 0.57   total_loss: 88.70\n",
      "=> STEP 3948/6250   lr: 0.000355   giou_loss: 1.57   conf_loss: 86.53   prob_loss: 0.58   total_loss: 88.68\n",
      "=> STEP 3949/6250   lr: 0.000355   giou_loss: 1.56   conf_loss: 86.50   prob_loss: 0.58   total_loss: 88.64\n",
      "=> STEP 3950/6250   lr: 0.000355   giou_loss: 1.56   conf_loss: 86.48   prob_loss: 0.58   total_loss: 88.62\n",
      "=> STEP 3951/6250   lr: 0.000355   giou_loss: 1.56   conf_loss: 86.46   prob_loss: 0.57   total_loss: 88.60\n",
      "=> STEP 3952/6250   lr: 0.000354   giou_loss: 1.57   conf_loss: 86.43   prob_loss: 0.57   total_loss: 88.57\n",
      "=> STEP 3953/6250   lr: 0.000354   giou_loss: 1.57   conf_loss: 86.41   prob_loss: 0.57   total_loss: 88.54\n",
      "=> STEP 3954/6250   lr: 0.000354   giou_loss: 1.57   conf_loss: 86.39   prob_loss: 0.57   total_loss: 88.53\n",
      "=> STEP 3955/6250   lr: 0.000354   giou_loss: 1.57   conf_loss: 86.36   prob_loss: 0.57   total_loss: 88.49\n",
      "=> STEP 3956/6250   lr: 0.000353   giou_loss: 1.57   conf_loss: 86.33   prob_loss: 0.57   total_loss: 88.47\n",
      "=> STEP 3957/6250   lr: 0.000353   giou_loss: 1.57   conf_loss: 86.31   prob_loss: 0.57   total_loss: 88.44\n",
      "=> STEP 3958/6250   lr: 0.000353   giou_loss: 1.57   conf_loss: 86.28   prob_loss: 0.57   total_loss: 88.42\n",
      "=> STEP 3959/6250   lr: 0.000353   giou_loss: 1.57   conf_loss: 86.26   prob_loss: 0.57   total_loss: 88.39\n",
      "=> STEP 3960/6250   lr: 0.000352   giou_loss: 1.57   conf_loss: 86.23   prob_loss: 0.57   total_loss: 88.37\n",
      "=> STEP 3961/6250   lr: 0.000352   giou_loss: 1.57   conf_loss: 86.21   prob_loss: 0.57   total_loss: 88.34\n",
      "=> STEP 3962/6250   lr: 0.000352   giou_loss: 1.57   conf_loss: 86.18   prob_loss: 0.57   total_loss: 88.32\n",
      "=> STEP 3963/6250   lr: 0.000352   giou_loss: 1.57   conf_loss: 86.16   prob_loss: 0.57   total_loss: 88.29\n",
      "=> STEP 3964/6250   lr: 0.000352   giou_loss: 1.57   conf_loss: 86.13   prob_loss: 0.57   total_loss: 88.27\n",
      "=> STEP 3965/6250   lr: 0.000351   giou_loss: 1.57   conf_loss: 86.11   prob_loss: 0.57   total_loss: 88.24\n",
      "=> STEP 3966/6250   lr: 0.000351   giou_loss: 1.56   conf_loss: 86.09   prob_loss: 0.57   total_loss: 88.22\n",
      "=> STEP 3967/6250   lr: 0.000351   giou_loss: 1.56   conf_loss: 86.06   prob_loss: 0.57   total_loss: 88.19\n",
      "=> STEP 3968/6250   lr: 0.000351   giou_loss: 1.56   conf_loss: 86.04   prob_loss: 0.57   total_loss: 88.17\n",
      "=> STEP 3969/6250   lr: 0.000350   giou_loss: 1.56   conf_loss: 86.01   prob_loss: 0.57   total_loss: 88.14\n",
      "=> STEP 3970/6250   lr: 0.000350   giou_loss: 1.56   conf_loss: 85.99   prob_loss: 0.57   total_loss: 88.12\n",
      "=> STEP 3971/6250   lr: 0.000350   giou_loss: 1.56   conf_loss: 85.96   prob_loss: 0.57   total_loss: 88.10\n",
      "=> STEP 3972/6250   lr: 0.000350   giou_loss: 1.57   conf_loss: 85.94   prob_loss: 0.57   total_loss: 88.08\n",
      "=> STEP 3973/6250   lr: 0.000349   giou_loss: 1.56   conf_loss: 85.92   prob_loss: 0.57   total_loss: 88.05\n",
      "=> STEP 3974/6250   lr: 0.000349   giou_loss: 1.57   conf_loss: 85.89   prob_loss: 0.57   total_loss: 88.03\n",
      "=> STEP 3975/6250   lr: 0.000349   giou_loss: 1.57   conf_loss: 85.87   prob_loss: 0.57   total_loss: 88.00\n",
      "=> STEP 3976/6250   lr: 0.000349   giou_loss: 1.56   conf_loss: 85.84   prob_loss: 0.57   total_loss: 87.98\n",
      "=> STEP 3977/6250   lr: 0.000349   giou_loss: 1.56   conf_loss: 85.82   prob_loss: 0.57   total_loss: 87.95\n",
      "=> STEP 3978/6250   lr: 0.000348   giou_loss: 1.56   conf_loss: 85.80   prob_loss: 0.57   total_loss: 87.93\n",
      "=> STEP 3979/6250   lr: 0.000348   giou_loss: 1.57   conf_loss: 85.77   prob_loss: 0.57   total_loss: 87.91\n",
      "=> STEP 3980/6250   lr: 0.000348   giou_loss: 1.56   conf_loss: 85.75   prob_loss: 0.57   total_loss: 87.88\n",
      "=> STEP 3981/6250   lr: 0.000348   giou_loss: 1.57   conf_loss: 85.73   prob_loss: 0.57   total_loss: 87.86\n",
      "=> STEP 3982/6250   lr: 0.000347   giou_loss: 1.60   conf_loss: 85.70   prob_loss: 0.57   total_loss: 87.87\n",
      "=> STEP 3983/6250   lr: 0.000347   giou_loss: 1.57   conf_loss: 85.68   prob_loss: 0.57   total_loss: 87.82\n",
      "=> STEP 3984/6250   lr: 0.000347   giou_loss: 1.57   conf_loss: 85.66   prob_loss: 0.57   total_loss: 87.79\n",
      "=> STEP 3985/6250   lr: 0.000347   giou_loss: 1.56   conf_loss: 85.64   prob_loss: 0.56   total_loss: 87.77\n",
      "=> STEP 3986/6250   lr: 0.000346   giou_loss: 1.56   conf_loss: 85.62   prob_loss: 0.56   total_loss: 87.75\n",
      "=> STEP 3987/6250   lr: 0.000346   giou_loss: 1.58   conf_loss: 85.60   prob_loss: 0.57   total_loss: 87.74\n",
      "=> STEP 3988/6250   lr: 0.000346   giou_loss: 1.57   conf_loss: 85.57   prob_loss: 0.56   total_loss: 87.71\n",
      "=> STEP 3989/6250   lr: 0.000346   giou_loss: 1.57   conf_loss: 85.55   prob_loss: 0.57   total_loss: 87.68\n",
      "=> STEP 3990/6250   lr: 0.000346   giou_loss: 1.57   conf_loss: 85.52   prob_loss: 0.57   total_loss: 87.66\n",
      "=> STEP 3991/6250   lr: 0.000345   giou_loss: 1.56   conf_loss: 85.50   prob_loss: 0.57   total_loss: 87.63\n",
      "=> STEP 3992/6250   lr: 0.000345   giou_loss: 1.56   conf_loss: 85.48   prob_loss: 0.57   total_loss: 87.61\n",
      "=> STEP 3993/6250   lr: 0.000345   giou_loss: 1.58   conf_loss: 85.45   prob_loss: 0.57   total_loss: 87.59\n",
      "=> STEP 3994/6250   lr: 0.000345   giou_loss: 1.58   conf_loss: 85.42   prob_loss: 0.57   total_loss: 87.56\n",
      "=> STEP 3995/6250   lr: 0.000344   giou_loss: 1.57   conf_loss: 85.40   prob_loss: 0.56   total_loss: 87.53\n",
      "=> STEP 3996/6250   lr: 0.000344   giou_loss: 1.57   conf_loss: 85.38   prob_loss: 0.56   total_loss: 87.51\n",
      "=> STEP 3997/6250   lr: 0.000344   giou_loss: 1.59   conf_loss: 85.36   prob_loss: 0.56   total_loss: 87.51\n",
      "=> STEP 3998/6250   lr: 0.000344   giou_loss: 1.56   conf_loss: 85.33   prob_loss: 0.56   total_loss: 87.46\n",
      "=> STEP 3999/6250   lr: 0.000343   giou_loss: 1.57   conf_loss: 85.31   prob_loss: 0.56   total_loss: 87.44\n",
      "=> STEP 4000/6250   lr: 0.000343   giou_loss: 1.57   conf_loss: 85.29   prob_loss: 0.56   total_loss: 87.42\n",
      "=> STEP 4001/6250   lr: 0.000343   giou_loss: 1.63   conf_loss: 85.26   prob_loss: 0.57   total_loss: 87.46\n",
      "=> STEP 4002/6250   lr: 0.000343   giou_loss: 1.61   conf_loss: 85.24   prob_loss: 0.56   total_loss: 87.41\n",
      "=> STEP 4003/6250   lr: 0.000343   giou_loss: 1.57   conf_loss: 85.22   prob_loss: 0.56   total_loss: 87.34\n",
      "=> STEP 4004/6250   lr: 0.000342   giou_loss: 1.65   conf_loss: 85.19   prob_loss: 0.56   total_loss: 87.41\n",
      "=> STEP 4005/6250   lr: 0.000342   giou_loss: 1.66   conf_loss: 85.17   prob_loss: 0.56   total_loss: 87.39\n",
      "=> STEP 4006/6250   lr: 0.000342   giou_loss: 1.57   conf_loss: 85.14   prob_loss: 0.56   total_loss: 87.28\n",
      "=> STEP 4007/6250   lr: 0.000342   giou_loss: 1.65   conf_loss: 85.12   prob_loss: 0.57   total_loss: 87.34\n",
      "=> STEP 4008/6250   lr: 0.000341   giou_loss: 1.71   conf_loss: 85.10   prob_loss: 0.57   total_loss: 87.37\n",
      "=> STEP 4009/6250   lr: 0.000341   giou_loss: 1.68   conf_loss: 85.08   prob_loss: 0.56   total_loss: 87.32\n",
      "=> STEP 4010/6250   lr: 0.000341   giou_loss: 1.57   conf_loss: 85.06   prob_loss: 0.56   total_loss: 87.19\n",
      "=> STEP 4011/6250   lr: 0.000341   giou_loss: 1.67   conf_loss: 85.04   prob_loss: 0.56   total_loss: 87.27\n",
      "=> STEP 4012/6250   lr: 0.000340   giou_loss: 1.75   conf_loss: 85.02   prob_loss: 0.56   total_loss: 87.33\n",
      "=> STEP 4013/6250   lr: 0.000340   giou_loss: 1.74   conf_loss: 84.99   prob_loss: 0.56   total_loss: 87.29\n",
      "=> STEP 4014/6250   lr: 0.000340   giou_loss: 1.66   conf_loss: 84.96   prob_loss: 0.56   total_loss: 87.18\n",
      "=> STEP 4015/6250   lr: 0.000340   giou_loss: 1.57   conf_loss: 84.94   prob_loss: 0.56   total_loss: 87.07\n",
      "=> STEP 4016/6250   lr: 0.000340   giou_loss: 1.71   conf_loss: 84.91   prob_loss: 0.57   total_loss: 87.18\n",
      "=> STEP 4017/6250   lr: 0.000339   giou_loss: 1.75   conf_loss: 84.89   prob_loss: 0.57   total_loss: 87.20\n",
      "=> STEP 4018/6250   lr: 0.000339   giou_loss: 1.72   conf_loss: 84.86   prob_loss: 0.57   total_loss: 87.15\n",
      "=> STEP 4019/6250   lr: 0.000339   giou_loss: 1.61   conf_loss: 84.84   prob_loss: 0.57   total_loss: 87.02\n",
      "=> STEP 4020/6250   lr: 0.000339   giou_loss: 1.63   conf_loss: 84.82   prob_loss: 0.57   total_loss: 87.01\n",
      "=> STEP 4021/6250   lr: 0.000338   giou_loss: 1.70   conf_loss: 84.80   prob_loss: 0.57   total_loss: 87.07\n",
      "=> STEP 4022/6250   lr: 0.000338   giou_loss: 1.69   conf_loss: 84.77   prob_loss: 0.57   total_loss: 87.03\n",
      "=> STEP 4023/6250   lr: 0.000338   giou_loss: 1.60   conf_loss: 84.75   prob_loss: 0.57   total_loss: 86.91\n",
      "=> STEP 4024/6250   lr: 0.000338   giou_loss: 1.62   conf_loss: 84.72   prob_loss: 0.57   total_loss: 86.92\n",
      "=> STEP 4025/6250   lr: 0.000337   giou_loss: 1.69   conf_loss: 84.70   prob_loss: 0.57   total_loss: 86.96\n",
      "=> STEP 4026/6250   lr: 0.000337   giou_loss: 1.67   conf_loss: 84.68   prob_loss: 0.57   total_loss: 86.91\n",
      "=> STEP 4027/6250   lr: 0.000337   giou_loss: 1.57   conf_loss: 84.65   prob_loss: 0.57   total_loss: 86.79\n",
      "=> STEP 4028/6250   lr: 0.000337   giou_loss: 1.66   conf_loss: 84.63   prob_loss: 0.57   total_loss: 86.86\n",
      "=> STEP 4029/6250   lr: 0.000337   giou_loss: 1.73   conf_loss: 84.61   prob_loss: 0.57   total_loss: 86.91\n",
      "=> STEP 4030/6250   lr: 0.000336   giou_loss: 1.73   conf_loss: 84.59   prob_loss: 0.57   total_loss: 86.88\n",
      "=> STEP 4031/6250   lr: 0.000336   giou_loss: 1.63   conf_loss: 84.56   prob_loss: 0.57   total_loss: 86.76\n",
      "=> STEP 4032/6250   lr: 0.000336   giou_loss: 1.60   conf_loss: 84.54   prob_loss: 0.57   total_loss: 86.70\n",
      "=> STEP 4033/6250   lr: 0.000336   giou_loss: 1.65   conf_loss: 84.51   prob_loss: 0.57   total_loss: 86.73\n",
      "=> STEP 4034/6250   lr: 0.000335   giou_loss: 1.61   conf_loss: 84.49   prob_loss: 0.57   total_loss: 86.67\n",
      "=> STEP 4035/6250   lr: 0.000335   giou_loss: 1.56   conf_loss: 84.47   prob_loss: 0.57   total_loss: 86.60\n",
      "=> STEP 4036/6250   lr: 0.000335   giou_loss: 1.66   conf_loss: 84.45   prob_loss: 0.57   total_loss: 86.67\n",
      "=> STEP 4037/6250   lr: 0.000335   giou_loss: 1.66   conf_loss: 84.43   prob_loss: 0.57   total_loss: 86.65\n",
      "=> STEP 4038/6250   lr: 0.000335   giou_loss: 1.58   conf_loss: 84.40   prob_loss: 0.57   total_loss: 86.55\n",
      "=> STEP 4039/6250   lr: 0.000334   giou_loss: 1.67   conf_loss: 84.38   prob_loss: 0.57   total_loss: 86.62\n",
      "=> STEP 4040/6250   lr: 0.000334   giou_loss: 1.69   conf_loss: 84.35   prob_loss: 0.57   total_loss: 86.61\n",
      "=> STEP 4041/6250   lr: 0.000334   giou_loss: 1.66   conf_loss: 84.33   prob_loss: 0.57   total_loss: 86.56\n",
      "=> STEP 4042/6250   lr: 0.000334   giou_loss: 1.56   conf_loss: 84.31   prob_loss: 0.57   total_loss: 86.45\n",
      "=> STEP 4043/6250   lr: 0.000333   giou_loss: 1.60   conf_loss: 84.29   prob_loss: 0.57   total_loss: 86.46\n",
      "=> STEP 4044/6250   lr: 0.000333   giou_loss: 1.59   conf_loss: 84.27   prob_loss: 0.57   total_loss: 86.43\n",
      "=> STEP 4045/6250   lr: 0.000333   giou_loss: 1.57   conf_loss: 84.25   prob_loss: 0.57   total_loss: 86.38\n",
      "=> STEP 4046/6250   lr: 0.000333   giou_loss: 1.65   conf_loss: 84.22   prob_loss: 0.57   total_loss: 86.44\n",
      "=> STEP 4047/6250   lr: 0.000332   giou_loss: 1.62   conf_loss: 84.20   prob_loss: 0.57   total_loss: 86.38\n",
      "=> STEP 4048/6250   lr: 0.000332   giou_loss: 1.56   conf_loss: 84.18   prob_loss: 0.57   total_loss: 86.31\n",
      "=> STEP 4049/6250   lr: 0.000332   giou_loss: 1.62   conf_loss: 84.16   prob_loss: 0.57   total_loss: 86.35\n",
      "=> STEP 4050/6250   lr: 0.000332   giou_loss: 1.64   conf_loss: 84.13   prob_loss: 0.57   total_loss: 86.34\n",
      "=> STEP 4051/6250   lr: 0.000332   giou_loss: 1.56   conf_loss: 84.11   prob_loss: 0.57   total_loss: 86.24\n",
      "=> STEP 4052/6250   lr: 0.000331   giou_loss: 1.61   conf_loss: 84.09   prob_loss: 0.57   total_loss: 86.27\n",
      "=> STEP 4053/6250   lr: 0.000331   giou_loss: 1.60   conf_loss: 84.07   prob_loss: 0.57   total_loss: 86.23\n",
      "=> STEP 4054/6250   lr: 0.000331   giou_loss: 1.57   conf_loss: 84.04   prob_loss: 0.56   total_loss: 86.18\n",
      "=> STEP 4055/6250   lr: 0.000331   giou_loss: 1.64   conf_loss: 84.02   prob_loss: 0.56   total_loss: 86.22\n",
      "=> STEP 4056/6250   lr: 0.000330   giou_loss: 1.63   conf_loss: 84.00   prob_loss: 0.57   total_loss: 86.20\n",
      "=> STEP 4057/6250   lr: 0.000330   giou_loss: 1.59   conf_loss: 83.97   prob_loss: 0.57   total_loss: 86.13\n",
      "=> STEP 4058/6250   lr: 0.000330   giou_loss: 1.58   conf_loss: 83.95   prob_loss: 0.57   total_loss: 86.10\n",
      "=> STEP 4059/6250   lr: 0.000330   giou_loss: 1.57   conf_loss: 83.93   prob_loss: 0.56   total_loss: 86.07\n",
      "=> STEP 4060/6250   lr: 0.000330   giou_loss: 1.58   conf_loss: 83.91   prob_loss: 0.56   total_loss: 86.05\n",
      "=> STEP 4061/6250   lr: 0.000329   giou_loss: 1.58   conf_loss: 83.89   prob_loss: 0.56   total_loss: 86.03\n",
      "=> STEP 4062/6250   lr: 0.000329   giou_loss: 1.57   conf_loss: 83.87   prob_loss: 0.56   total_loss: 86.00\n",
      "=> STEP 4063/6250   lr: 0.000329   giou_loss: 1.65   conf_loss: 83.84   prob_loss: 0.56   total_loss: 86.05\n",
      "=> STEP 4064/6250   lr: 0.000329   giou_loss: 1.61   conf_loss: 83.82   prob_loss: 0.56   total_loss: 86.00\n",
      "=> STEP 4065/6250   lr: 0.000328   giou_loss: 1.56   conf_loss: 83.80   prob_loss: 0.56   total_loss: 85.92\n",
      "=> STEP 4066/6250   lr: 0.000328   giou_loss: 1.68   conf_loss: 83.77   prob_loss: 0.57   total_loss: 86.02\n",
      "=> STEP 4067/6250   lr: 0.000328   giou_loss: 1.64   conf_loss: 83.75   prob_loss: 0.57   total_loss: 85.96\n",
      "=> STEP 4068/6250   lr: 0.000328   giou_loss: 1.57   conf_loss: 83.73   prob_loss: 0.57   total_loss: 85.86\n",
      "=> STEP 4069/6250   lr: 0.000327   giou_loss: 1.66   conf_loss: 83.71   prob_loss: 0.56   total_loss: 85.93\n",
      "=> STEP 4070/6250   lr: 0.000327   giou_loss: 1.65   conf_loss: 83.69   prob_loss: 0.56   total_loss: 85.90\n",
      "=> STEP 4071/6250   lr: 0.000327   giou_loss: 1.57   conf_loss: 83.67   prob_loss: 0.56   total_loss: 85.80\n",
      "=> STEP 4072/6250   lr: 0.000327   giou_loss: 1.65   conf_loss: 83.65   prob_loss: 0.56   total_loss: 85.86\n",
      "=> STEP 4073/6250   lr: 0.000327   giou_loss: 1.68   conf_loss: 83.62   prob_loss: 0.56   total_loss: 85.86\n",
      "=> STEP 4074/6250   lr: 0.000326   giou_loss: 1.56   conf_loss: 83.60   prob_loss: 0.57   total_loss: 85.73\n",
      "=> STEP 4075/6250   lr: 0.000326   giou_loss: 1.66   conf_loss: 83.58   prob_loss: 0.57   total_loss: 85.80\n",
      "=> STEP 4076/6250   lr: 0.000326   giou_loss: 1.79   conf_loss: 83.56   prob_loss: 0.57   total_loss: 85.92\n",
      "=> STEP 4077/6250   lr: 0.000326   giou_loss: 1.79   conf_loss: 83.53   prob_loss: 0.56   total_loss: 85.88\n",
      "=> STEP 4078/6250   lr: 0.000325   giou_loss: 1.64   conf_loss: 83.51   prob_loss: 0.56   total_loss: 85.72\n",
      "=> STEP 4079/6250   lr: 0.000325   giou_loss: 1.59   conf_loss: 83.50   prob_loss: 0.56   total_loss: 85.65\n",
      "=> STEP 4080/6250   lr: 0.000325   giou_loss: 1.76   conf_loss: 83.47   prob_loss: 0.57   total_loss: 85.80\n",
      "=> STEP 4081/6250   lr: 0.000325   giou_loss: 1.76   conf_loss: 83.45   prob_loss: 0.57   total_loss: 85.77\n",
      "=> STEP 4082/6250   lr: 0.000325   giou_loss: 1.66   conf_loss: 83.43   prob_loss: 0.57   total_loss: 85.65\n",
      "=> STEP 4083/6250   lr: 0.000324   giou_loss: 1.61   conf_loss: 83.41   prob_loss: 0.56   total_loss: 85.59\n",
      "=> STEP 4084/6250   lr: 0.000324   giou_loss: 1.62   conf_loss: 83.39   prob_loss: 0.56   total_loss: 85.57\n",
      "=> STEP 4085/6250   lr: 0.000324   giou_loss: 1.64   conf_loss: 83.37   prob_loss: 0.56   total_loss: 85.57\n",
      "=> STEP 4086/6250   lr: 0.000324   giou_loss: 1.66   conf_loss: 83.35   prob_loss: 0.56   total_loss: 85.57\n",
      "=> STEP 4087/6250   lr: 0.000323   giou_loss: 1.59   conf_loss: 83.34   prob_loss: 0.56   total_loss: 85.49\n",
      "=> STEP 4088/6250   lr: 0.000323   giou_loss: 1.56   conf_loss: 83.30   prob_loss: 0.57   total_loss: 85.43\n",
      "=> STEP 4089/6250   lr: 0.000323   giou_loss: 1.63   conf_loss: 83.28   prob_loss: 0.57   total_loss: 85.48\n",
      "=> STEP 4090/6250   lr: 0.000323   giou_loss: 1.67   conf_loss: 83.26   prob_loss: 0.57   total_loss: 85.49\n",
      "=> STEP 4091/6250   lr: 0.000323   giou_loss: 1.59   conf_loss: 83.24   prob_loss: 0.57   total_loss: 85.40\n",
      "=> STEP 4092/6250   lr: 0.000322   giou_loss: 1.58   conf_loss: 83.23   prob_loss: 0.57   total_loss: 85.38\n",
      "=> STEP 4093/6250   lr: 0.000322   giou_loss: 1.59   conf_loss: 83.20   prob_loss: 0.56   total_loss: 85.35\n",
      "=> STEP 4094/6250   lr: 0.000322   giou_loss: 1.58   conf_loss: 83.18   prob_loss: 0.56   total_loss: 85.32\n",
      "=> STEP 4095/6250   lr: 0.000322   giou_loss: 1.56   conf_loss: 83.16   prob_loss: 0.56   total_loss: 85.29\n",
      "=> STEP 4096/6250   lr: 0.000321   giou_loss: 1.57   conf_loss: 83.14   prob_loss: 0.56   total_loss: 85.27\n",
      "=> STEP 4097/6250   lr: 0.000321   giou_loss: 1.56   conf_loss: 83.11   prob_loss: 0.56   total_loss: 85.23\n",
      "=> STEP 4098/6250   lr: 0.000321   giou_loss: 1.56   conf_loss: 83.08   prob_loss: 0.56   total_loss: 85.21\n",
      "=> STEP 4099/6250   lr: 0.000321   giou_loss: 1.56   conf_loss: 83.06   prob_loss: 0.56   total_loss: 85.19\n",
      "=> STEP 4100/6250   lr: 0.000321   giou_loss: 1.56   conf_loss: 83.04   prob_loss: 0.56   total_loss: 85.17\n",
      "=> STEP 4101/6250   lr: 0.000320   giou_loss: 1.56   conf_loss: 83.02   prob_loss: 0.56   total_loss: 85.14\n",
      "=> STEP 4102/6250   lr: 0.000320   giou_loss: 1.56   conf_loss: 83.00   prob_loss: 0.56   total_loss: 85.12\n",
      "=> STEP 4103/6250   lr: 0.000320   giou_loss: 1.56   conf_loss: 82.98   prob_loss: 0.56   total_loss: 85.10\n",
      "=> STEP 4104/6250   lr: 0.000320   giou_loss: 1.56   conf_loss: 82.95   prob_loss: 0.56   total_loss: 85.08\n",
      "=> STEP 4105/6250   lr: 0.000319   giou_loss: 1.56   conf_loss: 82.93   prob_loss: 0.56   total_loss: 85.05\n",
      "=> STEP 4106/6250   lr: 0.000319   giou_loss: 1.56   conf_loss: 82.91   prob_loss: 0.56   total_loss: 85.03\n",
      "=> STEP 4107/6250   lr: 0.000319   giou_loss: 1.56   conf_loss: 82.88   prob_loss: 0.56   total_loss: 85.01\n",
      "=> STEP 4108/6250   lr: 0.000319   giou_loss: 1.56   conf_loss: 82.86   prob_loss: 0.56   total_loss: 84.99\n",
      "=> STEP 4109/6250   lr: 0.000318   giou_loss: 1.56   conf_loss: 82.84   prob_loss: 0.56   total_loss: 84.96\n",
      "=> STEP 4110/6250   lr: 0.000318   giou_loss: 1.56   conf_loss: 82.82   prob_loss: 0.56   total_loss: 84.94\n",
      "=> STEP 4111/6250   lr: 0.000318   giou_loss: 1.56   conf_loss: 82.80   prob_loss: 0.56   total_loss: 84.92\n",
      "=> STEP 4112/6250   lr: 0.000318   giou_loss: 1.57   conf_loss: 82.77   prob_loss: 0.56   total_loss: 84.90\n",
      "=> STEP 4113/6250   lr: 0.000318   giou_loss: 1.56   conf_loss: 82.75   prob_loss: 0.56   total_loss: 84.88\n",
      "=> STEP 4114/6250   lr: 0.000317   giou_loss: 1.57   conf_loss: 82.73   prob_loss: 0.56   total_loss: 84.86\n",
      "=> STEP 4115/6250   lr: 0.000317   giou_loss: 1.62   conf_loss: 82.71   prob_loss: 0.56   total_loss: 84.89\n",
      "=> STEP 4116/6250   lr: 0.000317   giou_loss: 1.59   conf_loss: 82.69   prob_loss: 0.56   total_loss: 84.83\n",
      "=> STEP 4117/6250   lr: 0.000317   giou_loss: 1.57   conf_loss: 82.67   prob_loss: 0.56   total_loss: 84.80\n",
      "=> STEP 4118/6250   lr: 0.000316   giou_loss: 1.61   conf_loss: 82.65   prob_loss: 0.56   total_loss: 84.82\n",
      "=> STEP 4119/6250   lr: 0.000316   giou_loss: 1.67   conf_loss: 82.62   prob_loss: 0.56   total_loss: 84.86\n",
      "=> STEP 4120/6250   lr: 0.000316   giou_loss: 1.61   conf_loss: 82.61   prob_loss: 0.56   total_loss: 84.78\n",
      "=> STEP 4121/6250   lr: 0.000316   giou_loss: 1.56   conf_loss: 82.59   prob_loss: 0.56   total_loss: 84.71\n",
      "=> STEP 4122/6250   lr: 0.000316   giou_loss: 1.62   conf_loss: 82.57   prob_loss: 0.56   total_loss: 84.75\n",
      "=> STEP 4123/6250   lr: 0.000315   giou_loss: 1.64   conf_loss: 82.55   prob_loss: 0.56   total_loss: 84.74\n",
      "=> STEP 4124/6250   lr: 0.000315   giou_loss: 1.57   conf_loss: 82.52   prob_loss: 0.56   total_loss: 84.65\n",
      "=> STEP 4125/6250   lr: 0.000315   giou_loss: 1.60   conf_loss: 82.50   prob_loss: 0.56   total_loss: 84.67\n",
      "=> STEP 4126/6250   lr: 0.000315   giou_loss: 1.58   conf_loss: 82.48   prob_loss: 0.56   total_loss: 84.62\n",
      "=> STEP 4127/6250   lr: 0.000314   giou_loss: 1.57   conf_loss: 82.46   prob_loss: 0.56   total_loss: 84.59\n",
      "=> STEP 4128/6250   lr: 0.000314   giou_loss: 1.58   conf_loss: 82.44   prob_loss: 0.56   total_loss: 84.58\n",
      "=> STEP 4129/6250   lr: 0.000314   giou_loss: 1.57   conf_loss: 82.42   prob_loss: 0.56   total_loss: 84.55\n",
      "=> STEP 4130/6250   lr: 0.000314   giou_loss: 1.57   conf_loss: 82.40   prob_loss: 0.56   total_loss: 84.52\n",
      "=> STEP 4131/6250   lr: 0.000314   giou_loss: 1.57   conf_loss: 82.38   prob_loss: 0.56   total_loss: 84.51\n",
      "=> STEP 4132/6250   lr: 0.000313   giou_loss: 1.57   conf_loss: 82.35   prob_loss: 0.56   total_loss: 84.48\n",
      "=> STEP 4133/6250   lr: 0.000313   giou_loss: 1.56   conf_loss: 82.33   prob_loss: 0.56   total_loss: 84.46\n",
      "=> STEP 4134/6250   lr: 0.000313   giou_loss: 1.56   conf_loss: 82.31   prob_loss: 0.56   total_loss: 84.44\n",
      "=> STEP 4135/6250   lr: 0.000313   giou_loss: 1.59   conf_loss: 82.29   prob_loss: 0.56   total_loss: 84.44\n",
      "=> STEP 4136/6250   lr: 0.000312   giou_loss: 1.56   conf_loss: 82.27   prob_loss: 0.56   total_loss: 84.39\n",
      "=> STEP 4137/6250   lr: 0.000312   giou_loss: 1.56   conf_loss: 82.25   prob_loss: 0.56   total_loss: 84.37\n",
      "=> STEP 4138/6250   lr: 0.000312   giou_loss: 1.56   conf_loss: 82.23   prob_loss: 0.56   total_loss: 84.35\n",
      "=> STEP 4139/6250   lr: 0.000312   giou_loss: 1.57   conf_loss: 82.21   prob_loss: 0.56   total_loss: 84.33\n",
      "=> STEP 4140/6250   lr: 0.000312   giou_loss: 1.59   conf_loss: 82.19   prob_loss: 0.55   total_loss: 84.33\n",
      "=> STEP 4141/6250   lr: 0.000311   giou_loss: 1.57   conf_loss: 82.17   prob_loss: 0.56   total_loss: 84.29\n",
      "=> STEP 4142/6250   lr: 0.000311   giou_loss: 1.62   conf_loss: 82.14   prob_loss: 0.56   total_loss: 84.32\n",
      "=> STEP 4143/6250   lr: 0.000311   giou_loss: 1.59   conf_loss: 82.12   prob_loss: 0.56   total_loss: 84.27\n",
      "=> STEP 4144/6250   lr: 0.000311   giou_loss: 1.62   conf_loss: 82.10   prob_loss: 0.56   total_loss: 84.27\n",
      "=> STEP 4145/6250   lr: 0.000310   giou_loss: 1.58   conf_loss: 82.08   prob_loss: 0.56   total_loss: 84.22\n",
      "=> STEP 4146/6250   lr: 0.000310   giou_loss: 1.57   conf_loss: 82.06   prob_loss: 0.56   total_loss: 84.19\n",
      "=> STEP 4147/6250   lr: 0.000310   giou_loss: 1.64   conf_loss: 82.04   prob_loss: 0.56   total_loss: 84.23\n",
      "=> STEP 4148/6250   lr: 0.000310   giou_loss: 1.65   conf_loss: 82.02   prob_loss: 0.56   total_loss: 84.22\n",
      "=> STEP 4149/6250   lr: 0.000310   giou_loss: 1.58   conf_loss: 82.00   prob_loss: 0.56   total_loss: 84.13\n",
      "=> STEP 4150/6250   lr: 0.000309   giou_loss: 1.56   conf_loss: 81.98   prob_loss: 0.56   total_loss: 84.10\n",
      "=> STEP 4151/6250   lr: 0.000309   giou_loss: 1.68   conf_loss: 81.96   prob_loss: 0.56   total_loss: 84.20\n",
      "=> STEP 4152/6250   lr: 0.000309   giou_loss: 1.70   conf_loss: 81.93   prob_loss: 0.56   total_loss: 84.20\n",
      "=> STEP 4153/6250   lr: 0.000309   giou_loss: 1.70   conf_loss: 81.92   prob_loss: 0.56   total_loss: 84.18\n",
      "=> STEP 4154/6250   lr: 0.000308   giou_loss: 1.59   conf_loss: 81.91   prob_loss: 0.56   total_loss: 84.06\n",
      "=> STEP 4155/6250   lr: 0.000308   giou_loss: 1.56   conf_loss: 81.88   prob_loss: 0.56   total_loss: 84.01\n",
      "=> STEP 4156/6250   lr: 0.000308   giou_loss: 1.68   conf_loss: 81.86   prob_loss: 0.55   total_loss: 84.09\n",
      "=> STEP 4157/6250   lr: 0.000308   giou_loss: 1.68   conf_loss: 81.85   prob_loss: 0.55   total_loss: 84.07\n",
      "=> STEP 4158/6250   lr: 0.000308   giou_loss: 1.57   conf_loss: 81.83   prob_loss: 0.55   total_loss: 83.96\n",
      "=> STEP 4159/6250   lr: 0.000307   giou_loss: 1.62   conf_loss: 81.81   prob_loss: 0.56   total_loss: 83.99\n",
      "=> STEP 4160/6250   lr: 0.000307   giou_loss: 1.67   conf_loss: 81.78   prob_loss: 0.56   total_loss: 84.01\n",
      "=> STEP 4161/6250   lr: 0.000307   giou_loss: 1.64   conf_loss: 81.76   prob_loss: 0.56   total_loss: 83.96\n",
      "=> STEP 4162/6250   lr: 0.000307   giou_loss: 1.56   conf_loss: 81.75   prob_loss: 0.56   total_loss: 83.87\n",
      "=> STEP 4163/6250   lr: 0.000306   giou_loss: 1.61   conf_loss: 81.73   prob_loss: 0.56   total_loss: 83.90\n",
      "=> STEP 4164/6250   lr: 0.000306   giou_loss: 1.61   conf_loss: 81.70   prob_loss: 0.56   total_loss: 83.88\n",
      "=> STEP 4165/6250   lr: 0.000306   giou_loss: 1.56   conf_loss: 81.68   prob_loss: 0.56   total_loss: 83.80\n",
      "=> STEP 4166/6250   lr: 0.000306   giou_loss: 1.58   conf_loss: 81.66   prob_loss: 0.56   total_loss: 83.80\n",
      "=> STEP 4167/6250   lr: 0.000306   giou_loss: 1.56   conf_loss: 81.64   prob_loss: 0.56   total_loss: 83.76\n",
      "=> STEP 4168/6250   lr: 0.000305   giou_loss: 1.56   conf_loss: 81.61   prob_loss: 0.56   total_loss: 83.74\n",
      "=> STEP 4169/6250   lr: 0.000305   giou_loss: 1.56   conf_loss: 81.59   prob_loss: 0.56   total_loss: 83.72\n",
      "=> STEP 4170/6250   lr: 0.000305   giou_loss: 1.56   conf_loss: 81.57   prob_loss: 0.56   total_loss: 83.69\n",
      "=> STEP 4171/6250   lr: 0.000305   giou_loss: 1.56   conf_loss: 81.55   prob_loss: 0.56   total_loss: 83.67\n",
      "=> STEP 4172/6250   lr: 0.000304   giou_loss: 1.57   conf_loss: 81.53   prob_loss: 0.56   total_loss: 83.66\n",
      "=> STEP 4173/6250   lr: 0.000304   giou_loss: 1.56   conf_loss: 81.51   prob_loss: 0.56   total_loss: 83.63\n",
      "=> STEP 4174/6250   lr: 0.000304   giou_loss: 1.61   conf_loss: 81.49   prob_loss: 0.56   total_loss: 83.65\n",
      "=> STEP 4175/6250   lr: 0.000304   giou_loss: 1.58   conf_loss: 81.46   prob_loss: 0.56   total_loss: 83.60\n",
      "=> STEP 4176/6250   lr: 0.000304   giou_loss: 1.58   conf_loss: 81.45   prob_loss: 0.56   total_loss: 83.58\n",
      "=> STEP 4177/6250   lr: 0.000303   giou_loss: 1.58   conf_loss: 81.43   prob_loss: 0.56   total_loss: 83.56\n",
      "=> STEP 4178/6250   lr: 0.000303   giou_loss: 1.57   conf_loss: 81.40   prob_loss: 0.56   total_loss: 83.53\n",
      "=> STEP 4179/6250   lr: 0.000303   giou_loss: 1.62   conf_loss: 81.38   prob_loss: 0.56   total_loss: 83.56\n",
      "=> STEP 4180/6250   lr: 0.000303   giou_loss: 1.59   conf_loss: 81.36   prob_loss: 0.56   total_loss: 83.51\n",
      "=> STEP 4181/6250   lr: 0.000303   giou_loss: 1.57   conf_loss: 81.35   prob_loss: 0.56   total_loss: 83.47\n",
      "=> STEP 4182/6250   lr: 0.000302   giou_loss: 1.58   conf_loss: 81.32   prob_loss: 0.56   total_loss: 83.46\n",
      "=> STEP 4183/6250   lr: 0.000302   giou_loss: 1.56   conf_loss: 81.30   prob_loss: 0.56   total_loss: 83.42\n",
      "=> STEP 4184/6250   lr: 0.000302   giou_loss: 1.62   conf_loss: 81.28   prob_loss: 0.56   total_loss: 83.46\n",
      "=> STEP 4185/6250   lr: 0.000302   giou_loss: 1.60   conf_loss: 81.26   prob_loss: 0.56   total_loss: 83.42\n",
      "=> STEP 4186/6250   lr: 0.000301   giou_loss: 1.56   conf_loss: 81.25   prob_loss: 0.56   total_loss: 83.37\n",
      "=> STEP 4187/6250   lr: 0.000301   giou_loss: 1.63   conf_loss: 81.22   prob_loss: 0.55   total_loss: 83.41\n",
      "=> STEP 4188/6250   lr: 0.000301   giou_loss: 1.63   conf_loss: 81.20   prob_loss: 0.55   total_loss: 83.39\n",
      "=> STEP 4189/6250   lr: 0.000301   giou_loss: 1.61   conf_loss: 81.19   prob_loss: 0.56   total_loss: 83.35\n",
      "=> STEP 4190/6250   lr: 0.000301   giou_loss: 1.58   conf_loss: 81.16   prob_loss: 0.56   total_loss: 83.30\n",
      "=> STEP 4191/6250   lr: 0.000300   giou_loss: 1.56   conf_loss: 81.14   prob_loss: 0.56   total_loss: 83.26\n",
      "=> STEP 4192/6250   lr: 0.000300   giou_loss: 1.64   conf_loss: 81.12   prob_loss: 0.56   total_loss: 83.33\n",
      "=> STEP 4193/6250   lr: 0.000300   giou_loss: 1.66   conf_loss: 81.10   prob_loss: 0.56   total_loss: 83.32\n",
      "=> STEP 4194/6250   lr: 0.000300   giou_loss: 1.61   conf_loss: 81.08   prob_loss: 0.56   total_loss: 83.25\n",
      "=> STEP 4195/6250   lr: 0.000299   giou_loss: 1.56   conf_loss: 81.07   prob_loss: 0.56   total_loss: 83.19\n",
      "=> STEP 4196/6250   lr: 0.000299   giou_loss: 1.62   conf_loss: 81.04   prob_loss: 0.55   total_loss: 83.22\n",
      "=> STEP 4197/6250   lr: 0.000299   giou_loss: 1.65   conf_loss: 81.02   prob_loss: 0.55   total_loss: 83.23\n",
      "=> STEP 4198/6250   lr: 0.000299   giou_loss: 1.59   conf_loss: 81.00   prob_loss: 0.55   total_loss: 83.15\n",
      "=> STEP 4199/6250   lr: 0.000299   giou_loss: 1.59   conf_loss: 80.99   prob_loss: 0.56   total_loss: 83.14\n",
      "=> STEP 4200/6250   lr: 0.000298   giou_loss: 1.63   conf_loss: 80.96   prob_loss: 0.56   total_loss: 83.15\n",
      "=> STEP 4201/6250   lr: 0.000298   giou_loss: 1.65   conf_loss: 80.94   prob_loss: 0.56   total_loss: 83.15\n",
      "=> STEP 4202/6250   lr: 0.000298   giou_loss: 1.59   conf_loss: 80.93   prob_loss: 0.56   total_loss: 83.08\n",
      "=> STEP 4203/6250   lr: 0.000298   giou_loss: 1.56   conf_loss: 80.92   prob_loss: 0.56   total_loss: 83.04\n",
      "=> STEP 4204/6250   lr: 0.000297   giou_loss: 1.64   conf_loss: 80.90   prob_loss: 0.55   total_loss: 83.09\n",
      "=> STEP 4205/6250   lr: 0.000297   giou_loss: 1.65   conf_loss: 80.87   prob_loss: 0.55   total_loss: 83.08\n",
      "=> STEP 4206/6250   lr: 0.000297   giou_loss: 1.62   conf_loss: 80.85   prob_loss: 0.55   total_loss: 83.03\n",
      "=> STEP 4207/6250   lr: 0.000297   giou_loss: 1.56   conf_loss: 80.84   prob_loss: 0.56   total_loss: 82.96\n",
      "=> STEP 4208/6250   lr: 0.000297   giou_loss: 1.63   conf_loss: 80.82   prob_loss: 0.56   total_loss: 83.01\n",
      "=> STEP 4209/6250   lr: 0.000296   giou_loss: 1.66   conf_loss: 80.78   prob_loss: 0.56   total_loss: 83.01\n",
      "=> STEP 4210/6250   lr: 0.000296   giou_loss: 1.60   conf_loss: 80.77   prob_loss: 0.56   total_loss: 82.93\n",
      "=> STEP 4211/6250   lr: 0.000296   giou_loss: 1.60   conf_loss: 80.76   prob_loss: 0.56   total_loss: 82.92\n",
      "=> STEP 4212/6250   lr: 0.000296   giou_loss: 1.64   conf_loss: 80.73   prob_loss: 0.56   total_loss: 82.93\n",
      "=> STEP 4213/6250   lr: 0.000296   giou_loss: 1.65   conf_loss: 80.71   prob_loss: 0.55   total_loss: 82.92\n",
      "=> STEP 4214/6250   lr: 0.000295   giou_loss: 1.62   conf_loss: 80.69   prob_loss: 0.55   total_loss: 82.86\n",
      "=> STEP 4215/6250   lr: 0.000295   giou_loss: 1.56   conf_loss: 80.69   prob_loss: 0.55   total_loss: 82.81\n",
      "=> STEP 4216/6250   lr: 0.000295   giou_loss: 1.64   conf_loss: 80.66   prob_loss: 0.56   total_loss: 82.85\n",
      "=> STEP 4217/6250   lr: 0.000295   giou_loss: 1.66   conf_loss: 80.62   prob_loss: 0.56   total_loss: 82.84\n",
      "=> STEP 4218/6250   lr: 0.000294   giou_loss: 1.60   conf_loss: 80.62   prob_loss: 0.56   total_loss: 82.77\n",
      "=> STEP 4219/6250   lr: 0.000294   giou_loss: 1.56   conf_loss: 80.61   prob_loss: 0.56   total_loss: 82.73\n",
      "=> STEP 4220/6250   lr: 0.000294   giou_loss: 1.62   conf_loss: 80.58   prob_loss: 0.56   total_loss: 82.75\n",
      "=> STEP 4221/6250   lr: 0.000294   giou_loss: 1.63   conf_loss: 80.55   prob_loss: 0.55   total_loss: 82.74\n",
      "=> STEP 4222/6250   lr: 0.000294   giou_loss: 1.60   conf_loss: 80.54   prob_loss: 0.55   total_loss: 82.69\n",
      "=> STEP 4223/6250   lr: 0.000293   giou_loss: 1.56   conf_loss: 80.53   prob_loss: 0.56   total_loss: 82.65\n",
      "=> STEP 4224/6250   lr: 0.000293   giou_loss: 1.65   conf_loss: 80.50   prob_loss: 0.56   total_loss: 82.71\n",
      "=> STEP 4225/6250   lr: 0.000293   giou_loss: 1.67   conf_loss: 80.47   prob_loss: 0.56   total_loss: 82.70\n",
      "=> STEP 4226/6250   lr: 0.000293   giou_loss: 1.61   conf_loss: 80.45   prob_loss: 0.56   total_loss: 82.63\n",
      "=> STEP 4227/6250   lr: 0.000292   giou_loss: 1.56   conf_loss: 80.45   prob_loss: 0.56   total_loss: 82.57\n",
      "=> STEP 4228/6250   lr: 0.000292   giou_loss: 1.59   conf_loss: 80.42   prob_loss: 0.56   total_loss: 82.57\n",
      "=> STEP 4229/6250   lr: 0.000292   giou_loss: 1.60   conf_loss: 80.39   prob_loss: 0.56   total_loss: 82.55\n",
      "=> STEP 4230/6250   lr: 0.000292   giou_loss: 1.57   conf_loss: 80.38   prob_loss: 0.55   total_loss: 82.50\n",
      "=> STEP 4231/6250   lr: 0.000292   giou_loss: 1.58   conf_loss: 80.36   prob_loss: 0.55   total_loss: 82.49\n",
      "=> STEP 4232/6250   lr: 0.000291   giou_loss: 1.56   conf_loss: 80.34   prob_loss: 0.55   total_loss: 82.45\n",
      "=> STEP 4233/6250   lr: 0.000291   giou_loss: 1.60   conf_loss: 80.31   prob_loss: 0.56   total_loss: 82.46\n",
      "=> STEP 4234/6250   lr: 0.000291   giou_loss: 1.59   conf_loss: 80.28   prob_loss: 0.56   total_loss: 82.43\n",
      "=> STEP 4235/6250   lr: 0.000291   giou_loss: 1.57   conf_loss: 80.27   prob_loss: 0.56   total_loss: 82.39\n",
      "=> STEP 4236/6250   lr: 0.000291   giou_loss: 1.66   conf_loss: 80.25   prob_loss: 0.56   total_loss: 82.46\n",
      "=> STEP 4237/6250   lr: 0.000290   giou_loss: 1.64   conf_loss: 80.21   prob_loss: 0.55   total_loss: 82.40\n",
      "=> STEP 4238/6250   lr: 0.000290   giou_loss: 1.57   conf_loss: 80.19   prob_loss: 0.55   total_loss: 82.31\n",
      "=> STEP 4239/6250   lr: 0.000290   giou_loss: 1.58   conf_loss: 80.17   prob_loss: 0.55   total_loss: 82.30\n",
      "=> STEP 4240/6250   lr: 0.000290   giou_loss: 1.67   conf_loss: 80.15   prob_loss: 0.56   total_loss: 82.37\n",
      "=> STEP 4241/6250   lr: 0.000289   giou_loss: 1.69   conf_loss: 80.11   prob_loss: 0.56   total_loss: 82.36\n",
      "=> STEP 4242/6250   lr: 0.000289   giou_loss: 1.69   conf_loss: 80.09   prob_loss: 0.55   total_loss: 82.33\n",
      "=> STEP 4243/6250   lr: 0.000289   giou_loss: 1.58   conf_loss: 80.08   prob_loss: 0.55   total_loss: 82.21\n",
      "=> STEP 4244/6250   lr: 0.000289   giou_loss: 1.57   conf_loss: 80.05   prob_loss: 0.55   total_loss: 82.18\n",
      "=> STEP 4245/6250   lr: 0.000289   giou_loss: 1.62   conf_loss: 80.02   prob_loss: 0.55   total_loss: 82.20\n",
      "=> STEP 4246/6250   lr: 0.000288   giou_loss: 1.61   conf_loss: 80.00   prob_loss: 0.55   total_loss: 82.16\n",
      "=> STEP 4247/6250   lr: 0.000288   giou_loss: 1.58   conf_loss: 79.97   prob_loss: 0.55   total_loss: 82.11\n",
      "=> STEP 4248/6250   lr: 0.000288   giou_loss: 1.56   conf_loss: 79.95   prob_loss: 0.55   total_loss: 82.07\n",
      "=> STEP 4249/6250   lr: 0.000288   giou_loss: 1.58   conf_loss: 79.93   prob_loss: 0.55   total_loss: 82.06\n",
      "=> STEP 4250/6250   lr: 0.000288   giou_loss: 1.57   conf_loss: 79.89   prob_loss: 0.55   total_loss: 82.01\n",
      "=> STEP 4251/6250   lr: 0.000287   giou_loss: 1.56   conf_loss: 79.87   prob_loss: 0.55   total_loss: 81.99\n",
      "=> STEP 4252/6250   lr: 0.000287   giou_loss: 1.60   conf_loss: 79.85   prob_loss: 0.55   total_loss: 82.00\n",
      "=> STEP 4253/6250   lr: 0.000287   giou_loss: 1.56   conf_loss: 79.82   prob_loss: 0.55   total_loss: 81.94\n",
      "=> STEP 4254/6250   lr: 0.000287   giou_loss: 1.56   conf_loss: 79.80   prob_loss: 0.55   total_loss: 81.91\n",
      "=> STEP 4255/6250   lr: 0.000286   giou_loss: 1.56   conf_loss: 79.77   prob_loss: 0.55   total_loss: 81.89\n",
      "=> STEP 4256/6250   lr: 0.000286   giou_loss: 1.58   conf_loss: 79.75   prob_loss: 0.55   total_loss: 81.88\n",
      "=> STEP 4257/6250   lr: 0.000286   giou_loss: 1.56   conf_loss: 79.73   prob_loss: 0.55   total_loss: 81.84\n",
      "=> STEP 4258/6250   lr: 0.000286   giou_loss: 1.58   conf_loss: 79.70   prob_loss: 0.55   total_loss: 81.83\n",
      "=> STEP 4259/6250   lr: 0.000286   giou_loss: 1.56   conf_loss: 79.67   prob_loss: 0.55   total_loss: 81.79\n",
      "=> STEP 4260/6250   lr: 0.000285   giou_loss: 1.56   conf_loss: 79.65   prob_loss: 0.55   total_loss: 81.77\n",
      "=> STEP 4261/6250   lr: 0.000285   giou_loss: 1.57   conf_loss: 79.63   prob_loss: 0.55   total_loss: 81.74\n",
      "=> STEP 4262/6250   lr: 0.000285   giou_loss: 1.56   conf_loss: 79.61   prob_loss: 0.55   total_loss: 81.72\n",
      "=> STEP 4263/6250   lr: 0.000285   giou_loss: 1.59   conf_loss: 79.58   prob_loss: 0.55   total_loss: 81.72\n",
      "=> STEP 4264/6250   lr: 0.000284   giou_loss: 1.56   conf_loss: 79.56   prob_loss: 0.55   total_loss: 81.67\n",
      "=> STEP 4265/6250   lr: 0.000284   giou_loss: 1.56   conf_loss: 79.54   prob_loss: 0.55   total_loss: 81.65\n",
      "=> STEP 4266/6250   lr: 0.000284   giou_loss: 1.58   conf_loss: 79.51   prob_loss: 0.55   total_loss: 81.64\n",
      "=> STEP 4267/6250   lr: 0.000284   giou_loss: 1.56   conf_loss: 79.49   prob_loss: 0.55   total_loss: 81.60\n",
      "=> STEP 4268/6250   lr: 0.000284   giou_loss: 1.59   conf_loss: 79.46   prob_loss: 0.55   total_loss: 81.60\n",
      "=> STEP 4269/6250   lr: 0.000283   giou_loss: 1.56   conf_loss: 79.44   prob_loss: 0.55   total_loss: 81.55\n",
      "=> STEP 4270/6250   lr: 0.000283   giou_loss: 1.57   conf_loss: 79.42   prob_loss: 0.55   total_loss: 81.53\n",
      "=> STEP 4271/6250   lr: 0.000283   giou_loss: 1.59   conf_loss: 79.39   prob_loss: 0.54   total_loss: 81.53\n",
      "=> STEP 4272/6250   lr: 0.000283   giou_loss: 1.57   conf_loss: 79.37   prob_loss: 0.54   total_loss: 81.48\n",
      "=> STEP 4273/6250   lr: 0.000283   giou_loss: 1.59   conf_loss: 79.35   prob_loss: 0.55   total_loss: 81.48\n",
      "=> STEP 4274/6250   lr: 0.000282   giou_loss: 1.56   conf_loss: 79.32   prob_loss: 0.55   total_loss: 81.43\n",
      "=> STEP 4275/6250   lr: 0.000282   giou_loss: 1.56   conf_loss: 79.29   prob_loss: 0.55   total_loss: 81.40\n",
      "=> STEP 4276/6250   lr: 0.000282   giou_loss: 1.59   conf_loss: 79.27   prob_loss: 0.55   total_loss: 81.40\n",
      "=> STEP 4277/6250   lr: 0.000282   giou_loss: 1.58   conf_loss: 79.24   prob_loss: 0.55   total_loss: 81.36\n",
      "=> STEP 4278/6250   lr: 0.000281   giou_loss: 1.57   conf_loss: 79.22   prob_loss: 0.54   total_loss: 81.34\n",
      "=> STEP 4279/6250   lr: 0.000281   giou_loss: 1.57   conf_loss: 79.19   prob_loss: 0.54   total_loss: 81.30\n",
      "=> STEP 4280/6250   lr: 0.000281   giou_loss: 1.57   conf_loss: 79.17   prob_loss: 0.54   total_loss: 81.28\n",
      "=> STEP 4281/6250   lr: 0.000281   giou_loss: 1.61   conf_loss: 79.14   prob_loss: 0.54   total_loss: 81.30\n",
      "=> STEP 4282/6250   lr: 0.000281   giou_loss: 1.57   conf_loss: 79.12   prob_loss: 0.54   total_loss: 81.23\n",
      "=> STEP 4283/6250   lr: 0.000280   giou_loss: 1.57   conf_loss: 79.09   prob_loss: 0.55   total_loss: 81.20\n",
      "=> STEP 4284/6250   lr: 0.000280   giou_loss: 1.60   conf_loss: 79.07   prob_loss: 0.55   total_loss: 81.21\n",
      "=> STEP 4285/6250   lr: 0.000280   giou_loss: 1.56   conf_loss: 79.05   prob_loss: 0.55   total_loss: 81.16\n",
      "=> STEP 4286/6250   lr: 0.000280   giou_loss: 1.60   conf_loss: 79.02   prob_loss: 0.55   total_loss: 81.17\n",
      "=> STEP 4287/6250   lr: 0.000280   giou_loss: 1.60   conf_loss: 79.00   prob_loss: 0.55   total_loss: 81.14\n",
      "=> STEP 4288/6250   lr: 0.000279   giou_loss: 1.56   conf_loss: 78.98   prob_loss: 0.54   total_loss: 81.09\n",
      "=> STEP 4289/6250   lr: 0.000279   giou_loss: 1.57   conf_loss: 78.96   prob_loss: 0.54   total_loss: 81.06\n",
      "=> STEP 4290/6250   lr: 0.000279   giou_loss: 1.60   conf_loss: 78.93   prob_loss: 0.54   total_loss: 81.07\n",
      "=> STEP 4291/6250   lr: 0.000279   giou_loss: 1.58   conf_loss: 78.91   prob_loss: 0.54   total_loss: 81.03\n",
      "=> STEP 4292/6250   lr: 0.000279   giou_loss: 1.56   conf_loss: 78.88   prob_loss: 0.54   total_loss: 80.99\n",
      "=> STEP 4293/6250   lr: 0.000278   giou_loss: 1.61   conf_loss: 78.86   prob_loss: 0.54   total_loss: 81.01\n",
      "=> STEP 4294/6250   lr: 0.000278   giou_loss: 1.58   conf_loss: 78.83   prob_loss: 0.54   total_loss: 80.95\n",
      "=> STEP 4295/6250   lr: 0.000278   giou_loss: 1.56   conf_loss: 78.80   prob_loss: 0.54   total_loss: 80.91\n",
      "=> STEP 4296/6250   lr: 0.000278   giou_loss: 1.58   conf_loss: 78.77   prob_loss: 0.54   total_loss: 80.89\n",
      "=> STEP 4297/6250   lr: 0.000277   giou_loss: 1.56   conf_loss: 78.73   prob_loss: 0.54   total_loss: 80.84\n",
      "=> STEP 4298/6250   lr: 0.000277   giou_loss: 1.58   conf_loss: 78.68   prob_loss: 0.54   total_loss: 80.80\n",
      "=> STEP 4299/6250   lr: 0.000277   giou_loss: 1.57   conf_loss: 78.62   prob_loss: 0.54   total_loss: 80.72\n",
      "=> STEP 4300/6250   lr: 0.000277   giou_loss: 1.57   conf_loss: 78.52   prob_loss: 0.54   total_loss: 80.63\n",
      "=> STEP 4301/6250   lr: 0.000277   giou_loss: 1.62   conf_loss: 78.39   prob_loss: 0.54   total_loss: 80.55\n",
      "=> STEP 4302/6250   lr: 0.000276   giou_loss: 1.57   conf_loss: 78.23   prob_loss: 0.54   total_loss: 80.33\n",
      "=> STEP 4303/6250   lr: 0.000276   giou_loss: 1.57   conf_loss: 78.02   prob_loss: 0.54   total_loss: 80.13\n",
      "=> STEP 4304/6250   lr: 0.000276   giou_loss: 1.58   conf_loss: 77.77   prob_loss: 0.54   total_loss: 79.90\n",
      "=> STEP 4305/6250   lr: 0.000276   giou_loss: 1.58   conf_loss: 77.50   prob_loss: 0.53   total_loss: 79.61\n",
      "=> STEP 4306/6250   lr: 0.000276   giou_loss: 1.56   conf_loss: 77.20   prob_loss: 0.53   total_loss: 79.29\n",
      "=> STEP 4307/6250   lr: 0.000275   giou_loss: 1.60   conf_loss: 76.83   prob_loss: 0.52   total_loss: 78.95\n",
      "=> STEP 4308/6250   lr: 0.000275   giou_loss: 1.60   conf_loss: 76.43   prob_loss: 0.52   total_loss: 78.55\n",
      "=> STEP 4309/6250   lr: 0.000275   giou_loss: 1.58   conf_loss: 76.06   prob_loss: 0.52   total_loss: 78.16\n",
      "=> STEP 4310/6250   lr: 0.000275   giou_loss: 1.60   conf_loss: 75.61   prob_loss: 0.51   total_loss: 77.72\n",
      "=> STEP 4311/6250   lr: 0.000274   giou_loss: 1.56   conf_loss: 74.86   prob_loss: 0.52   total_loss: 76.94\n",
      "=> STEP 4312/6250   lr: 0.000274   giou_loss: 1.61   conf_loss: 74.08   prob_loss: 0.51   total_loss: 76.20\n",
      "=> STEP 4313/6250   lr: 0.000274   giou_loss: 1.56   conf_loss: 73.41   prob_loss: 0.50   total_loss: 75.48\n",
      "=> STEP 4314/6250   lr: 0.000274   giou_loss: 1.61   conf_loss: 78.00   prob_loss: 0.50   total_loss: 80.12\n",
      "=> STEP 4315/6250   lr: 0.000274   giou_loss: 2.35   conf_loss: 81.25   prob_loss: 1.29   total_loss: 84.88\n",
      "=> STEP 4316/6250   lr: 0.000273   giou_loss: 2.86   conf_loss: 96.82   prob_loss: 7.17   total_loss: 106.86\n",
      "=> STEP 4317/6250   lr: 0.000273   giou_loss: 2.90   conf_loss: 93.38   prob_loss: 27.07   total_loss: 123.34\n",
      "=> STEP 4318/6250   lr: 0.000273   giou_loss: 2.90   conf_loss: 86.79   prob_loss: 26.78   total_loss: 116.47\n",
      "=> STEP 4319/6250   lr: 0.000273   giou_loss: 2.90   conf_loss: 82.30   prob_loss: 24.91   total_loss: 110.11\n",
      "=> STEP 4320/6250   lr: 0.000273   giou_loss: 2.90   conf_loss: 78.59   prob_loss: 23.30   total_loss: 104.79\n",
      "=> STEP 4321/6250   lr: 0.000272   giou_loss: 2.89   conf_loss: 78.26   prob_loss: 18.99   total_loss: 100.14\n",
      "=> STEP 4322/6250   lr: 0.000272   giou_loss: 2.89   conf_loss: 75.40   prob_loss: 14.19   total_loss: 92.48\n",
      "=> STEP 4323/6250   lr: 0.000272   giou_loss: 2.89   conf_loss: 74.11   prob_loss: 10.65   total_loss: 87.65\n",
      "=> STEP 4324/6250   lr: 0.000272   giou_loss: 2.87   conf_loss: 72.26   prob_loss: 7.67   total_loss: 82.79\n",
      "=> STEP 4325/6250   lr: 0.000272   giou_loss: 2.83   conf_loss: 71.57   prob_loss: 4.91   total_loss: 79.31\n",
      "=> STEP 4326/6250   lr: 0.000271   giou_loss: 2.90   conf_loss: 74.43   prob_loss: 19.87   total_loss: 97.19\n",
      "=> STEP 4327/6250   lr: 0.000271   giou_loss: 2.90   conf_loss: 73.78   prob_loss: 15.38   total_loss: 92.06\n",
      "=> STEP 4328/6250   lr: 0.000271   giou_loss: 2.89   conf_loss: 73.58   prob_loss: 10.41   total_loss: 86.88\n",
      "=> STEP 4329/6250   lr: 0.000271   giou_loss: 2.89   conf_loss: 72.76   prob_loss: 8.67   total_loss: 84.32\n",
      "=> STEP 4330/6250   lr: 0.000270   giou_loss: 2.87   conf_loss: 71.11   prob_loss: 6.25   total_loss: 80.23\n",
      "=> STEP 4331/6250   lr: 0.000270   giou_loss: 2.90   conf_loss: 75.80   prob_loss: 27.49   total_loss: 106.19\n",
      "=> STEP 4332/6250   lr: 0.000270   giou_loss: 2.90   conf_loss: 73.60   prob_loss: 27.14   total_loss: 103.63\n",
      "=> STEP 4333/6250   lr: 0.000270   giou_loss: 2.90   conf_loss: 71.57   prob_loss: 26.78   total_loss: 101.24\n",
      "=> STEP 4334/6250   lr: 0.000270   giou_loss: 2.90   conf_loss: 71.25   prob_loss: 26.38   total_loss: 100.53\n",
      "=> STEP 4335/6250   lr: 0.000269   giou_loss: 2.90   conf_loss: 70.45   prob_loss: 25.81   total_loss: 99.16\n",
      "=> STEP 4336/6250   lr: 0.000269   giou_loss: 2.90   conf_loss: 69.70   prob_loss: 24.98   total_loss: 97.58\n",
      "=> STEP 4337/6250   lr: 0.000269   giou_loss: 2.90   conf_loss: 69.55   prob_loss: 22.75   total_loss: 95.19\n",
      "=> STEP 4338/6250   lr: 0.000269   giou_loss: 2.90   conf_loss: 70.37   prob_loss: 17.63   total_loss: 90.89\n",
      "=> STEP 4339/6250   lr: 0.000269   giou_loss: 2.89   conf_loss: 69.70   prob_loss: 15.68   total_loss: 88.27\n",
      "=> STEP 4340/6250   lr: 0.000268   giou_loss: 2.89   conf_loss: 68.52   prob_loss: 11.66   total_loss: 83.07\n",
      "=> STEP 4341/6250   lr: 0.000268   giou_loss: 2.89   conf_loss: 68.17   prob_loss: 9.87   total_loss: 80.93\n",
      "=> STEP 4342/6250   lr: 0.000268   giou_loss: 2.88   conf_loss: 68.03   prob_loss: 7.89   total_loss: 78.80\n",
      "=> STEP 4343/6250   lr: 0.000268   giou_loss: 2.88   conf_loss: 67.65   prob_loss: 6.72   total_loss: 77.25\n",
      "=> STEP 4344/6250   lr: 0.000268   giou_loss: 2.88   conf_loss: 67.27   prob_loss: 5.57   total_loss: 75.72\n",
      "=> STEP 4345/6250   lr: 0.000267   giou_loss: 2.86   conf_loss: 67.02   prob_loss: 4.36   total_loss: 74.24\n",
      "=> STEP 4346/6250   lr: 0.000267   giou_loss: 2.85   conf_loss: 66.66   prob_loss: 3.73   total_loss: 73.24\n",
      "=> STEP 4347/6250   lr: 0.000267   giou_loss: 2.85   conf_loss: 66.02   prob_loss: 3.44   total_loss: 72.31\n",
      "=> STEP 4348/6250   lr: 0.000267   giou_loss: 2.85   conf_loss: 65.59   prob_loss: 3.42   total_loss: 71.87\n",
      "=> STEP 4349/6250   lr: 0.000267   giou_loss: 2.83   conf_loss: 65.27   prob_loss: 3.05   total_loss: 71.15\n",
      "=> STEP 4350/6250   lr: 0.000266   giou_loss: 3.15   conf_loss: 65.05   prob_loss: 2.78   total_loss: 70.98\n",
      "=> STEP 4351/6250   lr: 0.000266   giou_loss: 2.83   conf_loss: 64.59   prob_loss: 2.65   total_loss: 70.07\n",
      "=> STEP 4352/6250   lr: 0.000266   giou_loss: 2.84   conf_loss: 64.27   prob_loss: 2.52   total_loss: 69.63\n",
      "=> STEP 4353/6250   lr: 0.000266   giou_loss: 2.87   conf_loss: 64.33   prob_loss: 2.14   total_loss: 69.33\n",
      "=> STEP 4354/6250   lr: 0.000265   giou_loss: 2.86   conf_loss: 63.86   prob_loss: 2.11   total_loss: 68.84\n",
      "=> STEP 4355/6250   lr: 0.000265   giou_loss: 2.88   conf_loss: 63.68   prob_loss: 2.34   total_loss: 68.90\n",
      "=> STEP 4356/6250   lr: 0.000265   giou_loss: 2.88   conf_loss: 63.77   prob_loss: 1.97   total_loss: 68.62\n",
      "=> STEP 4357/6250   lr: 0.000265   giou_loss: 2.88   conf_loss: 63.72   prob_loss: 2.02   total_loss: 68.62\n",
      "=> STEP 4358/6250   lr: 0.000265   giou_loss: 2.89   conf_loss: 63.49   prob_loss: 1.81   total_loss: 68.19\n",
      "=> STEP 4359/6250   lr: 0.000264   giou_loss: 2.89   conf_loss: 63.21   prob_loss: 1.68   total_loss: 67.77\n",
      "=> STEP 4360/6250   lr: 0.000264   giou_loss: 2.89   conf_loss: 62.97   prob_loss: 1.73   total_loss: 67.58\n",
      "=> STEP 4361/6250   lr: 0.000264   giou_loss: 2.89   conf_loss: 62.81   prob_loss: 1.75   total_loss: 67.45\n",
      "=> STEP 4362/6250   lr: 0.000264   giou_loss: 2.89   conf_loss: 62.76   prob_loss: 1.38   total_loss: 67.02\n",
      "=> STEP 4363/6250   lr: 0.000264   giou_loss: 2.89   conf_loss: 62.76   prob_loss: 1.21   total_loss: 66.86\n",
      "=> STEP 4364/6250   lr: 0.000263   giou_loss: 2.89   conf_loss: 62.60   prob_loss: 1.19   total_loss: 66.67\n",
      "=> STEP 4365/6250   lr: 0.000263   giou_loss: 2.89   conf_loss: 62.29   prob_loss: 1.32   total_loss: 66.50\n",
      "=> STEP 4366/6250   lr: 0.000263   giou_loss: 2.89   conf_loss: 62.14   prob_loss: 1.34   total_loss: 66.37\n",
      "=> STEP 4367/6250   lr: 0.000263   giou_loss: 2.89   conf_loss: 62.28   prob_loss: 1.06   total_loss: 66.22\n",
      "=> STEP 4368/6250   lr: 0.000263   giou_loss: 2.88   conf_loss: 62.17   prob_loss: 0.99   total_loss: 66.05\n",
      "=> STEP 4369/6250   lr: 0.000262   giou_loss: 2.88   conf_loss: 61.86   prob_loss: 1.09   total_loss: 65.83\n",
      "=> STEP 4370/6250   lr: 0.000262   giou_loss: 2.88   conf_loss: 61.69   prob_loss: 1.27   total_loss: 65.85\n",
      "=> STEP 4371/6250   lr: 0.000262   giou_loss: 2.88   conf_loss: 61.93   prob_loss: 0.89   total_loss: 65.70\n",
      "=> STEP 4372/6250   lr: 0.000262   giou_loss: 2.88   conf_loss: 61.86   prob_loss: 0.91   total_loss: 65.65\n",
      "=> STEP 4373/6250   lr: 0.000262   giou_loss: 2.88   conf_loss: 61.62   prob_loss: 1.06   total_loss: 65.56\n",
      "=> STEP 4374/6250   lr: 0.000261   giou_loss: 2.88   conf_loss: 61.46   prob_loss: 1.01   total_loss: 65.35\n",
      "=> STEP 4375/6250   lr: 0.000261   giou_loss: 2.87   conf_loss: 61.51   prob_loss: 0.87   total_loss: 65.25\n",
      "=> STEP 4376/6250   lr: 0.000261   giou_loss: 2.87   conf_loss: 61.44   prob_loss: 0.82   total_loss: 65.13\n",
      "=> STEP 4377/6250   lr: 0.000261   giou_loss: 2.87   conf_loss: 61.31   prob_loss: 0.85   total_loss: 65.02\n",
      "=> STEP 4378/6250   lr: 0.000260   giou_loss: 2.86   conf_loss: 61.16   prob_loss: 0.91   total_loss: 64.94\n",
      "=> STEP 4379/6250   lr: 0.000260   giou_loss: 2.86   conf_loss: 61.08   prob_loss: 0.88   total_loss: 64.82\n",
      "=> STEP 4380/6250   lr: 0.000260   giou_loss: 2.84   conf_loss: 61.07   prob_loss: 0.77   total_loss: 64.69\n",
      "=> STEP 4381/6250   lr: 0.000260   giou_loss: 2.83   conf_loss: 61.05   prob_loss: 0.72   total_loss: 64.59\n",
      "=> STEP 4382/6250   lr: 0.000260   giou_loss: 2.81   conf_loss: 60.96   prob_loss: 0.72   total_loss: 64.49\n",
      "=> STEP 4383/6250   lr: 0.000259   giou_loss: 2.79   conf_loss: 60.84   prob_loss: 0.77   total_loss: 64.40\n",
      "=> STEP 4384/6250   lr: 0.000259   giou_loss: 2.74   conf_loss: 60.79   prob_loss: 0.74   total_loss: 64.26\n",
      "=> STEP 4385/6250   lr: 0.000259   giou_loss: 2.64   conf_loss: 60.81   prob_loss: 0.67   total_loss: 64.11\n",
      "=> STEP 4386/6250   lr: 0.000259   giou_loss: 2.63   conf_loss: 60.77   prob_loss: 0.65   total_loss: 64.05\n",
      "=> STEP 4387/6250   lr: 0.000259   giou_loss: 2.47   conf_loss: 60.79   prob_loss: 1.88   total_loss: 65.14\n",
      "=> STEP 4388/6250   lr: 0.000258   giou_loss: 2.79   conf_loss: 64.29   prob_loss: 2.26   total_loss: 69.34\n",
      "=> STEP 4389/6250   lr: 0.000258   giou_loss: 2.71   conf_loss: 64.92   prob_loss: 2.00   total_loss: 69.63\n",
      "=> STEP 4390/6250   lr: 0.000258   giou_loss: 2.78   conf_loss: 65.03   prob_loss: 3.03   total_loss: 70.84\n",
      "=> STEP 4391/6250   lr: 0.000258   giou_loss: 2.82   conf_loss: 64.58   prob_loss: 4.21   total_loss: 71.62\n",
      "=> STEP 4392/6250   lr: 0.000258   giou_loss: 2.81   conf_loss: 64.18   prob_loss: 4.17   total_loss: 71.15\n",
      "=> STEP 4393/6250   lr: 0.000257   giou_loss: 2.75   conf_loss: 63.89   prob_loss: 3.58   total_loss: 70.22\n",
      "=> STEP 4394/6250   lr: 0.000257   giou_loss: 2.66   conf_loss: 63.51   prob_loss: 3.16   total_loss: 69.33\n",
      "=> STEP 4395/6250   lr: 0.000257   giou_loss: 2.50   conf_loss: 63.22   prob_loss: 2.82   total_loss: 68.54\n",
      "=> STEP 4396/6250   lr: 0.000257   giou_loss: 2.16   conf_loss: 62.98   prob_loss: 2.52   total_loss: 67.67\n",
      "=> STEP 4397/6250   lr: 0.000257   giou_loss: 1.84   conf_loss: 62.68   prob_loss: 2.23   total_loss: 66.74\n",
      "=> STEP 4398/6250   lr: 0.000256   giou_loss: 1.95   conf_loss: 62.37   prob_loss: 2.06   total_loss: 66.38\n",
      "=> STEP 4399/6250   lr: 0.000256   giou_loss: 1.98   conf_loss: 62.13   prob_loss: 1.99   total_loss: 66.09\n",
      "=> STEP 4400/6250   lr: 0.000256   giou_loss: 1.98   conf_loss: 61.72   prob_loss: 1.98   total_loss: 65.68\n",
      "=> STEP 4401/6250   lr: 0.000256   giou_loss: 2.56   conf_loss: 61.42   prob_loss: 1.95   total_loss: 65.93\n",
      "=> STEP 4402/6250   lr: 0.000256   giou_loss: 2.50   conf_loss: 61.31   prob_loss: 1.82   total_loss: 65.63\n",
      "=> STEP 4403/6250   lr: 0.000255   giou_loss: 1.90   conf_loss: 61.18   prob_loss: 1.59   total_loss: 64.67\n",
      "=> STEP 4404/6250   lr: 0.000255   giou_loss: 2.22   conf_loss: 61.07   prob_loss: 1.47   total_loss: 64.76\n",
      "=> STEP 4405/6250   lr: 0.000255   giou_loss: 2.32   conf_loss: 60.93   prob_loss: 1.36   total_loss: 64.61\n",
      "=> STEP 4406/6250   lr: 0.000255   giou_loss: 2.18   conf_loss: 60.89   prob_loss: 1.22   total_loss: 64.29\n",
      "=> STEP 4407/6250   lr: 0.000255   giou_loss: 1.64   conf_loss: 60.87   prob_loss: 1.08   total_loss: 63.59\n",
      "=> STEP 4408/6250   lr: 0.000254   giou_loss: 2.21   conf_loss: 60.81   prob_loss: 0.94   total_loss: 63.97\n",
      "=> STEP 4409/6250   lr: 0.000254   giou_loss: 2.33   conf_loss: 60.81   prob_loss: 0.87   total_loss: 64.01\n",
      "=> STEP 4410/6250   lr: 0.000254   giou_loss: 2.34   conf_loss: 60.74   prob_loss: 0.84   total_loss: 63.92\n",
      "=> STEP 4411/6250   lr: 0.000254   giou_loss: 2.20   conf_loss: 60.57   prob_loss: 0.84   total_loss: 63.62\n",
      "=> STEP 4412/6250   lr: 0.000254   giou_loss: 1.94   conf_loss: 60.45   prob_loss: 0.86   total_loss: 63.25\n",
      "=> STEP 4413/6250   lr: 0.000253   giou_loss: 2.33   conf_loss: 60.33   prob_loss: 0.96   total_loss: 63.62\n",
      "=> STEP 4414/6250   lr: 0.000253   giou_loss: 2.63   conf_loss: 60.19   prob_loss: 0.98   total_loss: 63.80\n",
      "=> STEP 4415/6250   lr: 0.000253   giou_loss: 2.74   conf_loss: 60.15   prob_loss: 0.90   total_loss: 63.79\n",
      "=> STEP 4416/6250   lr: 0.000253   giou_loss: 2.78   conf_loss: 60.16   prob_loss: 0.83   total_loss: 63.77\n",
      "=> STEP 4417/6250   lr: 0.000252   giou_loss: 2.79   conf_loss: 60.15   prob_loss: 0.79   total_loss: 63.73\n",
      "=> STEP 4418/6250   lr: 0.000252   giou_loss: 2.79   conf_loss: 60.12   prob_loss: 0.75   total_loss: 63.67\n",
      "=> STEP 4419/6250   lr: 0.000252   giou_loss: 2.78   conf_loss: 60.07   prob_loss: 0.72   total_loss: 63.57\n",
      "=> STEP 4420/6250   lr: 0.000252   giou_loss: 2.75   conf_loss: 59.98   prob_loss: 0.70   total_loss: 63.43\n",
      "=> STEP 4421/6250   lr: 0.000252   giou_loss: 2.70   conf_loss: 59.90   prob_loss: 0.68   total_loss: 63.29\n",
      "=> STEP 4422/6250   lr: 0.000251   giou_loss: 2.60   conf_loss: 59.84   prob_loss: 0.67   total_loss: 63.12\n",
      "=> STEP 4423/6250   lr: 0.000251   giou_loss: 2.35   conf_loss: 59.80   prob_loss: 0.66   total_loss: 62.82\n",
      "=> STEP 4424/6250   lr: 0.000251   giou_loss: 1.79   conf_loss: 59.76   prob_loss: 0.65   total_loss: 62.20\n",
      "=> STEP 4425/6250   lr: 0.000251   giou_loss: 2.29   conf_loss: 59.75   prob_loss: 0.64   total_loss: 62.68\n",
      "=> STEP 4426/6250   lr: 0.000251   giou_loss: 2.59   conf_loss: 59.73   prob_loss: 0.64   total_loss: 62.96\n",
      "=> STEP 4427/6250   lr: 0.000250   giou_loss: 2.57   conf_loss: 59.67   prob_loss: 0.62   total_loss: 62.85\n",
      "=> STEP 4428/6250   lr: 0.000250   giou_loss: 2.58   conf_loss: 59.60   prob_loss: 0.62   total_loss: 62.80\n",
      "=> STEP 4429/6250   lr: 0.000250   giou_loss: 2.52   conf_loss: 59.54   prob_loss: 0.62   total_loss: 62.68\n",
      "=> STEP 4430/6250   lr: 0.000250   giou_loss: 2.40   conf_loss: 59.49   prob_loss: 0.64   total_loss: 62.53\n",
      "=> STEP 4431/6250   lr: 0.000250   giou_loss: 2.11   conf_loss: 59.44   prob_loss: 0.66   total_loss: 62.21\n",
      "=> STEP 4432/6250   lr: 0.000249   giou_loss: 1.99   conf_loss: 59.42   prob_loss: 0.68   total_loss: 62.09\n",
      "=> STEP 4433/6250   lr: 0.000249   giou_loss: 1.95   conf_loss: 59.38   prob_loss: 0.63   total_loss: 61.97\n",
      "=> STEP 4434/6250   lr: 0.000249   giou_loss: 2.14   conf_loss: 59.37   prob_loss: 0.59   total_loss: 62.11\n",
      "=> STEP 4435/6250   lr: 0.000249   giou_loss: 2.28   conf_loss: 59.38   prob_loss: 0.57   total_loss: 62.23\n",
      "=> STEP 4436/6250   lr: 0.000249   giou_loss: 2.23   conf_loss: 59.36   prob_loss: 0.55   total_loss: 62.15\n",
      "=> STEP 4437/6250   lr: 0.000248   giou_loss: 1.67   conf_loss: 59.32   prob_loss: 0.54   total_loss: 61.53\n",
      "=> STEP 4438/6250   lr: 0.000248   giou_loss: 2.44   conf_loss: 59.28   prob_loss: 0.53   total_loss: 62.25\n",
      "=> STEP 4439/6250   lr: 0.000248   giou_loss: 2.81   conf_loss: 59.21   prob_loss: 0.54   total_loss: 62.56\n",
      "=> STEP 4440/6250   lr: 0.000248   giou_loss: 2.84   conf_loss: 59.18   prob_loss: 0.56   total_loss: 62.58\n",
      "=> STEP 4441/6250   lr: 0.000248   giou_loss: 2.61   conf_loss: 59.17   prob_loss: 0.57   total_loss: 62.35\n",
      "=> STEP 4442/6250   lr: 0.000247   giou_loss: 2.02   conf_loss: 59.11   prob_loss: 0.57   total_loss: 61.70\n",
      "=> STEP 4443/6250   lr: 0.000247   giou_loss: 2.18   conf_loss: 59.05   prob_loss: 0.59   total_loss: 61.83\n",
      "=> STEP 4444/6250   lr: 0.000247   giou_loss: 2.28   conf_loss: 59.01   prob_loss: 0.56   total_loss: 61.86\n",
      "=> STEP 4445/6250   lr: 0.000247   giou_loss: 2.10   conf_loss: 59.09   prob_loss: 0.54   total_loss: 61.73\n",
      "=> STEP 4446/6250   lr: 0.000247   giou_loss: 1.64   conf_loss: 59.21   prob_loss: 0.57   total_loss: 61.42\n",
      "=> STEP 4447/6250   lr: 0.000246   giou_loss: 2.10   conf_loss: 59.09   prob_loss: 0.55   total_loss: 61.74\n",
      "=> STEP 4448/6250   lr: 0.000246   giou_loss: 2.12   conf_loss: 58.96   prob_loss: 0.52   total_loss: 61.60\n",
      "=> STEP 4449/6250   lr: 0.000246   giou_loss: 2.35   conf_loss: 58.92   prob_loss: 0.54   total_loss: 61.81\n",
      "=> STEP 4450/6250   lr: 0.000246   giou_loss: 2.21   conf_loss: 58.90   prob_loss: 0.57   total_loss: 61.68\n",
      "=> STEP 4451/6250   lr: 0.000246   giou_loss: 1.57   conf_loss: 58.87   prob_loss: 0.55   total_loss: 60.99\n",
      "=> STEP 4452/6250   lr: 0.000245   giou_loss: 2.07   conf_loss: 58.86   prob_loss: 0.56   total_loss: 61.50\n",
      "=> STEP 4453/6250   lr: 0.000245   giou_loss: 2.28   conf_loss: 58.80   prob_loss: 0.54   total_loss: 61.61\n",
      "=> STEP 4454/6250   lr: 0.000245   giou_loss: 2.33   conf_loss: 58.74   prob_loss: 0.53   total_loss: 61.59\n",
      "=> STEP 4455/6250   lr: 0.000245   giou_loss: 2.28   conf_loss: 58.70   prob_loss: 0.52   total_loss: 61.50\n",
      "=> STEP 4456/6250   lr: 0.000245   giou_loss: 2.21   conf_loss: 58.67   prob_loss: 0.52   total_loss: 61.40\n",
      "=> STEP 4457/6250   lr: 0.000244   giou_loss: 1.87   conf_loss: 58.64   prob_loss: 0.52   total_loss: 61.02\n",
      "=> STEP 4458/6250   lr: 0.000244   giou_loss: 1.88   conf_loss: 58.62   prob_loss: 0.51   total_loss: 61.00\n",
      "=> STEP 4459/6250   lr: 0.000244   giou_loss: 1.96   conf_loss: 58.59   prob_loss: 0.51   total_loss: 61.06\n",
      "=> STEP 4460/6250   lr: 0.000244   giou_loss: 1.95   conf_loss: 58.56   prob_loss: 0.51   total_loss: 61.02\n",
      "=> STEP 4461/6250   lr: 0.000244   giou_loss: 1.62   conf_loss: 58.52   prob_loss: 0.51   total_loss: 60.65\n",
      "=> STEP 4462/6250   lr: 0.000243   giou_loss: 1.80   conf_loss: 58.50   prob_loss: 0.51   total_loss: 60.81\n",
      "=> STEP 4463/6250   lr: 0.000243   giou_loss: 1.64   conf_loss: 58.48   prob_loss: 0.50   total_loss: 60.62\n",
      "=> STEP 4464/6250   lr: 0.000243   giou_loss: 1.90   conf_loss: 58.47   prob_loss: 0.49   total_loss: 60.86\n",
      "=> STEP 4465/6250   lr: 0.000243   giou_loss: 1.87   conf_loss: 58.41   prob_loss: 0.49   total_loss: 60.77\n",
      "=> STEP 4466/6250   lr: 0.000243   giou_loss: 1.81   conf_loss: 58.37   prob_loss: 0.50   total_loss: 60.67\n",
      "=> STEP 4467/6250   lr: 0.000242   giou_loss: 1.84   conf_loss: 58.35   prob_loss: 0.52   total_loss: 60.71\n",
      "=> STEP 4468/6250   lr: 0.000242   giou_loss: 1.93   conf_loss: 58.31   prob_loss: 0.52   total_loss: 60.76\n",
      "=> STEP 4469/6250   lr: 0.000242   giou_loss: 1.84   conf_loss: 58.26   prob_loss: 0.52   total_loss: 60.62\n",
      "=> STEP 4470/6250   lr: 0.000242   giou_loss: 1.71   conf_loss: 58.27   prob_loss: 0.50   total_loss: 60.48\n",
      "=> STEP 4471/6250   lr: 0.000242   giou_loss: 1.78   conf_loss: 58.25   prob_loss: 0.50   total_loss: 60.53\n",
      "=> STEP 4472/6250   lr: 0.000241   giou_loss: 1.56   conf_loss: 58.20   prob_loss: 0.49   total_loss: 60.26\n",
      "=> STEP 4473/6250   lr: 0.000241   giou_loss: 1.75   conf_loss: 58.18   prob_loss: 0.49   total_loss: 60.41\n",
      "=> STEP 4474/6250   lr: 0.000241   giou_loss: 1.67   conf_loss: 58.15   prob_loss: 0.49   total_loss: 60.31\n",
      "=> STEP 4475/6250   lr: 0.000241   giou_loss: 1.81   conf_loss: 58.12   prob_loss: 0.49   total_loss: 60.42\n",
      "=> STEP 4476/6250   lr: 0.000241   giou_loss: 1.78   conf_loss: 58.08   prob_loss: 0.50   total_loss: 60.36\n",
      "=> STEP 4477/6250   lr: 0.000240   giou_loss: 1.74   conf_loss: 58.05   prob_loss: 0.50   total_loss: 60.29\n",
      "=> STEP 4478/6250   lr: 0.000240   giou_loss: 1.81   conf_loss: 58.03   prob_loss: 0.49   total_loss: 60.33\n",
      "=> STEP 4479/6250   lr: 0.000240   giou_loss: 2.07   conf_loss: 58.02   prob_loss: 0.48   total_loss: 60.57\n",
      "=> STEP 4480/6250   lr: 0.000240   giou_loss: 2.11   conf_loss: 57.99   prob_loss: 0.48   total_loss: 60.58\n",
      "=> STEP 4481/6250   lr: 0.000240   giou_loss: 1.90   conf_loss: 57.95   prob_loss: 0.49   total_loss: 60.34\n",
      "=> STEP 4482/6250   lr: 0.000239   giou_loss: 1.73   conf_loss: 57.92   prob_loss: 0.50   total_loss: 60.15\n",
      "=> STEP 4483/6250   lr: 0.000239   giou_loss: 1.96   conf_loss: 57.90   prob_loss: 0.51   total_loss: 60.37\n",
      "=> STEP 4484/6250   lr: 0.000239   giou_loss: 2.14   conf_loss: 57.87   prob_loss: 0.51   total_loss: 60.52\n",
      "=> STEP 4485/6250   lr: 0.000239   giou_loss: 2.18   conf_loss: 57.85   prob_loss: 0.50   total_loss: 60.53\n",
      "=> STEP 4486/6250   lr: 0.000239   giou_loss: 2.09   conf_loss: 57.85   prob_loss: 0.49   total_loss: 60.43\n",
      "=> STEP 4487/6250   lr: 0.000238   giou_loss: 1.94   conf_loss: 57.83   prob_loss: 0.48   total_loss: 60.25\n",
      "=> STEP 4488/6250   lr: 0.000238   giou_loss: 1.57   conf_loss: 57.82   prob_loss: 0.47   total_loss: 59.86\n",
      "=> STEP 4489/6250   lr: 0.000238   giou_loss: 1.83   conf_loss: 57.81   prob_loss: 0.47   total_loss: 60.10\n",
      "=> STEP 4490/6250   lr: 0.000238   giou_loss: 1.96   conf_loss: 57.78   prob_loss: 0.47   total_loss: 60.21\n",
      "=> STEP 4491/6250   lr: 0.000238   giou_loss: 1.87   conf_loss: 57.75   prob_loss: 0.47   total_loss: 60.09\n",
      "=> STEP 4492/6250   lr: 0.000237   giou_loss: 1.71   conf_loss: 57.73   prob_loss: 0.47   total_loss: 59.91\n",
      "=> STEP 4493/6250   lr: 0.000237   giou_loss: 1.73   conf_loss: 57.70   prob_loss: 0.47   total_loss: 59.90\n",
      "=> STEP 4494/6250   lr: 0.000237   giou_loss: 1.95   conf_loss: 57.66   prob_loss: 0.48   total_loss: 60.10\n",
      "=> STEP 4495/6250   lr: 0.000237   giou_loss: 1.88   conf_loss: 57.64   prob_loss: 0.48   total_loss: 60.00\n",
      "=> STEP 4496/6250   lr: 0.000237   giou_loss: 1.71   conf_loss: 57.61   prob_loss: 0.49   total_loss: 59.81\n",
      "=> STEP 4497/6250   lr: 0.000236   giou_loss: 1.76   conf_loss: 57.58   prob_loss: 0.50   total_loss: 59.83\n",
      "=> STEP 4498/6250   lr: 0.000236   giou_loss: 1.89   conf_loss: 57.55   prob_loss: 0.50   total_loss: 59.95\n",
      "=> STEP 4499/6250   lr: 0.000236   giou_loss: 1.86   conf_loss: 57.53   prob_loss: 0.50   total_loss: 59.90\n",
      "=> STEP 4500/6250   lr: 0.000236   giou_loss: 1.68   conf_loss: 57.51   prob_loss: 0.50   total_loss: 59.68\n",
      "=> STEP 4501/6250   lr: 0.000236   giou_loss: 1.74   conf_loss: 57.49   prob_loss: 0.49   total_loss: 59.73\n",
      "=> STEP 4502/6250   lr: 0.000235   giou_loss: 1.92   conf_loss: 57.48   prob_loss: 0.49   total_loss: 59.89\n",
      "=> STEP 4503/6250   lr: 0.000235   giou_loss: 1.84   conf_loss: 57.47   prob_loss: 0.48   total_loss: 59.79\n",
      "=> STEP 4504/6250   lr: 0.000235   giou_loss: 1.60   conf_loss: 57.46   prob_loss: 0.47   total_loss: 59.54\n",
      "=> STEP 4505/6250   lr: 0.000235   giou_loss: 1.67   conf_loss: 57.43   prob_loss: 0.47   total_loss: 59.57\n",
      "=> STEP 4506/6250   lr: 0.000235   giou_loss: 1.69   conf_loss: 57.41   prob_loss: 0.47   total_loss: 59.57\n",
      "=> STEP 4507/6250   lr: 0.000234   giou_loss: 1.73   conf_loss: 57.40   prob_loss: 0.47   total_loss: 59.60\n",
      "=> STEP 4508/6250   lr: 0.000234   giou_loss: 1.66   conf_loss: 57.37   prob_loss: 0.47   total_loss: 59.50\n",
      "=> STEP 4509/6250   lr: 0.000234   giou_loss: 1.85   conf_loss: 57.34   prob_loss: 0.47   total_loss: 59.66\n",
      "=> STEP 4510/6250   lr: 0.000234   giou_loss: 1.91   conf_loss: 57.31   prob_loss: 0.48   total_loss: 59.70\n",
      "=> STEP 4511/6250   lr: 0.000234   giou_loss: 1.67   conf_loss: 57.29   prob_loss: 0.48   total_loss: 59.45\n",
      "=> STEP 4512/6250   lr: 0.000233   giou_loss: 1.89   conf_loss: 57.28   prob_loss: 0.48   total_loss: 59.65\n",
      "=> STEP 4513/6250   lr: 0.000233   giou_loss: 1.96   conf_loss: 57.25   prob_loss: 0.48   total_loss: 59.69\n",
      "=> STEP 4514/6250   lr: 0.000233   giou_loss: 1.79   conf_loss: 57.23   prob_loss: 0.48   total_loss: 59.50\n",
      "=> STEP 4515/6250   lr: 0.000233   giou_loss: 1.76   conf_loss: 57.23   prob_loss: 0.48   total_loss: 59.47\n",
      "=> STEP 4516/6250   lr: 0.000233   giou_loss: 1.76   conf_loss: 57.18   prob_loss: 0.48   total_loss: 59.43\n",
      "=> STEP 4517/6250   lr: 0.000232   giou_loss: 1.66   conf_loss: 57.17   prob_loss: 0.48   total_loss: 59.30\n",
      "=> STEP 4518/6250   lr: 0.000232   giou_loss: 1.75   conf_loss: 57.16   prob_loss: 0.48   total_loss: 59.38\n",
      "=> STEP 4519/6250   lr: 0.000232   giou_loss: 1.81   conf_loss: 57.13   prob_loss: 0.48   total_loss: 59.41\n",
      "=> STEP 4520/6250   lr: 0.000232   giou_loss: 1.78   conf_loss: 57.11   prob_loss: 0.47   total_loss: 59.36\n",
      "=> STEP 4521/6250   lr: 0.000232   giou_loss: 1.78   conf_loss: 57.09   prob_loss: 0.47   total_loss: 59.34\n",
      "=> STEP 4522/6250   lr: 0.000232   giou_loss: 1.85   conf_loss: 57.07   prob_loss: 0.47   total_loss: 59.38\n",
      "=> STEP 4523/6250   lr: 0.000231   giou_loss: 1.89   conf_loss: 57.06   prob_loss: 0.47   total_loss: 59.41\n",
      "=> STEP 4524/6250   lr: 0.000231   giou_loss: 1.97   conf_loss: 57.03   prob_loss: 0.47   total_loss: 59.47\n",
      "=> STEP 4525/6250   lr: 0.000231   giou_loss: 1.75   conf_loss: 57.01   prob_loss: 0.47   total_loss: 59.23\n",
      "=> STEP 4526/6250   lr: 0.000231   giou_loss: 1.87   conf_loss: 56.99   prob_loss: 0.48   total_loss: 59.33\n",
      "=> STEP 4527/6250   lr: 0.000231   giou_loss: 2.11   conf_loss: 56.96   prob_loss: 0.48   total_loss: 59.55\n",
      "=> STEP 4528/6250   lr: 0.000230   giou_loss: 2.13   conf_loss: 56.95   prob_loss: 0.48   total_loss: 59.56\n",
      "=> STEP 4529/6250   lr: 0.000230   giou_loss: 1.96   conf_loss: 56.93   prob_loss: 0.48   total_loss: 59.36\n",
      "=> STEP 4530/6250   lr: 0.000230   giou_loss: 1.79   conf_loss: 56.91   prob_loss: 0.47   total_loss: 59.17\n",
      "=> STEP 4531/6250   lr: 0.000230   giou_loss: 1.94   conf_loss: 56.90   prob_loss: 0.47   total_loss: 59.31\n",
      "=> STEP 4532/6250   lr: 0.000230   giou_loss: 2.01   conf_loss: 56.89   prob_loss: 0.47   total_loss: 59.36\n",
      "=> STEP 4533/6250   lr: 0.000229   giou_loss: 1.92   conf_loss: 56.87   prob_loss: 0.47   total_loss: 59.26\n",
      "=> STEP 4534/6250   lr: 0.000229   giou_loss: 1.93   conf_loss: 56.84   prob_loss: 0.48   total_loss: 59.25\n",
      "=> STEP 4535/6250   lr: 0.000229   giou_loss: 1.88   conf_loss: 56.81   prob_loss: 0.49   total_loss: 59.18\n",
      "=> STEP 4536/6250   lr: 0.000229   giou_loss: 1.86   conf_loss: 56.79   prob_loss: 0.49   total_loss: 59.14\n",
      "=> STEP 4537/6250   lr: 0.000229   giou_loss: 1.59   conf_loss: 56.78   prob_loss: 0.48   total_loss: 58.85\n",
      "=> STEP 4538/6250   lr: 0.000228   giou_loss: 1.98   conf_loss: 56.77   prob_loss: 0.47   total_loss: 59.23\n",
      "=> STEP 4539/6250   lr: 0.000228   giou_loss: 2.11   conf_loss: 56.76   prob_loss: 0.47   total_loss: 59.34\n",
      "=> STEP 4540/6250   lr: 0.000228   giou_loss: 2.04   conf_loss: 56.75   prob_loss: 0.47   total_loss: 59.25\n",
      "=> STEP 4541/6250   lr: 0.000228   giou_loss: 1.71   conf_loss: 56.72   prob_loss: 0.47   total_loss: 58.89\n",
      "=> STEP 4542/6250   lr: 0.000228   giou_loss: 1.95   conf_loss: 56.70   prob_loss: 0.48   total_loss: 59.14\n",
      "=> STEP 4543/6250   lr: 0.000227   giou_loss: 2.18   conf_loss: 56.67   prob_loss: 0.48   total_loss: 59.33\n",
      "=> STEP 4544/6250   lr: 0.000227   giou_loss: 2.20   conf_loss: 56.66   prob_loss: 0.48   total_loss: 59.34\n",
      "=> STEP 4545/6250   lr: 0.000227   giou_loss: 2.08   conf_loss: 56.64   prob_loss: 0.48   total_loss: 59.19\n",
      "=> STEP 4546/6250   lr: 0.000227   giou_loss: 1.74   conf_loss: 56.61   prob_loss: 0.47   total_loss: 58.82\n",
      "=> STEP 4547/6250   lr: 0.000227   giou_loss: 1.90   conf_loss: 56.60   prob_loss: 0.48   total_loss: 58.97\n",
      "=> STEP 4548/6250   lr: 0.000226   giou_loss: 2.12   conf_loss: 56.59   prob_loss: 0.48   total_loss: 59.19\n",
      "=> STEP 4549/6250   lr: 0.000226   giou_loss: 2.12   conf_loss: 56.58   prob_loss: 0.48   total_loss: 59.18\n",
      "=> STEP 4550/6250   lr: 0.000226   giou_loss: 1.92   conf_loss: 56.55   prob_loss: 0.49   total_loss: 58.97\n",
      "=> STEP 4551/6250   lr: 0.000226   giou_loss: 1.62   conf_loss: 56.54   prob_loss: 0.51   total_loss: 58.67\n",
      "=> STEP 4552/6250   lr: 0.000226   giou_loss: 1.59   conf_loss: 56.51   prob_loss: 0.48   total_loss: 58.58\n",
      "=> STEP 4553/6250   lr: 0.000225   giou_loss: 1.58   conf_loss: 56.49   prob_loss: 0.47   total_loss: 58.54\n",
      "=> STEP 4554/6250   lr: 0.000225   giou_loss: 1.56   conf_loss: 56.47   prob_loss: 0.47   total_loss: 58.50\n",
      "=> STEP 4555/6250   lr: 0.000225   giou_loss: 1.56   conf_loss: 56.46   prob_loss: 0.47   total_loss: 58.49\n",
      "=> STEP 4556/6250   lr: 0.000225   giou_loss: 1.57   conf_loss: 56.45   prob_loss: 0.48   total_loss: 58.50\n",
      "=> STEP 4557/6250   lr: 0.000225   giou_loss: 1.71   conf_loss: 56.42   prob_loss: 0.48   total_loss: 58.61\n",
      "=> STEP 4558/6250   lr: 0.000225   giou_loss: 1.70   conf_loss: 56.39   prob_loss: 0.48   total_loss: 58.58\n",
      "=> STEP 4559/6250   lr: 0.000224   giou_loss: 1.57   conf_loss: 56.36   prob_loss: 0.49   total_loss: 58.41\n",
      "=> STEP 4560/6250   lr: 0.000224   giou_loss: 1.75   conf_loss: 56.34   prob_loss: 0.49   total_loss: 58.57\n",
      "=> STEP 4561/6250   lr: 0.000224   giou_loss: 1.71   conf_loss: 56.32   prob_loss: 0.48   total_loss: 58.51\n",
      "=> STEP 4562/6250   lr: 0.000224   giou_loss: 1.65   conf_loss: 56.31   prob_loss: 0.47   total_loss: 58.43\n",
      "=> STEP 4563/6250   lr: 0.000224   giou_loss: 1.61   conf_loss: 56.30   prob_loss: 0.47   total_loss: 58.38\n",
      "=> STEP 4564/6250   lr: 0.000223   giou_loss: 1.76   conf_loss: 56.28   prob_loss: 0.47   total_loss: 58.51\n",
      "=> STEP 4565/6250   lr: 0.000223   giou_loss: 1.69   conf_loss: 56.26   prob_loss: 0.47   total_loss: 58.42\n",
      "=> STEP 4566/6250   lr: 0.000223   giou_loss: 1.71   conf_loss: 56.24   prob_loss: 0.47   total_loss: 58.41\n",
      "=> STEP 4567/6250   lr: 0.000223   giou_loss: 1.66   conf_loss: 56.22   prob_loss: 0.47   total_loss: 58.34\n",
      "=> STEP 4568/6250   lr: 0.000223   giou_loss: 1.57   conf_loss: 56.21   prob_loss: 0.46   total_loss: 58.24\n",
      "=> STEP 4569/6250   lr: 0.000222   giou_loss: 1.75   conf_loss: 56.19   prob_loss: 0.46   total_loss: 58.41\n",
      "=> STEP 4570/6250   lr: 0.000222   giou_loss: 1.80   conf_loss: 56.17   prob_loss: 0.46   total_loss: 58.42\n",
      "=> STEP 4571/6250   lr: 0.000222   giou_loss: 1.63   conf_loss: 56.15   prob_loss: 0.47   total_loss: 58.25\n",
      "=> STEP 4572/6250   lr: 0.000222   giou_loss: 1.77   conf_loss: 56.14   prob_loss: 0.47   total_loss: 58.38\n",
      "=> STEP 4573/6250   lr: 0.000222   giou_loss: 1.86   conf_loss: 56.10   prob_loss: 0.47   total_loss: 58.44\n",
      "=> STEP 4574/6250   lr: 0.000221   giou_loss: 1.79   conf_loss: 56.10   prob_loss: 0.47   total_loss: 58.36\n",
      "=> STEP 4575/6250   lr: 0.000221   giou_loss: 1.57   conf_loss: 56.09   prob_loss: 0.47   total_loss: 58.13\n",
      "=> STEP 4576/6250   lr: 0.000221   giou_loss: 1.77   conf_loss: 56.07   prob_loss: 0.47   total_loss: 58.30\n",
      "=> STEP 4577/6250   lr: 0.000221   giou_loss: 1.90   conf_loss: 56.05   prob_loss: 0.46   total_loss: 58.41\n",
      "=> STEP 4578/6250   lr: 0.000221   giou_loss: 1.87   conf_loss: 56.04   prob_loss: 0.47   total_loss: 58.38\n",
      "=> STEP 4579/6250   lr: 0.000221   giou_loss: 1.77   conf_loss: 56.02   prob_loss: 0.47   total_loss: 58.25\n",
      "=> STEP 4580/6250   lr: 0.000220   giou_loss: 1.56   conf_loss: 55.99   prob_loss: 0.47   total_loss: 58.03\n",
      "=> STEP 4581/6250   lr: 0.000220   giou_loss: 1.69   conf_loss: 55.97   prob_loss: 0.47   total_loss: 58.13\n",
      "=> STEP 4582/6250   lr: 0.000220   giou_loss: 1.73   conf_loss: 55.97   prob_loss: 0.47   total_loss: 58.17\n",
      "=> STEP 4583/6250   lr: 0.000220   giou_loss: 1.63   conf_loss: 55.94   prob_loss: 0.47   total_loss: 58.04\n",
      "=> STEP 4584/6250   lr: 0.000220   giou_loss: 1.65   conf_loss: 55.92   prob_loss: 0.47   total_loss: 58.04\n",
      "=> STEP 4585/6250   lr: 0.000219   giou_loss: 1.72   conf_loss: 55.91   prob_loss: 0.47   total_loss: 58.10\n",
      "=> STEP 4586/6250   lr: 0.000219   giou_loss: 1.63   conf_loss: 55.89   prob_loss: 0.47   total_loss: 57.99\n",
      "=> STEP 4587/6250   lr: 0.000219   giou_loss: 1.69   conf_loss: 55.87   prob_loss: 0.47   total_loss: 58.02\n",
      "=> STEP 4588/6250   lr: 0.000219   giou_loss: 1.73   conf_loss: 55.84   prob_loss: 0.47   total_loss: 58.04\n",
      "=> STEP 4589/6250   lr: 0.000219   giou_loss: 1.61   conf_loss: 55.85   prob_loss: 0.47   total_loss: 57.93\n",
      "=> STEP 4590/6250   lr: 0.000218   giou_loss: 1.67   conf_loss: 55.82   prob_loss: 0.47   total_loss: 57.95\n",
      "=> STEP 4591/6250   lr: 0.000218   giou_loss: 1.74   conf_loss: 55.78   prob_loss: 0.47   total_loss: 57.98\n",
      "=> STEP 4592/6250   lr: 0.000218   giou_loss: 1.69   conf_loss: 55.78   prob_loss: 0.47   total_loss: 57.94\n",
      "=> STEP 4593/6250   lr: 0.000218   giou_loss: 1.62   conf_loss: 55.78   prob_loss: 0.47   total_loss: 57.86\n",
      "=> STEP 4594/6250   lr: 0.000218   giou_loss: 1.70   conf_loss: 55.73   prob_loss: 0.46   total_loss: 57.89\n",
      "=> STEP 4595/6250   lr: 0.000217   giou_loss: 1.72   conf_loss: 55.72   prob_loss: 0.46   total_loss: 57.91\n",
      "=> STEP 4596/6250   lr: 0.000217   giou_loss: 1.66   conf_loss: 55.73   prob_loss: 0.46   total_loss: 57.85\n",
      "=> STEP 4597/6250   lr: 0.000217   giou_loss: 1.76   conf_loss: 55.69   prob_loss: 0.46   total_loss: 57.91\n",
      "=> STEP 4598/6250   lr: 0.000217   giou_loss: 1.84   conf_loss: 55.66   prob_loss: 0.46   total_loss: 57.96\n",
      "=> STEP 4599/6250   lr: 0.000217   giou_loss: 1.81   conf_loss: 55.69   prob_loss: 0.46   total_loss: 57.96\n",
      "=> STEP 4600/6250   lr: 0.000217   giou_loss: 1.56   conf_loss: 55.64   prob_loss: 0.46   total_loss: 57.67\n",
      "=> STEP 4601/6250   lr: 0.000216   giou_loss: 1.71   conf_loss: 55.59   prob_loss: 0.47   total_loss: 57.77\n",
      "=> STEP 4602/6250   lr: 0.000216   giou_loss: 1.64   conf_loss: 55.61   prob_loss: 0.46   total_loss: 57.72\n",
      "=> STEP 4603/6250   lr: 0.000216   giou_loss: 1.61   conf_loss: 55.60   prob_loss: 0.46   total_loss: 57.67\n",
      "=> STEP 4604/6250   lr: 0.000216   giou_loss: 1.56   conf_loss: 55.54   prob_loss: 0.46   total_loss: 57.56\n",
      "=> STEP 4605/6250   lr: 0.000216   giou_loss: 1.58   conf_loss: 55.56   prob_loss: 0.46   total_loss: 57.60\n",
      "=> STEP 4606/6250   lr: 0.000215   giou_loss: 1.56   conf_loss: 55.52   prob_loss: 0.46   total_loss: 57.55\n",
      "=> STEP 4607/6250   lr: 0.000215   giou_loss: 1.58   conf_loss: 55.47   prob_loss: 0.46   total_loss: 57.52\n",
      "=> STEP 4608/6250   lr: 0.000215   giou_loss: 1.56   conf_loss: 55.47   prob_loss: 0.46   total_loss: 57.50\n",
      "=> STEP 4609/6250   lr: 0.000215   giou_loss: 1.58   conf_loss: 55.45   prob_loss: 0.46   total_loss: 57.49\n",
      "=> STEP 4610/6250   lr: 0.000215   giou_loss: 1.56   conf_loss: 55.41   prob_loss: 0.46   total_loss: 57.43\n",
      "=> STEP 4611/6250   lr: 0.000214   giou_loss: 1.60   conf_loss: 55.40   prob_loss: 0.46   total_loss: 57.47\n",
      "=> STEP 4612/6250   lr: 0.000214   giou_loss: 1.56   conf_loss: 55.38   prob_loss: 0.46   total_loss: 57.41\n",
      "=> STEP 4613/6250   lr: 0.000214   giou_loss: 1.57   conf_loss: 55.34   prob_loss: 0.47   total_loss: 57.37\n",
      "=> STEP 4614/6250   lr: 0.000214   giou_loss: 1.60   conf_loss: 55.33   prob_loss: 0.46   total_loss: 57.39\n",
      "=> STEP 4615/6250   lr: 0.000214   giou_loss: 1.56   conf_loss: 55.31   prob_loss: 0.46   total_loss: 57.34\n",
      "=> STEP 4616/6250   lr: 0.000213   giou_loss: 1.60   conf_loss: 55.29   prob_loss: 0.46   total_loss: 57.34\n",
      "=> STEP 4617/6250   lr: 0.000213   giou_loss: 1.56   conf_loss: 55.27   prob_loss: 0.46   total_loss: 57.29\n",
      "=> STEP 4618/6250   lr: 0.000213   giou_loss: 1.56   conf_loss: 55.25   prob_loss: 0.46   total_loss: 57.27\n",
      "=> STEP 4619/6250   lr: 0.000213   giou_loss: 1.57   conf_loss: 55.22   prob_loss: 0.46   total_loss: 57.26\n",
      "=> STEP 4620/6250   lr: 0.000213   giou_loss: 1.60   conf_loss: 55.21   prob_loss: 0.46   total_loss: 57.27\n",
      "=> STEP 4621/6250   lr: 0.000213   giou_loss: 1.69   conf_loss: 55.18   prob_loss: 0.46   total_loss: 57.34\n",
      "=> STEP 4622/6250   lr: 0.000212   giou_loss: 1.62   conf_loss: 55.17   prob_loss: 0.46   total_loss: 57.25\n",
      "=> STEP 4623/6250   lr: 0.000212   giou_loss: 1.67   conf_loss: 55.15   prob_loss: 0.46   total_loss: 57.28\n",
      "=> STEP 4624/6250   lr: 0.000212   giou_loss: 1.69   conf_loss: 55.13   prob_loss: 0.46   total_loss: 57.28\n",
      "=> STEP 4625/6250   lr: 0.000212   giou_loss: 1.61   conf_loss: 55.11   prob_loss: 0.46   total_loss: 57.19\n",
      "=> STEP 4626/6250   lr: 0.000212   giou_loss: 1.56   conf_loss: 55.09   prob_loss: 0.46   total_loss: 57.12\n",
      "=> STEP 4627/6250   lr: 0.000211   giou_loss: 1.66   conf_loss: 55.08   prob_loss: 0.46   total_loss: 57.20\n",
      "=> STEP 4628/6250   lr: 0.000211   giou_loss: 1.60   conf_loss: 55.06   prob_loss: 0.46   total_loss: 57.12\n",
      "=> STEP 4629/6250   lr: 0.000211   giou_loss: 1.61   conf_loss: 55.04   prob_loss: 0.46   total_loss: 57.11\n",
      "=> STEP 4630/6250   lr: 0.000211   giou_loss: 1.64   conf_loss: 55.03   prob_loss: 0.46   total_loss: 57.12\n",
      "=> STEP 4631/6250   lr: 0.000211   giou_loss: 1.61   conf_loss: 55.00   prob_loss: 0.46   total_loss: 57.07\n",
      "=> STEP 4632/6250   lr: 0.000211   giou_loss: 1.61   conf_loss: 54.99   prob_loss: 0.47   total_loss: 57.06\n",
      "=> STEP 4633/6250   lr: 0.000210   giou_loss: 1.56   conf_loss: 54.97   prob_loss: 0.46   total_loss: 56.99\n",
      "=> STEP 4634/6250   lr: 0.000210   giou_loss: 1.56   conf_loss: 54.95   prob_loss: 0.46   total_loss: 56.98\n",
      "=> STEP 4635/6250   lr: 0.000210   giou_loss: 1.57   conf_loss: 54.94   prob_loss: 0.46   total_loss: 56.96\n",
      "=> STEP 4636/6250   lr: 0.000210   giou_loss: 1.60   conf_loss: 54.92   prob_loss: 0.46   total_loss: 56.99\n",
      "=> STEP 4637/6250   lr: 0.000210   giou_loss: 1.62   conf_loss: 54.91   prob_loss: 0.46   total_loss: 56.98\n",
      "=> STEP 4638/6250   lr: 0.000209   giou_loss: 1.64   conf_loss: 54.89   prob_loss: 0.46   total_loss: 56.99\n",
      "=> STEP 4639/6250   lr: 0.000209   giou_loss: 1.60   conf_loss: 54.87   prob_loss: 0.46   total_loss: 56.93\n",
      "=> STEP 4640/6250   lr: 0.000209   giou_loss: 1.59   conf_loss: 54.85   prob_loss: 0.46   total_loss: 56.91\n",
      "=> STEP 4641/6250   lr: 0.000209   giou_loss: 1.63   conf_loss: 54.84   prob_loss: 0.46   total_loss: 56.93\n",
      "=> STEP 4642/6250   lr: 0.000209   giou_loss: 1.61   conf_loss: 54.84   prob_loss: 0.46   total_loss: 56.92\n",
      "=> STEP 4643/6250   lr: 0.000208   giou_loss: 1.62   conf_loss: 54.80   prob_loss: 0.46   total_loss: 56.88\n",
      "=> STEP 4644/6250   lr: 0.000208   giou_loss: 1.59   conf_loss: 54.80   prob_loss: 0.46   total_loss: 56.85\n",
      "=> STEP 4645/6250   lr: 0.000208   giou_loss: 1.73   conf_loss: 54.77   prob_loss: 0.46   total_loss: 56.97\n",
      "=> STEP 4646/6250   lr: 0.000208   giou_loss: 1.74   conf_loss: 54.74   prob_loss: 0.46   total_loss: 56.94\n",
      "=> STEP 4647/6250   lr: 0.000208   giou_loss: 1.63   conf_loss: 54.75   prob_loss: 0.46   total_loss: 56.84\n",
      "=> STEP 4648/6250   lr: 0.000208   giou_loss: 1.64   conf_loss: 54.72   prob_loss: 0.46   total_loss: 56.82\n",
      "=> STEP 4649/6250   lr: 0.000207   giou_loss: 1.65   conf_loss: 54.70   prob_loss: 0.46   total_loss: 56.81\n",
      "=> STEP 4650/6250   lr: 0.000207   giou_loss: 1.59   conf_loss: 54.69   prob_loss: 0.46   total_loss: 56.74\n",
      "=> STEP 4651/6250   lr: 0.000207   giou_loss: 1.66   conf_loss: 54.67   prob_loss: 0.46   total_loss: 56.80\n",
      "=> STEP 4652/6250   lr: 0.000207   giou_loss: 1.65   conf_loss: 54.65   prob_loss: 0.46   total_loss: 56.76\n",
      "=> STEP 4653/6250   lr: 0.000207   giou_loss: 1.64   conf_loss: 54.65   prob_loss: 0.46   total_loss: 56.75\n",
      "=> STEP 4654/6250   lr: 0.000206   giou_loss: 1.57   conf_loss: 54.61   prob_loss: 0.46   total_loss: 56.65\n",
      "=> STEP 4655/6250   lr: 0.000206   giou_loss: 1.78   conf_loss: 54.61   prob_loss: 0.46   total_loss: 56.85\n",
      "=> STEP 4656/6250   lr: 0.000206   giou_loss: 1.76   conf_loss: 54.61   prob_loss: 0.46   total_loss: 56.82\n",
      "=> STEP 4657/6250   lr: 0.000206   giou_loss: 1.58   conf_loss: 54.56   prob_loss: 0.46   total_loss: 56.60\n",
      "=> STEP 4658/6250   lr: 0.000206   giou_loss: 1.66   conf_loss: 54.58   prob_loss: 0.46   total_loss: 56.71\n",
      "=> STEP 4659/6250   lr: 0.000206   giou_loss: 1.56   conf_loss: 54.54   prob_loss: 0.46   total_loss: 56.56\n",
      "=> STEP 4660/6250   lr: 0.000205   giou_loss: 1.71   conf_loss: 54.53   prob_loss: 0.46   total_loss: 56.70\n",
      "=> STEP 4661/6250   lr: 0.000205   giou_loss: 1.66   conf_loss: 54.52   prob_loss: 0.46   total_loss: 56.64\n",
      "=> STEP 4662/6250   lr: 0.000205   giou_loss: 1.56   conf_loss: 54.50   prob_loss: 0.46   total_loss: 56.52\n",
      "=> STEP 4663/6250   lr: 0.000205   giou_loss: 1.63   conf_loss: 54.47   prob_loss: 0.46   total_loss: 56.56\n",
      "=> STEP 4664/6250   lr: 0.000205   giou_loss: 1.66   conf_loss: 54.47   prob_loss: 0.46   total_loss: 56.59\n",
      "=> STEP 4665/6250   lr: 0.000204   giou_loss: 1.56   conf_loss: 54.46   prob_loss: 0.46   total_loss: 56.48\n",
      "=> STEP 4666/6250   lr: 0.000204   giou_loss: 1.71   conf_loss: 54.46   prob_loss: 0.46   total_loss: 56.63\n",
      "=> STEP 4667/6250   lr: 0.000204   giou_loss: 1.65   conf_loss: 54.46   prob_loss: 0.46   total_loss: 56.57\n",
      "=> STEP 4668/6250   lr: 0.000204   giou_loss: 1.57   conf_loss: 54.42   prob_loss: 0.46   total_loss: 56.45\n",
      "=> STEP 4669/6250   lr: 0.000204   giou_loss: 1.70   conf_loss: 54.40   prob_loss: 0.46   total_loss: 56.56\n",
      "=> STEP 4670/6250   lr: 0.000203   giou_loss: 1.60   conf_loss: 54.43   prob_loss: 0.46   total_loss: 56.49\n",
      "=> STEP 4671/6250   lr: 0.000203   giou_loss: 1.75   conf_loss: 54.37   prob_loss: 0.46   total_loss: 56.58\n",
      "=> STEP 4672/6250   lr: 0.000203   giou_loss: 1.79   conf_loss: 54.38   prob_loss: 0.46   total_loss: 56.63\n",
      "=> STEP 4673/6250   lr: 0.000203   giou_loss: 1.64   conf_loss: 54.37   prob_loss: 0.46   total_loss: 56.47\n",
      "=> STEP 4674/6250   lr: 0.000203   giou_loss: 1.79   conf_loss: 54.35   prob_loss: 0.47   total_loss: 56.61\n",
      "=> STEP 4675/6250   lr: 0.000203   giou_loss: 1.83   conf_loss: 54.30   prob_loss: 0.47   total_loss: 56.60\n",
      "=> STEP 4676/6250   lr: 0.000202   giou_loss: 1.70   conf_loss: 54.33   prob_loss: 0.46   total_loss: 56.49\n",
      "=> STEP 4677/6250   lr: 0.000202   giou_loss: 1.74   conf_loss: 54.32   prob_loss: 0.46   total_loss: 56.52\n",
      "=> STEP 4678/6250   lr: 0.000202   giou_loss: 1.74   conf_loss: 54.29   prob_loss: 0.45   total_loss: 56.48\n",
      "=> STEP 4679/6250   lr: 0.000202   giou_loss: 1.59   conf_loss: 54.33   prob_loss: 0.45   total_loss: 56.37\n",
      "=> STEP 4680/6250   lr: 0.000202   giou_loss: 1.58   conf_loss: 54.28   prob_loss: 0.45   total_loss: 56.31\n",
      "=> STEP 4681/6250   lr: 0.000201   giou_loss: 1.59   conf_loss: 54.24   prob_loss: 0.46   total_loss: 56.29\n",
      "=> STEP 4682/6250   lr: 0.000201   giou_loss: 1.58   conf_loss: 54.24   prob_loss: 0.46   total_loss: 56.28\n",
      "=> STEP 4683/6250   lr: 0.000201   giou_loss: 1.56   conf_loss: 54.22   prob_loss: 0.46   total_loss: 56.24\n",
      "=> STEP 4684/6250   lr: 0.000201   giou_loss: 1.65   conf_loss: 54.19   prob_loss: 0.46   total_loss: 56.30\n",
      "=> STEP 4685/6250   lr: 0.000201   giou_loss: 1.57   conf_loss: 54.18   prob_loss: 0.46   total_loss: 56.21\n",
      "=> STEP 4686/6250   lr: 0.000201   giou_loss: 1.61   conf_loss: 54.17   prob_loss: 0.46   total_loss: 56.24\n",
      "=> STEP 4687/6250   lr: 0.000200   giou_loss: 1.62   conf_loss: 54.14   prob_loss: 0.46   total_loss: 56.21\n",
      "=> STEP 4688/6250   lr: 0.000200   giou_loss: 1.56   conf_loss: 54.13   prob_loss: 0.46   total_loss: 56.15\n",
      "=> STEP 4689/6250   lr: 0.000200   giou_loss: 1.63   conf_loss: 54.12   prob_loss: 0.46   total_loss: 56.20\n",
      "=> STEP 4690/6250   lr: 0.000200   giou_loss: 1.61   conf_loss: 54.09   prob_loss: 0.46   total_loss: 56.15\n",
      "=> STEP 4691/6250   lr: 0.000200   giou_loss: 1.59   conf_loss: 54.08   prob_loss: 0.46   total_loss: 56.13\n",
      "=> STEP 4692/6250   lr: 0.000199   giou_loss: 1.59   conf_loss: 54.06   prob_loss: 0.46   total_loss: 56.12\n",
      "=> STEP 4693/6250   lr: 0.000199   giou_loss: 1.56   conf_loss: 54.03   prob_loss: 0.46   total_loss: 56.05\n",
      "=> STEP 4694/6250   lr: 0.000199   giou_loss: 1.65   conf_loss: 54.02   prob_loss: 0.46   total_loss: 56.12\n",
      "=> STEP 4695/6250   lr: 0.000199   giou_loss: 1.56   conf_loss: 54.01   prob_loss: 0.46   total_loss: 56.04\n",
      "=> STEP 4696/6250   lr: 0.000199   giou_loss: 1.66   conf_loss: 53.99   prob_loss: 0.46   total_loss: 56.10\n",
      "=> STEP 4697/6250   lr: 0.000199   giou_loss: 1.56   conf_loss: 53.97   prob_loss: 0.46   total_loss: 55.99\n",
      "=> STEP 4698/6250   lr: 0.000198   giou_loss: 1.62   conf_loss: 53.97   prob_loss: 0.46   total_loss: 56.04\n",
      "=> STEP 4699/6250   lr: 0.000198   giou_loss: 1.62   conf_loss: 53.95   prob_loss: 0.46   total_loss: 56.03\n",
      "=> STEP 4700/6250   lr: 0.000198   giou_loss: 1.63   conf_loss: 53.92   prob_loss: 0.46   total_loss: 56.01\n",
      "=> STEP 4701/6250   lr: 0.000198   giou_loss: 1.56   conf_loss: 53.92   prob_loss: 0.46   total_loss: 55.94\n",
      "=> STEP 4702/6250   lr: 0.000198   giou_loss: 1.60   conf_loss: 53.90   prob_loss: 0.46   total_loss: 55.96\n",
      "=> STEP 4703/6250   lr: 0.000197   giou_loss: 1.56   conf_loss: 53.89   prob_loss: 0.46   total_loss: 55.91\n",
      "=> STEP 4704/6250   lr: 0.000197   giou_loss: 1.64   conf_loss: 53.88   prob_loss: 0.46   total_loss: 55.97\n",
      "=> STEP 4705/6250   lr: 0.000197   giou_loss: 1.56   conf_loss: 53.86   prob_loss: 0.46   total_loss: 55.88\n",
      "=> STEP 4706/6250   lr: 0.000197   giou_loss: 1.58   conf_loss: 53.86   prob_loss: 0.46   total_loss: 55.90\n",
      "=> STEP 4707/6250   lr: 0.000197   giou_loss: 1.62   conf_loss: 53.82   prob_loss: 0.46   total_loss: 55.91\n",
      "=> STEP 4708/6250   lr: 0.000197   giou_loss: 1.56   conf_loss: 53.83   prob_loss: 0.46   total_loss: 55.85\n",
      "=> STEP 4709/6250   lr: 0.000196   giou_loss: 1.67   conf_loss: 53.81   prob_loss: 0.46   total_loss: 55.94\n",
      "=> STEP 4710/6250   lr: 0.000196   giou_loss: 1.56   conf_loss: 53.80   prob_loss: 0.46   total_loss: 55.82\n",
      "=> STEP 4711/6250   lr: 0.000196   giou_loss: 1.61   conf_loss: 53.78   prob_loss: 0.46   total_loss: 55.85\n",
      "=> STEP 4712/6250   lr: 0.000196   giou_loss: 1.57   conf_loss: 53.77   prob_loss: 0.46   total_loss: 55.79\n",
      "=> STEP 4713/6250   lr: 0.000196   giou_loss: 1.62   conf_loss: 53.76   prob_loss: 0.46   total_loss: 55.84\n",
      "=> STEP 4714/6250   lr: 0.000196   giou_loss: 1.56   conf_loss: 53.74   prob_loss: 0.46   total_loss: 55.77\n",
      "=> STEP 4715/6250   lr: 0.000195   giou_loss: 1.65   conf_loss: 53.73   prob_loss: 0.46   total_loss: 55.85\n",
      "=> STEP 4716/6250   lr: 0.000195   giou_loss: 1.58   conf_loss: 53.71   prob_loss: 0.46   total_loss: 55.75\n",
      "=> STEP 4717/6250   lr: 0.000195   giou_loss: 1.64   conf_loss: 53.72   prob_loss: 0.46   total_loss: 55.82\n",
      "=> STEP 4718/6250   lr: 0.000195   giou_loss: 1.63   conf_loss: 53.69   prob_loss: 0.46   total_loss: 55.78\n",
      "=> STEP 4719/6250   lr: 0.000195   giou_loss: 1.62   conf_loss: 53.69   prob_loss: 0.46   total_loss: 55.77\n",
      "=> STEP 4720/6250   lr: 0.000194   giou_loss: 1.58   conf_loss: 53.67   prob_loss: 0.46   total_loss: 55.71\n",
      "=> STEP 4721/6250   lr: 0.000194   giou_loss: 1.62   conf_loss: 53.66   prob_loss: 0.46   total_loss: 55.74\n",
      "=> STEP 4722/6250   lr: 0.000194   giou_loss: 1.56   conf_loss: 53.64   prob_loss: 0.46   total_loss: 55.66\n",
      "=> STEP 4723/6250   lr: 0.000194   giou_loss: 1.61   conf_loss: 53.63   prob_loss: 0.46   total_loss: 55.70\n",
      "=> STEP 4724/6250   lr: 0.000194   giou_loss: 1.57   conf_loss: 53.62   prob_loss: 0.46   total_loss: 55.64\n",
      "=> STEP 4725/6250   lr: 0.000194   giou_loss: 1.64   conf_loss: 53.60   prob_loss: 0.46   total_loss: 55.70\n",
      "=> STEP 4726/6250   lr: 0.000193   giou_loss: 1.66   conf_loss: 53.59   prob_loss: 0.46   total_loss: 55.71\n",
      "=> STEP 4727/6250   lr: 0.000193   giou_loss: 1.75   conf_loss: 53.58   prob_loss: 0.46   total_loss: 55.79\n",
      "=> STEP 4728/6250   lr: 0.000193   giou_loss: 1.80   conf_loss: 53.57   prob_loss: 0.46   total_loss: 55.82\n",
      "=> STEP 4729/6250   lr: 0.000193   giou_loss: 1.73   conf_loss: 53.56   prob_loss: 0.45   total_loss: 55.75\n",
      "=> STEP 4730/6250   lr: 0.000193   giou_loss: 1.58   conf_loss: 53.56   prob_loss: 0.45   total_loss: 55.59\n",
      "=> STEP 4731/6250   lr: 0.000192   giou_loss: 1.69   conf_loss: 53.55   prob_loss: 0.45   total_loss: 55.69\n",
      "=> STEP 4732/6250   lr: 0.000192   giou_loss: 1.57   conf_loss: 53.53   prob_loss: 0.45   total_loss: 55.55\n",
      "=> STEP 4733/6250   lr: 0.000192   giou_loss: 1.75   conf_loss: 53.51   prob_loss: 0.46   total_loss: 55.72\n",
      "=> STEP 4734/6250   lr: 0.000192   giou_loss: 1.75   conf_loss: 53.49   prob_loss: 0.46   total_loss: 55.70\n",
      "=> STEP 4735/6250   lr: 0.000192   giou_loss: 1.60   conf_loss: 53.49   prob_loss: 0.46   total_loss: 55.54\n",
      "=> STEP 4736/6250   lr: 0.000192   giou_loss: 1.60   conf_loss: 53.48   prob_loss: 0.46   total_loss: 55.53\n",
      "=> STEP 4737/6250   lr: 0.000191   giou_loss: 1.62   conf_loss: 53.46   prob_loss: 0.46   total_loss: 55.53\n",
      "=> STEP 4738/6250   lr: 0.000191   giou_loss: 1.60   conf_loss: 53.45   prob_loss: 0.46   total_loss: 55.51\n",
      "=> STEP 4739/6250   lr: 0.000191   giou_loss: 1.61   conf_loss: 53.44   prob_loss: 0.45   total_loss: 55.51\n",
      "=> STEP 4740/6250   lr: 0.000191   giou_loss: 1.63   conf_loss: 53.43   prob_loss: 0.45   total_loss: 55.52\n",
      "=> STEP 4741/6250   lr: 0.000191   giou_loss: 1.61   conf_loss: 53.42   prob_loss: 0.46   total_loss: 55.48\n",
      "=> STEP 4742/6250   lr: 0.000191   giou_loss: 1.58   conf_loss: 53.41   prob_loss: 0.45   total_loss: 55.45\n",
      "=> STEP 4743/6250   lr: 0.000190   giou_loss: 1.72   conf_loss: 53.39   prob_loss: 0.45   total_loss: 55.57\n",
      "=> STEP 4744/6250   lr: 0.000190   giou_loss: 1.66   conf_loss: 53.39   prob_loss: 0.46   total_loss: 55.50\n",
      "=> STEP 4745/6250   lr: 0.000190   giou_loss: 1.62   conf_loss: 53.36   prob_loss: 0.46   total_loss: 55.44\n",
      "=> STEP 4746/6250   lr: 0.000190   giou_loss: 1.63   conf_loss: 53.36   prob_loss: 0.46   total_loss: 55.45\n",
      "=> STEP 4747/6250   lr: 0.000190   giou_loss: 1.56   conf_loss: 53.34   prob_loss: 0.46   total_loss: 55.36\n",
      "=> STEP 4748/6250   lr: 0.000189   giou_loss: 1.58   conf_loss: 53.33   prob_loss: 0.46   total_loss: 55.37\n",
      "=> STEP 4749/6250   lr: 0.000189   giou_loss: 1.62   conf_loss: 53.33   prob_loss: 0.46   total_loss: 55.41\n",
      "=> STEP 4750/6250   lr: 0.000189   giou_loss: 1.59   conf_loss: 53.31   prob_loss: 0.45   total_loss: 55.36\n",
      "=> STEP 4751/6250   lr: 0.000189   giou_loss: 1.64   conf_loss: 53.31   prob_loss: 0.45   total_loss: 55.40\n",
      "=> STEP 4752/6250   lr: 0.000189   giou_loss: 1.65   conf_loss: 53.30   prob_loss: 0.45   total_loss: 55.40\n",
      "=> STEP 4753/6250   lr: 0.000189   giou_loss: 1.56   conf_loss: 53.28   prob_loss: 0.46   total_loss: 55.30\n",
      "=> STEP 4754/6250   lr: 0.000188   giou_loss: 1.68   conf_loss: 53.27   prob_loss: 0.46   total_loss: 55.41\n",
      "=> STEP 4755/6250   lr: 0.000188   giou_loss: 1.61   conf_loss: 53.26   prob_loss: 0.46   total_loss: 55.33\n",
      "=> STEP 4756/6250   lr: 0.000188   giou_loss: 1.61   conf_loss: 53.25   prob_loss: 0.46   total_loss: 55.32\n",
      "=> STEP 4757/6250   lr: 0.000188   giou_loss: 1.62   conf_loss: 53.24   prob_loss: 0.46   total_loss: 55.32\n",
      "=> STEP 4758/6250   lr: 0.000188   giou_loss: 1.58   conf_loss: 53.22   prob_loss: 0.46   total_loss: 55.26\n",
      "=> STEP 4759/6250   lr: 0.000188   giou_loss: 1.57   conf_loss: 53.22   prob_loss: 0.46   total_loss: 55.24\n",
      "=> STEP 4760/6250   lr: 0.000187   giou_loss: 1.57   conf_loss: 53.19   prob_loss: 0.46   total_loss: 55.22\n",
      "=> STEP 4761/6250   lr: 0.000187   giou_loss: 1.57   conf_loss: 53.19   prob_loss: 0.46   total_loss: 55.21\n",
      "=> STEP 4762/6250   lr: 0.000187   giou_loss: 1.57   conf_loss: 53.17   prob_loss: 0.46   total_loss: 55.20\n",
      "=> STEP 4763/6250   lr: 0.000187   giou_loss: 1.62   conf_loss: 53.16   prob_loss: 0.46   total_loss: 55.24\n",
      "=> STEP 4764/6250   lr: 0.000187   giou_loss: 1.59   conf_loss: 53.16   prob_loss: 0.45   total_loss: 55.20\n",
      "=> STEP 4765/6250   lr: 0.000186   giou_loss: 1.64   conf_loss: 53.14   prob_loss: 0.45   total_loss: 55.24\n",
      "=> STEP 4766/6250   lr: 0.000186   giou_loss: 1.65   conf_loss: 53.14   prob_loss: 0.45   total_loss: 55.24\n",
      "=> STEP 4767/6250   lr: 0.000186   giou_loss: 1.56   conf_loss: 53.12   prob_loss: 0.46   total_loss: 55.14\n",
      "=> STEP 4768/6250   lr: 0.000186   giou_loss: 1.66   conf_loss: 53.11   prob_loss: 0.46   total_loss: 55.22\n",
      "=> STEP 4769/6250   lr: 0.000186   giou_loss: 1.62   conf_loss: 53.11   prob_loss: 0.45   total_loss: 55.18\n",
      "=> STEP 4770/6250   lr: 0.000186   giou_loss: 1.61   conf_loss: 53.09   prob_loss: 0.45   total_loss: 55.16\n",
      "=> STEP 4771/6250   lr: 0.000185   giou_loss: 1.62   conf_loss: 53.10   prob_loss: 0.46   total_loss: 55.18\n",
      "=> STEP 4772/6250   lr: 0.000185   giou_loss: 1.57   conf_loss: 53.06   prob_loss: 0.46   total_loss: 55.09\n",
      "=> STEP 4773/6250   lr: 0.000185   giou_loss: 1.56   conf_loss: 53.11   prob_loss: 0.45   total_loss: 55.12\n",
      "=> STEP 4774/6250   lr: 0.000185   giou_loss: 1.58   conf_loss: 53.05   prob_loss: 0.46   total_loss: 55.08\n",
      "=> STEP 4775/6250   lr: 0.000185   giou_loss: 1.56   conf_loss: 53.11   prob_loss: 0.46   total_loss: 55.13\n",
      "=> STEP 4776/6250   lr: 0.000185   giou_loss: 1.64   conf_loss: 53.03   prob_loss: 0.46   total_loss: 55.12\n",
      "=> STEP 4777/6250   lr: 0.000184   giou_loss: 1.64   conf_loss: 53.12   prob_loss: 0.45   total_loss: 55.21\n",
      "=> STEP 4778/6250   lr: 0.000184   giou_loss: 1.56   conf_loss: 53.13   prob_loss: 0.45   total_loss: 55.15\n",
      "=> STEP 4779/6250   lr: 0.000184   giou_loss: 1.83   conf_loss: 53.05   prob_loss: 0.45   total_loss: 55.33\n",
      "=> STEP 4780/6250   lr: 0.000184   giou_loss: 1.90   conf_loss: 53.02   prob_loss: 0.45   total_loss: 55.37\n",
      "=> STEP 4781/6250   lr: 0.000184   giou_loss: 1.74   conf_loss: 53.01   prob_loss: 0.45   total_loss: 55.20\n",
      "=> STEP 4782/6250   lr: 0.000184   giou_loss: 1.65   conf_loss: 53.00   prob_loss: 0.46   total_loss: 55.12\n",
      "=> STEP 4783/6250   lr: 0.000183   giou_loss: 1.75   conf_loss: 53.00   prob_loss: 0.46   total_loss: 55.21\n",
      "=> STEP 4784/6250   lr: 0.000183   giou_loss: 1.58   conf_loss: 52.95   prob_loss: 0.46   total_loss: 54.99\n",
      "=> STEP 4785/6250   lr: 0.000183   giou_loss: 1.72   conf_loss: 53.02   prob_loss: 0.45   total_loss: 55.19\n",
      "=> STEP 4786/6250   lr: 0.000183   giou_loss: 1.83   conf_loss: 52.95   prob_loss: 0.45   total_loss: 55.23\n",
      "=> STEP 4787/6250   lr: 0.000183   giou_loss: 1.72   conf_loss: 52.97   prob_loss: 0.45   total_loss: 55.14\n",
      "=> STEP 4788/6250   lr: 0.000183   giou_loss: 1.60   conf_loss: 52.97   prob_loss: 0.45   total_loss: 55.02\n",
      "=> STEP 4789/6250   lr: 0.000182   giou_loss: 1.76   conf_loss: 52.92   prob_loss: 0.45   total_loss: 55.14\n",
      "=> STEP 4790/6250   lr: 0.000182   giou_loss: 1.71   conf_loss: 52.92   prob_loss: 0.45   total_loss: 55.08\n",
      "=> STEP 4791/6250   lr: 0.000182   giou_loss: 1.58   conf_loss: 52.91   prob_loss: 0.46   total_loss: 54.94\n",
      "=> STEP 4792/6250   lr: 0.000182   giou_loss: 1.80   conf_loss: 52.89   prob_loss: 0.46   total_loss: 55.16\n",
      "=> STEP 4793/6250   lr: 0.000182   giou_loss: 1.85   conf_loss: 52.88   prob_loss: 0.46   total_loss: 55.19\n",
      "=> STEP 4794/6250   lr: 0.000181   giou_loss: 1.75   conf_loss: 52.88   prob_loss: 0.46   total_loss: 55.09\n",
      "=> STEP 4795/6250   lr: 0.000181   giou_loss: 1.65   conf_loss: 52.86   prob_loss: 0.46   total_loss: 54.97\n",
      "=> STEP 4796/6250   lr: 0.000181   giou_loss: 1.76   conf_loss: 52.84   prob_loss: 0.46   total_loss: 55.06\n",
      "=> STEP 4797/6250   lr: 0.000181   giou_loss: 1.80   conf_loss: 52.83   prob_loss: 0.46   total_loss: 55.09\n",
      "=> STEP 4798/6250   lr: 0.000181   giou_loss: 1.80   conf_loss: 52.83   prob_loss: 0.46   total_loss: 55.09\n",
      "=> STEP 4799/6250   lr: 0.000181   giou_loss: 1.67   conf_loss: 52.82   prob_loss: 0.45   total_loss: 54.95\n",
      "=> STEP 4800/6250   lr: 0.000180   giou_loss: 1.64   conf_loss: 52.80   prob_loss: 0.45   total_loss: 54.89\n",
      "=> STEP 4801/6250   lr: 0.000180   giou_loss: 1.75   conf_loss: 52.79   prob_loss: 0.45   total_loss: 54.99\n",
      "=> STEP 4802/6250   lr: 0.000180   giou_loss: 1.68   conf_loss: 52.81   prob_loss: 0.45   total_loss: 54.93\n",
      "=> STEP 4803/6250   lr: 0.000180   giou_loss: 1.58   conf_loss: 52.79   prob_loss: 0.45   total_loss: 54.83\n",
      "=> STEP 4804/6250   lr: 0.000180   giou_loss: 1.62   conf_loss: 52.75   prob_loss: 0.45   total_loss: 54.82\n",
      "=> STEP 4805/6250   lr: 0.000180   giou_loss: 1.66   conf_loss: 52.79   prob_loss: 0.45   total_loss: 54.90\n",
      "=> STEP 4806/6250   lr: 0.000179   giou_loss: 1.59   conf_loss: 52.76   prob_loss: 0.45   total_loss: 54.80\n",
      "=> STEP 4807/6250   lr: 0.000179   giou_loss: 1.75   conf_loss: 52.73   prob_loss: 0.45   total_loss: 54.93\n",
      "=> STEP 4808/6250   lr: 0.000179   giou_loss: 1.76   conf_loss: 52.74   prob_loss: 0.45   total_loss: 54.95\n",
      "=> STEP 4809/6250   lr: 0.000179   giou_loss: 1.57   conf_loss: 52.72   prob_loss: 0.45   total_loss: 54.74\n",
      "=> STEP 4810/6250   lr: 0.000179   giou_loss: 1.74   conf_loss: 52.69   prob_loss: 0.45   total_loss: 54.89\n",
      "=> STEP 4811/6250   lr: 0.000179   giou_loss: 1.80   conf_loss: 52.69   prob_loss: 0.46   total_loss: 54.95\n",
      "=> STEP 4812/6250   lr: 0.000178   giou_loss: 1.74   conf_loss: 52.68   prob_loss: 0.46   total_loss: 54.88\n",
      "=> STEP 4813/6250   lr: 0.000178   giou_loss: 1.66   conf_loss: 52.65   prob_loss: 0.46   total_loss: 54.77\n",
      "=> STEP 4814/6250   lr: 0.000178   giou_loss: 1.83   conf_loss: 52.65   prob_loss: 0.46   total_loss: 54.95\n",
      "=> STEP 4815/6250   lr: 0.000178   giou_loss: 1.85   conf_loss: 52.65   prob_loss: 0.46   total_loss: 54.97\n",
      "=> STEP 4816/6250   lr: 0.000178   giou_loss: 1.84   conf_loss: 52.64   prob_loss: 0.45   total_loss: 54.93\n",
      "=> STEP 4817/6250   lr: 0.000178   giou_loss: 1.80   conf_loss: 52.63   prob_loss: 0.45   total_loss: 54.88\n",
      "=> STEP 4818/6250   lr: 0.000177   giou_loss: 1.63   conf_loss: 52.64   prob_loss: 0.44   total_loss: 54.71\n",
      "=> STEP 4819/6250   lr: 0.000177   giou_loss: 1.67   conf_loss: 52.63   prob_loss: 0.44   total_loss: 54.75\n",
      "=> STEP 4820/6250   lr: 0.000177   giou_loss: 1.72   conf_loss: 52.60   prob_loss: 0.44   total_loss: 54.77\n",
      "=> STEP 4821/6250   lr: 0.000177   giou_loss: 1.61   conf_loss: 52.61   prob_loss: 0.44   total_loss: 54.66\n",
      "=> STEP 4822/6250   lr: 0.000177   giou_loss: 1.73   conf_loss: 52.62   prob_loss: 0.45   total_loss: 54.80\n",
      "=> STEP 4823/6250   lr: 0.000177   giou_loss: 1.79   conf_loss: 52.58   prob_loss: 0.45   total_loss: 54.81\n",
      "=> STEP 4824/6250   lr: 0.000176   giou_loss: 1.66   conf_loss: 52.57   prob_loss: 0.45   total_loss: 54.68\n",
      "=> STEP 4825/6250   lr: 0.000176   giou_loss: 1.56   conf_loss: 52.58   prob_loss: 0.45   total_loss: 54.60\n",
      "=> STEP 4826/6250   lr: 0.000176   giou_loss: 1.72   conf_loss: 52.55   prob_loss: 0.46   total_loss: 54.73\n",
      "=> STEP 4827/6250   lr: 0.000176   giou_loss: 1.65   conf_loss: 52.52   prob_loss: 0.47   total_loss: 54.64\n",
      "=> STEP 4828/6250   lr: 0.000176   giou_loss: 1.59   conf_loss: 52.56   prob_loss: 0.48   total_loss: 54.63\n",
      "=> STEP 4829/6250   lr: 0.000176   giou_loss: 1.69   conf_loss: 52.54   prob_loss: 0.47   total_loss: 54.70\n",
      "=> STEP 4830/6250   lr: 0.000175   giou_loss: 1.67   conf_loss: 52.51   prob_loss: 0.46   total_loss: 54.64\n",
      "=> STEP 4831/6250   lr: 0.000175   giou_loss: 1.56   conf_loss: 52.52   prob_loss: 0.45   total_loss: 54.53\n",
      "=> STEP 4832/6250   lr: 0.000175   giou_loss: 1.58   conf_loss: 52.54   prob_loss: 0.44   total_loss: 54.56\n",
      "=> STEP 4833/6250   lr: 0.000175   giou_loss: 1.63   conf_loss: 52.51   prob_loss: 0.44   total_loss: 54.59\n",
      "=> STEP 4834/6250   lr: 0.000175   giou_loss: 1.57   conf_loss: 52.49   prob_loss: 0.44   total_loss: 54.50\n",
      "=> STEP 4835/6250   lr: 0.000175   giou_loss: 1.70   conf_loss: 52.48   prob_loss: 0.44   total_loss: 54.62\n",
      "=> STEP 4836/6250   lr: 0.000174   giou_loss: 1.68   conf_loss: 52.48   prob_loss: 0.44   total_loss: 54.60\n",
      "=> STEP 4837/6250   lr: 0.000174   giou_loss: 1.62   conf_loss: 52.44   prob_loss: 0.45   total_loss: 54.51\n",
      "=> STEP 4838/6250   lr: 0.000174   giou_loss: 1.64   conf_loss: 52.42   prob_loss: 0.46   total_loss: 54.52\n",
      "=> STEP 4839/6250   lr: 0.000174   giou_loss: 1.56   conf_loss: 52.44   prob_loss: 0.46   total_loss: 54.46\n",
      "=> STEP 4840/6250   lr: 0.000174   giou_loss: 1.70   conf_loss: 52.40   prob_loss: 0.45   total_loss: 54.56\n",
      "=> STEP 4841/6250   lr: 0.000174   giou_loss: 1.67   conf_loss: 52.40   prob_loss: 0.46   total_loss: 54.53\n",
      "=> STEP 4842/6250   lr: 0.000173   giou_loss: 1.63   conf_loss: 52.40   prob_loss: 0.46   total_loss: 54.49\n",
      "=> STEP 4843/6250   lr: 0.000173   giou_loss: 1.57   conf_loss: 52.38   prob_loss: 0.45   total_loss: 54.40\n",
      "=> STEP 4844/6250   lr: 0.000173   giou_loss: 1.59   conf_loss: 52.36   prob_loss: 0.45   total_loss: 54.41\n",
      "=> STEP 4845/6250   lr: 0.000173   giou_loss: 1.63   conf_loss: 52.36   prob_loss: 0.45   total_loss: 54.45\n",
      "=> STEP 4846/6250   lr: 0.000173   giou_loss: 1.56   conf_loss: 52.33   prob_loss: 0.45   total_loss: 54.35\n",
      "=> STEP 4847/6250   lr: 0.000173   giou_loss: 1.57   conf_loss: 52.33   prob_loss: 0.45   total_loss: 54.35\n",
      "=> STEP 4848/6250   lr: 0.000172   giou_loss: 1.56   conf_loss: 52.31   prob_loss: 0.45   total_loss: 54.33\n",
      "=> STEP 4849/6250   lr: 0.000172   giou_loss: 1.59   conf_loss: 52.29   prob_loss: 0.45   total_loss: 54.33\n",
      "=> STEP 4850/6250   lr: 0.000172   giou_loss: 1.62   conf_loss: 52.27   prob_loss: 0.46   total_loss: 54.35\n",
      "=> STEP 4851/6250   lr: 0.000172   giou_loss: 1.57   conf_loss: 52.27   prob_loss: 0.45   total_loss: 54.29\n",
      "=> STEP 4852/6250   lr: 0.000172   giou_loss: 1.62   conf_loss: 52.26   prob_loss: 0.45   total_loss: 54.33\n",
      "=> STEP 4853/6250   lr: 0.000172   giou_loss: 1.57   conf_loss: 52.24   prob_loss: 0.45   total_loss: 54.26\n",
      "=> STEP 4854/6250   lr: 0.000171   giou_loss: 1.62   conf_loss: 52.23   prob_loss: 0.45   total_loss: 54.31\n",
      "=> STEP 4855/6250   lr: 0.000171   giou_loss: 1.63   conf_loss: 52.23   prob_loss: 0.45   total_loss: 54.31\n",
      "=> STEP 4856/6250   lr: 0.000171   giou_loss: 1.61   conf_loss: 52.22   prob_loss: 0.45   total_loss: 54.27\n",
      "=> STEP 4857/6250   lr: 0.000171   giou_loss: 1.56   conf_loss: 52.20   prob_loss: 0.45   total_loss: 54.22\n",
      "=> STEP 4858/6250   lr: 0.000171   giou_loss: 1.60   conf_loss: 52.19   prob_loss: 0.45   total_loss: 54.25\n",
      "=> STEP 4859/6250   lr: 0.000171   giou_loss: 1.56   conf_loss: 52.18   prob_loss: 0.45   total_loss: 54.19\n",
      "=> STEP 4860/6250   lr: 0.000170   giou_loss: 1.63   conf_loss: 52.17   prob_loss: 0.45   total_loss: 54.25\n",
      "=> STEP 4861/6250   lr: 0.000170   giou_loss: 1.61   conf_loss: 52.16   prob_loss: 0.45   total_loss: 54.22\n",
      "=> STEP 4862/6250   lr: 0.000170   giou_loss: 1.67   conf_loss: 52.14   prob_loss: 0.46   total_loss: 54.27\n",
      "=> STEP 4863/6250   lr: 0.000170   giou_loss: 1.71   conf_loss: 52.13   prob_loss: 0.46   total_loss: 54.30\n",
      "=> STEP 4864/6250   lr: 0.000170   giou_loss: 1.66   conf_loss: 52.13   prob_loss: 0.45   total_loss: 54.24\n",
      "=> STEP 4865/6250   lr: 0.000170   giou_loss: 1.62   conf_loss: 52.12   prob_loss: 0.45   total_loss: 54.19\n",
      "=> STEP 4866/6250   lr: 0.000169   giou_loss: 1.66   conf_loss: 52.11   prob_loss: 0.45   total_loss: 54.22\n",
      "=> STEP 4867/6250   lr: 0.000169   giou_loss: 1.63   conf_loss: 52.10   prob_loss: 0.45   total_loss: 54.18\n",
      "=> STEP 4868/6250   lr: 0.000169   giou_loss: 1.66   conf_loss: 52.10   prob_loss: 0.45   total_loss: 54.21\n",
      "=> STEP 4869/6250   lr: 0.000169   giou_loss: 1.71   conf_loss: 52.08   prob_loss: 0.45   total_loss: 54.25\n",
      "=> STEP 4870/6250   lr: 0.000169   giou_loss: 1.65   conf_loss: 52.07   prob_loss: 0.45   total_loss: 54.18\n",
      "=> STEP 4871/6250   lr: 0.000169   giou_loss: 1.60   conf_loss: 52.07   prob_loss: 0.45   total_loss: 54.11\n",
      "=> STEP 4872/6250   lr: 0.000168   giou_loss: 1.62   conf_loss: 52.06   prob_loss: 0.45   total_loss: 54.12\n",
      "=> STEP 4873/6250   lr: 0.000168   giou_loss: 1.56   conf_loss: 52.05   prob_loss: 0.45   total_loss: 54.06\n",
      "=> STEP 4874/6250   lr: 0.000168   giou_loss: 1.65   conf_loss: 52.04   prob_loss: 0.45   total_loss: 54.13\n",
      "=> STEP 4875/6250   lr: 0.000168   giou_loss: 1.57   conf_loss: 52.02   prob_loss: 0.45   total_loss: 54.04\n",
      "=> STEP 4876/6250   lr: 0.000168   giou_loss: 1.58   conf_loss: 52.02   prob_loss: 0.45   total_loss: 54.05\n",
      "=> STEP 4877/6250   lr: 0.000168   giou_loss: 1.60   conf_loss: 52.00   prob_loss: 0.45   total_loss: 54.05\n",
      "=> STEP 4878/6250   lr: 0.000167   giou_loss: 1.61   conf_loss: 52.00   prob_loss: 0.45   total_loss: 54.05\n",
      "=> STEP 4879/6250   lr: 0.000167   giou_loss: 1.56   conf_loss: 51.99   prob_loss: 0.45   total_loss: 54.00\n",
      "=> STEP 4880/6250   lr: 0.000167   giou_loss: 1.65   conf_loss: 51.98   prob_loss: 0.45   total_loss: 54.07\n",
      "=> STEP 4881/6250   lr: 0.000167   giou_loss: 1.57   conf_loss: 51.96   prob_loss: 0.45   total_loss: 53.98\n",
      "=> STEP 4882/6250   lr: 0.000167   giou_loss: 1.57   conf_loss: 51.96   prob_loss: 0.45   total_loss: 53.98\n",
      "=> STEP 4883/6250   lr: 0.000167   giou_loss: 1.59   conf_loss: 51.94   prob_loss: 0.45   total_loss: 53.99\n",
      "=> STEP 4884/6250   lr: 0.000166   giou_loss: 1.56   conf_loss: 51.94   prob_loss: 0.45   total_loss: 53.95\n",
      "=> STEP 4885/6250   lr: 0.000166   giou_loss: 1.62   conf_loss: 51.92   prob_loss: 0.45   total_loss: 53.99\n",
      "=> STEP 4886/6250   lr: 0.000166   giou_loss: 1.64   conf_loss: 51.91   prob_loss: 0.45   total_loss: 54.00\n",
      "=> STEP 4887/6250   lr: 0.000166   giou_loss: 1.60   conf_loss: 51.91   prob_loss: 0.45   total_loss: 53.96\n",
      "=> STEP 4888/6250   lr: 0.000166   giou_loss: 1.57   conf_loss: 51.89   prob_loss: 0.45   total_loss: 53.91\n",
      "=> STEP 4889/6250   lr: 0.000166   giou_loss: 1.69   conf_loss: 51.88   prob_loss: 0.45   total_loss: 54.02\n",
      "=> STEP 4890/6250   lr: 0.000165   giou_loss: 1.67   conf_loss: 51.88   prob_loss: 0.45   total_loss: 54.00\n",
      "=> STEP 4891/6250   lr: 0.000165   giou_loss: 1.57   conf_loss: 51.86   prob_loss: 0.45   total_loss: 53.89\n",
      "=> STEP 4892/6250   lr: 0.000165   giou_loss: 1.65   conf_loss: 51.86   prob_loss: 0.45   total_loss: 53.96\n",
      "=> STEP 4893/6250   lr: 0.000165   giou_loss: 1.66   conf_loss: 51.85   prob_loss: 0.45   total_loss: 53.96\n",
      "=> STEP 4894/6250   lr: 0.000165   giou_loss: 1.65   conf_loss: 51.83   prob_loss: 0.45   total_loss: 53.93\n",
      "=> STEP 4895/6250   lr: 0.000165   giou_loss: 1.57   conf_loss: 51.82   prob_loss: 0.45   total_loss: 53.85\n",
      "=> STEP 4896/6250   lr: 0.000164   giou_loss: 1.68   conf_loss: 51.82   prob_loss: 0.45   total_loss: 53.95\n",
      "=> STEP 4897/6250   lr: 0.000164   giou_loss: 1.71   conf_loss: 51.81   prob_loss: 0.45   total_loss: 53.98\n",
      "=> STEP 4898/6250   lr: 0.000164   giou_loss: 1.59   conf_loss: 51.79   prob_loss: 0.45   total_loss: 53.83\n",
      "=> STEP 4899/6250   lr: 0.000164   giou_loss: 1.65   conf_loss: 51.80   prob_loss: 0.45   total_loss: 53.90\n",
      "=> STEP 4900/6250   lr: 0.000164   giou_loss: 1.69   conf_loss: 51.78   prob_loss: 0.45   total_loss: 53.92\n",
      "=> STEP 4901/6250   lr: 0.000164   giou_loss: 1.63   conf_loss: 51.77   prob_loss: 0.45   total_loss: 53.85\n",
      "=> STEP 4902/6250   lr: 0.000163   giou_loss: 1.58   conf_loss: 51.77   prob_loss: 0.45   total_loss: 53.80\n",
      "=> STEP 4903/6250   lr: 0.000163   giou_loss: 1.60   conf_loss: 51.76   prob_loss: 0.45   total_loss: 53.81\n",
      "=> STEP 4904/6250   lr: 0.000163   giou_loss: 1.56   conf_loss: 51.75   prob_loss: 0.45   total_loss: 53.76\n",
      "=> STEP 4905/6250   lr: 0.000163   giou_loss: 1.63   conf_loss: 51.74   prob_loss: 0.45   total_loss: 53.82\n",
      "=> STEP 4906/6250   lr: 0.000163   giou_loss: 1.59   conf_loss: 51.73   prob_loss: 0.45   total_loss: 53.76\n",
      "=> STEP 4907/6250   lr: 0.000163   giou_loss: 1.62   conf_loss: 51.72   prob_loss: 0.45   total_loss: 53.79\n",
      "=> STEP 4908/6250   lr: 0.000163   giou_loss: 1.64   conf_loss: 51.70   prob_loss: 0.45   total_loss: 53.80\n",
      "=> STEP 4909/6250   lr: 0.000162   giou_loss: 1.56   conf_loss: 51.70   prob_loss: 0.45   total_loss: 53.71\n",
      "=> STEP 4910/6250   lr: 0.000162   giou_loss: 1.58   conf_loss: 51.69   prob_loss: 0.45   total_loss: 53.72\n",
      "=> STEP 4911/6250   lr: 0.000162   giou_loss: 1.56   conf_loss: 51.67   prob_loss: 0.45   total_loss: 53.69\n",
      "=> STEP 4912/6250   lr: 0.000162   giou_loss: 1.56   conf_loss: 51.67   prob_loss: 0.45   total_loss: 53.68\n",
      "=> STEP 4913/6250   lr: 0.000162   giou_loss: 1.59   conf_loss: 51.66   prob_loss: 0.45   total_loss: 53.70\n",
      "=> STEP 4914/6250   lr: 0.000162   giou_loss: 1.56   conf_loss: 51.65   prob_loss: 0.45   total_loss: 53.66\n",
      "=> STEP 4915/6250   lr: 0.000161   giou_loss: 1.61   conf_loss: 51.64   prob_loss: 0.45   total_loss: 53.70\n",
      "=> STEP 4916/6250   lr: 0.000161   giou_loss: 1.56   conf_loss: 51.63   prob_loss: 0.45   total_loss: 53.64\n",
      "=> STEP 4917/6250   lr: 0.000161   giou_loss: 1.56   conf_loss: 51.62   prob_loss: 0.45   total_loss: 53.63\n",
      "=> STEP 4918/6250   lr: 0.000161   giou_loss: 1.58   conf_loss: 51.61   prob_loss: 0.45   total_loss: 53.64\n",
      "=> STEP 4919/6250   lr: 0.000161   giou_loss: 1.56   conf_loss: 51.60   prob_loss: 0.45   total_loss: 53.61\n",
      "=> STEP 4920/6250   lr: 0.000161   giou_loss: 1.61   conf_loss: 51.59   prob_loss: 0.45   total_loss: 53.65\n",
      "=> STEP 4921/6250   lr: 0.000160   giou_loss: 1.58   conf_loss: 51.58   prob_loss: 0.45   total_loss: 53.61\n",
      "=> STEP 4922/6250   lr: 0.000160   giou_loss: 1.56   conf_loss: 51.57   prob_loss: 0.45   total_loss: 53.59\n",
      "=> STEP 4923/6250   lr: 0.000160   giou_loss: 1.57   conf_loss: 51.56   prob_loss: 0.45   total_loss: 53.59\n",
      "=> STEP 4924/6250   lr: 0.000160   giou_loss: 1.57   conf_loss: 51.55   prob_loss: 0.45   total_loss: 53.57\n",
      "=> STEP 4925/6250   lr: 0.000160   giou_loss: 1.63   conf_loss: 51.54   prob_loss: 0.45   total_loss: 53.62\n",
      "=> STEP 4926/6250   lr: 0.000160   giou_loss: 1.57   conf_loss: 51.54   prob_loss: 0.45   total_loss: 53.56\n",
      "=> STEP 4927/6250   lr: 0.000159   giou_loss: 1.65   conf_loss: 51.53   prob_loss: 0.45   total_loss: 53.62\n",
      "=> STEP 4928/6250   lr: 0.000159   giou_loss: 1.67   conf_loss: 51.52   prob_loss: 0.45   total_loss: 53.64\n",
      "=> STEP 4929/6250   lr: 0.000159   giou_loss: 1.59   conf_loss: 51.51   prob_loss: 0.45   total_loss: 53.55\n",
      "=> STEP 4930/6250   lr: 0.000159   giou_loss: 1.65   conf_loss: 51.50   prob_loss: 0.45   total_loss: 53.60\n",
      "=> STEP 4931/6250   lr: 0.000159   giou_loss: 1.70   conf_loss: 51.49   prob_loss: 0.45   total_loss: 53.64\n",
      "=> STEP 4932/6250   lr: 0.000159   giou_loss: 1.63   conf_loss: 51.48   prob_loss: 0.45   total_loss: 53.56\n",
      "=> STEP 4933/6250   lr: 0.000159   giou_loss: 1.62   conf_loss: 51.48   prob_loss: 0.45   total_loss: 53.54\n",
      "=> STEP 4934/6250   lr: 0.000158   giou_loss: 1.60   conf_loss: 51.47   prob_loss: 0.45   total_loss: 53.52\n",
      "=> STEP 4935/6250   lr: 0.000158   giou_loss: 1.57   conf_loss: 51.45   prob_loss: 0.45   total_loss: 53.47\n",
      "=> STEP 4936/6250   lr: 0.000158   giou_loss: 1.66   conf_loss: 51.45   prob_loss: 0.45   total_loss: 53.56\n",
      "=> STEP 4937/6250   lr: 0.000158   giou_loss: 1.58   conf_loss: 51.44   prob_loss: 0.45   total_loss: 53.47\n",
      "=> STEP 4938/6250   lr: 0.000158   giou_loss: 1.63   conf_loss: 51.43   prob_loss: 0.45   total_loss: 53.51\n",
      "=> STEP 4939/6250   lr: 0.000158   giou_loss: 1.68   conf_loss: 51.43   prob_loss: 0.45   total_loss: 53.55\n",
      "=> STEP 4940/6250   lr: 0.000157   giou_loss: 1.56   conf_loss: 51.41   prob_loss: 0.45   total_loss: 53.42\n",
      "=> STEP 4941/6250   lr: 0.000157   giou_loss: 1.69   conf_loss: 51.40   prob_loss: 0.45   total_loss: 53.54\n",
      "=> STEP 4942/6250   lr: 0.000157   giou_loss: 1.78   conf_loss: 51.39   prob_loss: 0.45   total_loss: 53.62\n",
      "=> STEP 4943/6250   lr: 0.000157   giou_loss: 1.71   conf_loss: 51.38   prob_loss: 0.45   total_loss: 53.54\n",
      "=> STEP 4944/6250   lr: 0.000157   giou_loss: 1.57   conf_loss: 51.38   prob_loss: 0.45   total_loss: 53.40\n",
      "=> STEP 4945/6250   lr: 0.000157   giou_loss: 1.74   conf_loss: 51.38   prob_loss: 0.45   total_loss: 53.57\n",
      "=> STEP 4946/6250   lr: 0.000156   giou_loss: 1.80   conf_loss: 51.37   prob_loss: 0.45   total_loss: 53.62\n",
      "=> STEP 4947/6250   lr: 0.000156   giou_loss: 1.79   conf_loss: 51.36   prob_loss: 0.45   total_loss: 53.60\n",
      "=> STEP 4948/6250   lr: 0.000156   giou_loss: 1.70   conf_loss: 51.35   prob_loss: 0.45   total_loss: 53.50\n",
      "=> STEP 4949/6250   lr: 0.000156   giou_loss: 1.57   conf_loss: 51.34   prob_loss: 0.45   total_loss: 53.36\n",
      "=> STEP 4950/6250   lr: 0.000156   giou_loss: 1.69   conf_loss: 51.33   prob_loss: 0.45   total_loss: 53.47\n",
      "=> STEP 4951/6250   lr: 0.000156   giou_loss: 1.71   conf_loss: 51.32   prob_loss: 0.45   total_loss: 53.49\n",
      "=> STEP 4952/6250   lr: 0.000156   giou_loss: 1.70   conf_loss: 51.32   prob_loss: 0.45   total_loss: 53.47\n",
      "=> STEP 4953/6250   lr: 0.000155   giou_loss: 1.60   conf_loss: 51.31   prob_loss: 0.45   total_loss: 53.36\n",
      "=> STEP 4954/6250   lr: 0.000155   giou_loss: 1.66   conf_loss: 51.30   prob_loss: 0.45   total_loss: 53.41\n",
      "=> STEP 4955/6250   lr: 0.000155   giou_loss: 1.72   conf_loss: 51.29   prob_loss: 0.45   total_loss: 53.46\n",
      "=> STEP 4956/6250   lr: 0.000155   giou_loss: 1.65   conf_loss: 51.29   prob_loss: 0.45   total_loss: 53.40\n",
      "=> STEP 4957/6250   lr: 0.000155   giou_loss: 1.65   conf_loss: 51.28   prob_loss: 0.45   total_loss: 53.38\n",
      "=> STEP 4958/6250   lr: 0.000155   giou_loss: 1.68   conf_loss: 51.26   prob_loss: 0.45   total_loss: 53.39\n",
      "=> STEP 4959/6250   lr: 0.000154   giou_loss: 1.67   conf_loss: 51.26   prob_loss: 0.45   total_loss: 53.39\n",
      "=> STEP 4960/6250   lr: 0.000154   giou_loss: 1.63   conf_loss: 51.27   prob_loss: 0.45   total_loss: 53.36\n",
      "=> STEP 4961/6250   lr: 0.000154   giou_loss: 1.59   conf_loss: 51.25   prob_loss: 0.45   total_loss: 53.29\n",
      "=> STEP 4962/6250   lr: 0.000154   giou_loss: 1.64   conf_loss: 51.24   prob_loss: 0.45   total_loss: 53.33\n",
      "=> STEP 4963/6250   lr: 0.000154   giou_loss: 1.64   conf_loss: 51.24   prob_loss: 0.45   total_loss: 53.33\n",
      "=> STEP 4964/6250   lr: 0.000154   giou_loss: 1.56   conf_loss: 51.23   prob_loss: 0.45   total_loss: 53.24\n",
      "=> STEP 4965/6250   lr: 0.000154   giou_loss: 1.59   conf_loss: 51.21   prob_loss: 0.45   total_loss: 53.25\n",
      "=> STEP 4966/6250   lr: 0.000153   giou_loss: 1.58   conf_loss: 51.20   prob_loss: 0.45   total_loss: 53.23\n",
      "=> STEP 4967/6250   lr: 0.000153   giou_loss: 1.60   conf_loss: 51.20   prob_loss: 0.45   total_loss: 53.25\n",
      "=> STEP 4968/6250   lr: 0.000153   giou_loss: 1.57   conf_loss: 51.18   prob_loss: 0.45   total_loss: 53.20\n",
      "=> STEP 4969/6250   lr: 0.000153   giou_loss: 1.62   conf_loss: 51.17   prob_loss: 0.45   total_loss: 53.24\n",
      "=> STEP 4970/6250   lr: 0.000153   giou_loss: 1.63   conf_loss: 51.17   prob_loss: 0.45   total_loss: 53.25\n",
      "=> STEP 4971/6250   lr: 0.000153   giou_loss: 1.56   conf_loss: 51.16   prob_loss: 0.45   total_loss: 53.18\n",
      "=> STEP 4972/6250   lr: 0.000152   giou_loss: 1.58   conf_loss: 51.15   prob_loss: 0.45   total_loss: 53.17\n",
      "=> STEP 4973/6250   lr: 0.000152   giou_loss: 1.57   conf_loss: 51.13   prob_loss: 0.45   total_loss: 53.15\n",
      "=> STEP 4974/6250   lr: 0.000152   giou_loss: 1.57   conf_loss: 51.13   prob_loss: 0.45   total_loss: 53.15\n",
      "=> STEP 4975/6250   lr: 0.000152   giou_loss: 1.60   conf_loss: 51.12   prob_loss: 0.45   total_loss: 53.17\n",
      "=> STEP 4976/6250   lr: 0.000152   giou_loss: 1.57   conf_loss: 51.11   prob_loss: 0.45   total_loss: 53.12\n",
      "=> STEP 4977/6250   lr: 0.000152   giou_loss: 1.60   conf_loss: 51.10   prob_loss: 0.45   total_loss: 53.15\n",
      "=> STEP 4978/6250   lr: 0.000152   giou_loss: 1.57   conf_loss: 51.09   prob_loss: 0.45   total_loss: 53.11\n",
      "=> STEP 4979/6250   lr: 0.000151   giou_loss: 1.56   conf_loss: 51.08   prob_loss: 0.45   total_loss: 53.09\n",
      "=> STEP 4980/6250   lr: 0.000151   giou_loss: 1.58   conf_loss: 51.07   prob_loss: 0.45   total_loss: 53.10\n",
      "=> STEP 4981/6250   lr: 0.000151   giou_loss: 1.56   conf_loss: 51.06   prob_loss: 0.45   total_loss: 53.08\n",
      "=> STEP 4982/6250   lr: 0.000151   giou_loss: 1.60   conf_loss: 51.05   prob_loss: 0.45   total_loss: 53.10\n",
      "=> STEP 4983/6250   lr: 0.000151   giou_loss: 1.56   conf_loss: 51.04   prob_loss: 0.45   total_loss: 53.06\n",
      "=> STEP 4984/6250   lr: 0.000151   giou_loss: 1.56   conf_loss: 51.04   prob_loss: 0.45   total_loss: 53.05\n",
      "=> STEP 4985/6250   lr: 0.000150   giou_loss: 1.59   conf_loss: 51.03   prob_loss: 0.45   total_loss: 53.06\n",
      "=> STEP 4986/6250   lr: 0.000150   giou_loss: 1.56   conf_loss: 51.02   prob_loss: 0.45   total_loss: 53.03\n",
      "=> STEP 4987/6250   lr: 0.000150   giou_loss: 1.58   conf_loss: 51.01   prob_loss: 0.45   total_loss: 53.04\n",
      "=> STEP 4988/6250   lr: 0.000150   giou_loss: 1.56   conf_loss: 51.00   prob_loss: 0.45   total_loss: 53.01\n",
      "=> STEP 4989/6250   lr: 0.000150   giou_loss: 1.59   conf_loss: 51.00   prob_loss: 0.45   total_loss: 53.03\n",
      "=> STEP 4990/6250   lr: 0.000150   giou_loss: 1.56   conf_loss: 50.99   prob_loss: 0.45   total_loss: 53.00\n",
      "=> STEP 4991/6250   lr: 0.000150   giou_loss: 1.64   conf_loss: 50.97   prob_loss: 0.45   total_loss: 53.07\n",
      "=> STEP 4992/6250   lr: 0.000149   giou_loss: 1.57   conf_loss: 50.98   prob_loss: 0.45   total_loss: 52.99\n",
      "=> STEP 4993/6250   lr: 0.000149   giou_loss: 1.64   conf_loss: 50.97   prob_loss: 0.45   total_loss: 53.06\n",
      "=> STEP 4994/6250   lr: 0.000149   giou_loss: 1.68   conf_loss: 50.96   prob_loss: 0.45   total_loss: 53.08\n",
      "=> STEP 4995/6250   lr: 0.000149   giou_loss: 1.61   conf_loss: 50.96   prob_loss: 0.45   total_loss: 53.02\n",
      "=> STEP 4996/6250   lr: 0.000149   giou_loss: 1.60   conf_loss: 50.95   prob_loss: 0.45   total_loss: 53.00\n",
      "=> STEP 4997/6250   lr: 0.000149   giou_loss: 1.64   conf_loss: 50.93   prob_loss: 0.45   total_loss: 53.02\n",
      "=> STEP 4998/6250   lr: 0.000148   giou_loss: 1.58   conf_loss: 50.93   prob_loss: 0.45   total_loss: 52.96\n",
      "=> STEP 4999/6250   lr: 0.000148   giou_loss: 1.63   conf_loss: 50.93   prob_loss: 0.45   total_loss: 53.01\n",
      "=> STEP 5000/6250   lr: 0.000148   giou_loss: 1.67   conf_loss: 50.91   prob_loss: 0.45   total_loss: 53.03\n",
      "=> STEP 5001/6250   lr: 0.000148   giou_loss: 1.60   conf_loss: 50.92   prob_loss: 0.45   total_loss: 52.96\n",
      "=> STEP 5002/6250   lr: 0.000148   giou_loss: 1.61   conf_loss: 50.92   prob_loss: 0.45   total_loss: 52.98\n",
      "=> STEP 5003/6250   lr: 0.000148   giou_loss: 1.64   conf_loss: 50.89   prob_loss: 0.45   total_loss: 52.98\n",
      "=> STEP 5004/6250   lr: 0.000148   giou_loss: 1.56   conf_loss: 50.90   prob_loss: 0.45   total_loss: 52.91\n",
      "=> STEP 5005/6250   lr: 0.000147   giou_loss: 1.60   conf_loss: 50.89   prob_loss: 0.45   total_loss: 52.93\n",
      "=> STEP 5006/6250   lr: 0.000147   giou_loss: 1.59   conf_loss: 50.88   prob_loss: 0.45   total_loss: 52.92\n",
      "=> STEP 5007/6250   lr: 0.000147   giou_loss: 1.56   conf_loss: 50.87   prob_loss: 0.45   total_loss: 52.88\n",
      "=> STEP 5008/6250   lr: 0.000147   giou_loss: 1.58   conf_loss: 50.85   prob_loss: 0.45   total_loss: 52.88\n",
      "=> STEP 5009/6250   lr: 0.000147   giou_loss: 1.56   conf_loss: 50.86   prob_loss: 0.45   total_loss: 52.87\n",
      "=> STEP 5010/6250   lr: 0.000147   giou_loss: 1.60   conf_loss: 50.85   prob_loss: 0.45   total_loss: 52.89\n",
      "=> STEP 5011/6250   lr: 0.000146   giou_loss: 1.59   conf_loss: 50.82   prob_loss: 0.45   total_loss: 52.86\n",
      "=> STEP 5012/6250   lr: 0.000146   giou_loss: 1.56   conf_loss: 50.83   prob_loss: 0.45   total_loss: 52.84\n",
      "=> STEP 5013/6250   lr: 0.000146   giou_loss: 1.59   conf_loss: 50.81   prob_loss: 0.45   total_loss: 52.85\n",
      "=> STEP 5014/6250   lr: 0.000146   giou_loss: 1.57   conf_loss: 50.80   prob_loss: 0.45   total_loss: 52.82\n",
      "=> STEP 5015/6250   lr: 0.000146   giou_loss: 1.58   conf_loss: 50.80   prob_loss: 0.45   total_loss: 52.83\n",
      "=> STEP 5016/6250   lr: 0.000146   giou_loss: 1.56   conf_loss: 50.78   prob_loss: 0.45   total_loss: 52.79\n",
      "=> STEP 5017/6250   lr: 0.000146   giou_loss: 1.56   conf_loss: 50.78   prob_loss: 0.45   total_loss: 52.79\n",
      "=> STEP 5018/6250   lr: 0.000145   giou_loss: 1.60   conf_loss: 50.77   prob_loss: 0.45   total_loss: 52.82\n",
      "=> STEP 5019/6250   lr: 0.000145   giou_loss: 1.56   conf_loss: 50.75   prob_loss: 0.45   total_loss: 52.76\n",
      "=> STEP 5020/6250   lr: 0.000145   giou_loss: 1.60   conf_loss: 50.76   prob_loss: 0.45   total_loss: 52.80\n",
      "=> STEP 5021/6250   lr: 0.000145   giou_loss: 1.56   conf_loss: 50.74   prob_loss: 0.45   total_loss: 52.75\n",
      "=> STEP 5022/6250   lr: 0.000145   giou_loss: 1.57   conf_loss: 50.73   prob_loss: 0.45   total_loss: 52.74\n",
      "=> STEP 5023/6250   lr: 0.000145   giou_loss: 1.59   conf_loss: 50.73   prob_loss: 0.45   total_loss: 52.77\n",
      "=> STEP 5024/6250   lr: 0.000145   giou_loss: 1.57   conf_loss: 50.71   prob_loss: 0.45   total_loss: 52.73\n",
      "=> STEP 5025/6250   lr: 0.000144   giou_loss: 1.61   conf_loss: 50.71   prob_loss: 0.45   total_loss: 52.76\n",
      "=> STEP 5026/6250   lr: 0.000144   giou_loss: 1.60   conf_loss: 50.70   prob_loss: 0.45   total_loss: 52.75\n",
      "=> STEP 5027/6250   lr: 0.000144   giou_loss: 1.62   conf_loss: 50.69   prob_loss: 0.45   total_loss: 52.75\n",
      "=> STEP 5028/6250   lr: 0.000144   giou_loss: 1.65   conf_loss: 50.68   prob_loss: 0.45   total_loss: 52.78\n",
      "=> STEP 5029/6250   lr: 0.000144   giou_loss: 1.58   conf_loss: 50.67   prob_loss: 0.45   total_loss: 52.70\n",
      "=> STEP 5030/6250   lr: 0.000144   giou_loss: 1.64   conf_loss: 50.67   prob_loss: 0.45   total_loss: 52.75\n",
      "=> STEP 5031/6250   lr: 0.000144   giou_loss: 1.68   conf_loss: 50.66   prob_loss: 0.45   total_loss: 52.79\n",
      "=> STEP 5032/6250   lr: 0.000143   giou_loss: 1.64   conf_loss: 50.65   prob_loss: 0.45   total_loss: 52.73\n",
      "=> STEP 5033/6250   lr: 0.000143   giou_loss: 1.56   conf_loss: 50.65   prob_loss: 0.45   total_loss: 52.66\n",
      "=> STEP 5034/6250   lr: 0.000143   giou_loss: 1.68   conf_loss: 50.64   prob_loss: 0.45   total_loss: 52.76\n",
      "=> STEP 5035/6250   lr: 0.000143   giou_loss: 1.69   conf_loss: 50.62   prob_loss: 0.45   total_loss: 52.76\n",
      "=> STEP 5036/6250   lr: 0.000143   giou_loss: 1.61   conf_loss: 50.62   prob_loss: 0.45   total_loss: 52.68\n",
      "=> STEP 5037/6250   lr: 0.000143   giou_loss: 1.61   conf_loss: 50.62   prob_loss: 0.45   total_loss: 52.67\n",
      "=> STEP 5038/6250   lr: 0.000142   giou_loss: 1.65   conf_loss: 50.60   prob_loss: 0.45   total_loss: 52.70\n",
      "=> STEP 5039/6250   lr: 0.000142   giou_loss: 1.60   conf_loss: 50.60   prob_loss: 0.45   total_loss: 52.65\n",
      "=> STEP 5040/6250   lr: 0.000142   giou_loss: 1.60   conf_loss: 50.60   prob_loss: 0.45   total_loss: 52.64\n",
      "=> STEP 5041/6250   lr: 0.000142   giou_loss: 1.62   conf_loss: 50.58   prob_loss: 0.45   total_loss: 52.65\n",
      "=> STEP 5042/6250   lr: 0.000142   giou_loss: 1.56   conf_loss: 50.57   prob_loss: 0.45   total_loss: 52.58\n",
      "=> STEP 5043/6250   lr: 0.000142   giou_loss: 1.57   conf_loss: 50.57   prob_loss: 0.45   total_loss: 52.59\n",
      "=> STEP 5044/6250   lr: 0.000142   giou_loss: 1.56   conf_loss: 50.56   prob_loss: 0.45   total_loss: 52.57\n",
      "=> STEP 5045/6250   lr: 0.000141   giou_loss: 1.56   conf_loss: 50.55   prob_loss: 0.45   total_loss: 52.56\n",
      "=> STEP 5046/6250   lr: 0.000141   giou_loss: 1.60   conf_loss: 50.54   prob_loss: 0.45   total_loss: 52.59\n",
      "=> STEP 5047/6250   lr: 0.000141   giou_loss: 1.56   conf_loss: 50.53   prob_loss: 0.45   total_loss: 52.55\n",
      "=> STEP 5048/6250   lr: 0.000141   giou_loss: 1.57   conf_loss: 50.53   prob_loss: 0.45   total_loss: 52.54\n",
      "=> STEP 5049/6250   lr: 0.000141   giou_loss: 1.63   conf_loss: 50.51   prob_loss: 0.45   total_loss: 52.59\n",
      "=> STEP 5050/6250   lr: 0.000141   giou_loss: 1.61   conf_loss: 50.51   prob_loss: 0.45   total_loss: 52.56\n",
      "=> STEP 5051/6250   lr: 0.000141   giou_loss: 1.56   conf_loss: 50.50   prob_loss: 0.45   total_loss: 52.51\n",
      "=> STEP 5052/6250   lr: 0.000140   giou_loss: 1.71   conf_loss: 50.50   prob_loss: 0.44   total_loss: 52.65\n",
      "=> STEP 5053/6250   lr: 0.000140   giou_loss: 1.69   conf_loss: 50.49   prob_loss: 0.44   total_loss: 52.62\n",
      "=> STEP 5054/6250   lr: 0.000140   giou_loss: 1.60   conf_loss: 50.48   prob_loss: 0.45   total_loss: 52.53\n",
      "=> STEP 5055/6250   lr: 0.000140   giou_loss: 1.65   conf_loss: 50.47   prob_loss: 0.45   total_loss: 52.57\n",
      "=> STEP 5056/6250   lr: 0.000140   giou_loss: 1.63   conf_loss: 50.46   prob_loss: 0.45   total_loss: 52.54\n",
      "=> STEP 5057/6250   lr: 0.000140   giou_loss: 1.58   conf_loss: 50.45   prob_loss: 0.45   total_loss: 52.48\n",
      "=> STEP 5058/6250   lr: 0.000140   giou_loss: 1.56   conf_loss: 50.45   prob_loss: 0.45   total_loss: 52.46\n",
      "=> STEP 5059/6250   lr: 0.000139   giou_loss: 1.66   conf_loss: 50.44   prob_loss: 0.45   total_loss: 52.55\n",
      "=> STEP 5060/6250   lr: 0.000139   giou_loss: 1.68   conf_loss: 50.43   prob_loss: 0.45   total_loss: 52.56\n",
      "=> STEP 5061/6250   lr: 0.000139   giou_loss: 1.60   conf_loss: 50.42   prob_loss: 0.45   total_loss: 52.48\n",
      "=> STEP 5062/6250   lr: 0.000139   giou_loss: 1.60   conf_loss: 50.42   prob_loss: 0.45   total_loss: 52.47\n",
      "=> STEP 5063/6250   lr: 0.000139   giou_loss: 1.67   conf_loss: 50.41   prob_loss: 0.45   total_loss: 52.53\n",
      "=> STEP 5064/6250   lr: 0.000139   giou_loss: 1.70   conf_loss: 50.40   prob_loss: 0.45   total_loss: 52.55\n",
      "=> STEP 5065/6250   lr: 0.000139   giou_loss: 1.64   conf_loss: 50.39   prob_loss: 0.45   total_loss: 52.48\n",
      "=> STEP 5066/6250   lr: 0.000138   giou_loss: 1.56   conf_loss: 50.40   prob_loss: 0.45   total_loss: 52.41\n",
      "=> STEP 5067/6250   lr: 0.000138   giou_loss: 1.60   conf_loss: 50.39   prob_loss: 0.45   total_loss: 52.43\n",
      "=> STEP 5068/6250   lr: 0.000138   giou_loss: 1.61   conf_loss: 50.38   prob_loss: 0.45   total_loss: 52.44\n",
      "=> STEP 5069/6250   lr: 0.000138   giou_loss: 1.56   conf_loss: 50.37   prob_loss: 0.45   total_loss: 52.38\n",
      "=> STEP 5070/6250   lr: 0.000138   giou_loss: 1.56   conf_loss: 50.37   prob_loss: 0.44   total_loss: 52.38\n",
      "=> STEP 5071/6250   lr: 0.000138   giou_loss: 1.57   conf_loss: 50.36   prob_loss: 0.44   total_loss: 52.37\n",
      "=> STEP 5072/6250   lr: 0.000138   giou_loss: 1.58   conf_loss: 50.35   prob_loss: 0.44   total_loss: 52.37\n",
      "=> STEP 5073/6250   lr: 0.000137   giou_loss: 1.57   conf_loss: 50.34   prob_loss: 0.44   total_loss: 52.35\n",
      "=> STEP 5074/6250   lr: 0.000137   giou_loss: 1.59   conf_loss: 50.34   prob_loss: 0.45   total_loss: 52.38\n",
      "=> STEP 5075/6250   lr: 0.000137   giou_loss: 1.60   conf_loss: 50.32   prob_loss: 0.45   total_loss: 52.37\n",
      "=> STEP 5076/6250   lr: 0.000137   giou_loss: 1.56   conf_loss: 50.32   prob_loss: 0.45   total_loss: 52.33\n",
      "=> STEP 5077/6250   lr: 0.000137   giou_loss: 1.63   conf_loss: 50.32   prob_loss: 0.45   total_loss: 52.39\n",
      "=> STEP 5078/6250   lr: 0.000137   giou_loss: 1.60   conf_loss: 50.30   prob_loss: 0.45   total_loss: 52.35\n",
      "=> STEP 5079/6250   lr: 0.000137   giou_loss: 1.60   conf_loss: 50.29   prob_loss: 0.45   total_loss: 52.34\n",
      "=> STEP 5080/6250   lr: 0.000136   giou_loss: 1.58   conf_loss: 50.29   prob_loss: 0.45   total_loss: 52.32\n",
      "=> STEP 5081/6250   lr: 0.000136   giou_loss: 1.57   conf_loss: 50.28   prob_loss: 0.45   total_loss: 52.30\n",
      "=> STEP 5082/6250   lr: 0.000136   giou_loss: 1.57   conf_loss: 50.27   prob_loss: 0.45   total_loss: 52.29\n",
      "=> STEP 5083/6250   lr: 0.000136   giou_loss: 1.60   conf_loss: 50.26   prob_loss: 0.45   total_loss: 52.31\n",
      "=> STEP 5084/6250   lr: 0.000136   giou_loss: 1.62   conf_loss: 50.25   prob_loss: 0.45   total_loss: 52.32\n",
      "=> STEP 5085/6250   lr: 0.000136   giou_loss: 1.57   conf_loss: 50.25   prob_loss: 0.45   total_loss: 52.26\n",
      "=> STEP 5086/6250   lr: 0.000136   giou_loss: 1.59   conf_loss: 50.24   prob_loss: 0.45   total_loss: 52.28\n",
      "=> STEP 5087/6250   lr: 0.000135   giou_loss: 1.59   conf_loss: 50.23   prob_loss: 0.44   total_loss: 52.26\n",
      "=> STEP 5088/6250   lr: 0.000135   giou_loss: 1.56   conf_loss: 50.23   prob_loss: 0.44   total_loss: 52.24\n",
      "=> STEP 5089/6250   lr: 0.000135   giou_loss: 1.56   conf_loss: 50.22   prob_loss: 0.44   total_loss: 52.23\n",
      "=> STEP 5090/6250   lr: 0.000135   giou_loss: 1.59   conf_loss: 50.21   prob_loss: 0.45   total_loss: 52.24\n",
      "=> STEP 5091/6250   lr: 0.000135   giou_loss: 1.56   conf_loss: 50.20   prob_loss: 0.45   total_loss: 52.21\n",
      "=> STEP 5092/6250   lr: 0.000135   giou_loss: 1.56   conf_loss: 50.19   prob_loss: 0.45   total_loss: 52.20\n",
      "=> STEP 5093/6250   lr: 0.000135   giou_loss: 1.56   conf_loss: 50.19   prob_loss: 0.45   total_loss: 52.20\n",
      "=> STEP 5094/6250   lr: 0.000134   giou_loss: 1.57   conf_loss: 50.18   prob_loss: 0.45   total_loss: 52.19\n",
      "=> STEP 5095/6250   lr: 0.000134   giou_loss: 1.58   conf_loss: 50.17   prob_loss: 0.44   total_loss: 52.19\n",
      "=> STEP 5096/6250   lr: 0.000134   giou_loss: 1.56   conf_loss: 50.16   prob_loss: 0.45   total_loss: 52.17\n",
      "=> STEP 5097/6250   lr: 0.000134   giou_loss: 1.57   conf_loss: 50.15   prob_loss: 0.45   total_loss: 52.16\n",
      "=> STEP 5098/6250   lr: 0.000134   giou_loss: 1.57   conf_loss: 50.15   prob_loss: 0.45   total_loss: 52.16\n",
      "=> STEP 5099/6250   lr: 0.000134   giou_loss: 1.57   conf_loss: 50.14   prob_loss: 0.45   total_loss: 52.15\n",
      "=> STEP 5100/6250   lr: 0.000134   giou_loss: 1.57   conf_loss: 50.13   prob_loss: 0.45   total_loss: 52.14\n",
      "=> STEP 5101/6250   lr: 0.000133   giou_loss: 1.56   conf_loss: 50.12   prob_loss: 0.45   total_loss: 52.13\n",
      "=> STEP 5102/6250   lr: 0.000133   giou_loss: 1.59   conf_loss: 50.12   prob_loss: 0.45   total_loss: 52.16\n",
      "=> STEP 5103/6250   lr: 0.000133   giou_loss: 1.58   conf_loss: 50.11   prob_loss: 0.44   total_loss: 52.13\n",
      "=> STEP 5104/6250   lr: 0.000133   giou_loss: 1.56   conf_loss: 50.10   prob_loss: 0.45   total_loss: 52.11\n",
      "=> STEP 5105/6250   lr: 0.000133   giou_loss: 1.61   conf_loss: 50.09   prob_loss: 0.45   total_loss: 52.15\n",
      "=> STEP 5106/6250   lr: 0.000133   giou_loss: 1.60   conf_loss: 50.09   prob_loss: 0.45   total_loss: 52.13\n",
      "=> STEP 5107/6250   lr: 0.000133   giou_loss: 1.56   conf_loss: 50.08   prob_loss: 0.45   total_loss: 52.09\n",
      "=> STEP 5108/6250   lr: 0.000132   giou_loss: 1.57   conf_loss: 50.07   prob_loss: 0.45   total_loss: 52.08\n",
      "=> STEP 5109/6250   lr: 0.000132   giou_loss: 1.56   conf_loss: 50.07   prob_loss: 0.45   total_loss: 52.08\n",
      "=> STEP 5110/6250   lr: 0.000132   giou_loss: 1.56   conf_loss: 50.06   prob_loss: 0.45   total_loss: 52.06\n",
      "=> STEP 5111/6250   lr: 0.000132   giou_loss: 1.56   conf_loss: 50.05   prob_loss: 0.45   total_loss: 52.06\n",
      "=> STEP 5112/6250   lr: 0.000132   giou_loss: 1.56   conf_loss: 50.04   prob_loss: 0.45   total_loss: 52.05\n",
      "=> STEP 5113/6250   lr: 0.000132   giou_loss: 1.57   conf_loss: 50.04   prob_loss: 0.45   total_loss: 52.05\n",
      "=> STEP 5114/6250   lr: 0.000132   giou_loss: 1.58   conf_loss: 50.03   prob_loss: 0.45   total_loss: 52.05\n",
      "=> STEP 5115/6250   lr: 0.000131   giou_loss: 1.56   conf_loss: 50.02   prob_loss: 0.45   total_loss: 52.03\n",
      "=> STEP 5116/6250   lr: 0.000131   giou_loss: 1.56   conf_loss: 50.02   prob_loss: 0.45   total_loss: 52.02\n",
      "=> STEP 5117/6250   lr: 0.000131   giou_loss: 1.56   conf_loss: 50.01   prob_loss: 0.45   total_loss: 52.01\n",
      "=> STEP 5118/6250   lr: 0.000131   giou_loss: 1.56   conf_loss: 50.00   prob_loss: 0.45   total_loss: 52.01\n",
      "=> STEP 5119/6250   lr: 0.000131   giou_loss: 1.56   conf_loss: 49.99   prob_loss: 0.44   total_loss: 52.00\n",
      "=> STEP 5120/6250   lr: 0.000131   giou_loss: 1.57   conf_loss: 49.99   prob_loss: 0.44   total_loss: 52.00\n",
      "=> STEP 5121/6250   lr: 0.000131   giou_loss: 1.58   conf_loss: 49.98   prob_loss: 0.45   total_loss: 52.00\n",
      "=> STEP 5122/6250   lr: 0.000130   giou_loss: 1.59   conf_loss: 49.97   prob_loss: 0.45   total_loss: 52.01\n",
      "=> STEP 5123/6250   lr: 0.000130   giou_loss: 1.60   conf_loss: 49.96   prob_loss: 0.45   total_loss: 52.01\n",
      "=> STEP 5124/6250   lr: 0.000130   giou_loss: 1.61   conf_loss: 49.96   prob_loss: 0.45   total_loss: 52.01\n",
      "=> STEP 5125/6250   lr: 0.000130   giou_loss: 1.56   conf_loss: 49.95   prob_loss: 0.45   total_loss: 51.96\n",
      "=> STEP 5126/6250   lr: 0.000130   giou_loss: 1.61   conf_loss: 49.94   prob_loss: 0.45   total_loss: 52.00\n",
      "=> STEP 5127/6250   lr: 0.000130   giou_loss: 1.59   conf_loss: 49.93   prob_loss: 0.45   total_loss: 51.97\n",
      "=> STEP 5128/6250   lr: 0.000130   giou_loss: 1.58   conf_loss: 49.93   prob_loss: 0.45   total_loss: 51.96\n",
      "=> STEP 5129/6250   lr: 0.000130   giou_loss: 1.61   conf_loss: 49.92   prob_loss: 0.44   total_loss: 51.97\n",
      "=> STEP 5130/6250   lr: 0.000129   giou_loss: 1.56   conf_loss: 49.91   prob_loss: 0.45   total_loss: 51.92\n",
      "=> STEP 5131/6250   lr: 0.000129   giou_loss: 1.64   conf_loss: 49.91   prob_loss: 0.45   total_loss: 52.00\n",
      "=> STEP 5132/6250   lr: 0.000129   giou_loss: 1.67   conf_loss: 49.90   prob_loss: 0.45   total_loss: 52.01\n",
      "=> STEP 5133/6250   lr: 0.000129   giou_loss: 1.57   conf_loss: 49.89   prob_loss: 0.45   total_loss: 51.90\n",
      "=> STEP 5134/6250   lr: 0.000129   giou_loss: 1.64   conf_loss: 49.89   prob_loss: 0.45   total_loss: 51.97\n",
      "=> STEP 5135/6250   lr: 0.000129   giou_loss: 1.64   conf_loss: 49.88   prob_loss: 0.45   total_loss: 51.96\n",
      "=> STEP 5136/6250   lr: 0.000129   giou_loss: 1.56   conf_loss: 49.87   prob_loss: 0.45   total_loss: 51.88\n",
      "=> STEP 5137/6250   lr: 0.000128   giou_loss: 1.59   conf_loss: 49.87   prob_loss: 0.45   total_loss: 51.90\n",
      "=> STEP 5138/6250   lr: 0.000128   giou_loss: 1.59   conf_loss: 49.86   prob_loss: 0.45   total_loss: 51.89\n",
      "=> STEP 5139/6250   lr: 0.000128   giou_loss: 1.59   conf_loss: 49.86   prob_loss: 0.44   total_loss: 51.89\n",
      "=> STEP 5140/6250   lr: 0.000128   giou_loss: 1.58   conf_loss: 49.85   prob_loss: 0.44   total_loss: 51.88\n",
      "=> STEP 5141/6250   lr: 0.000128   giou_loss: 1.57   conf_loss: 49.84   prob_loss: 0.45   total_loss: 51.85\n",
      "=> STEP 5142/6250   lr: 0.000128   giou_loss: 1.60   conf_loss: 49.83   prob_loss: 0.45   total_loss: 51.87\n",
      "=> STEP 5143/6250   lr: 0.000128   giou_loss: 1.57   conf_loss: 49.83   prob_loss: 0.45   total_loss: 51.84\n",
      "=> STEP 5144/6250   lr: 0.000127   giou_loss: 1.56   conf_loss: 49.82   prob_loss: 0.44   total_loss: 51.83\n",
      "=> STEP 5145/6250   lr: 0.000127   giou_loss: 1.56   conf_loss: 49.81   prob_loss: 0.44   total_loss: 51.82\n",
      "=> STEP 5146/6250   lr: 0.000127   giou_loss: 1.59   conf_loss: 49.81   prob_loss: 0.44   total_loss: 51.84\n",
      "=> STEP 5147/6250   lr: 0.000127   giou_loss: 1.57   conf_loss: 49.80   prob_loss: 0.44   total_loss: 51.81\n",
      "=> STEP 5148/6250   lr: 0.000127   giou_loss: 1.56   conf_loss: 49.79   prob_loss: 0.44   total_loss: 51.80\n",
      "=> STEP 5149/6250   lr: 0.000127   giou_loss: 1.57   conf_loss: 49.78   prob_loss: 0.45   total_loss: 51.79\n",
      "=> STEP 5150/6250   lr: 0.000127   giou_loss: 1.57   conf_loss: 49.77   prob_loss: 0.45   total_loss: 51.78\n",
      "=> STEP 5151/6250   lr: 0.000126   giou_loss: 1.59   conf_loss: 49.77   prob_loss: 0.45   total_loss: 51.80\n",
      "=> STEP 5152/6250   lr: 0.000126   giou_loss: 1.56   conf_loss: 49.76   prob_loss: 0.44   total_loss: 51.77\n",
      "=> STEP 5153/6250   lr: 0.000126   giou_loss: 1.58   conf_loss: 49.76   prob_loss: 0.44   total_loss: 51.78\n",
      "=> STEP 5154/6250   lr: 0.000126   giou_loss: 1.58   conf_loss: 49.75   prob_loss: 0.44   total_loss: 51.77\n",
      "=> STEP 5155/6250   lr: 0.000126   giou_loss: 1.62   conf_loss: 49.74   prob_loss: 0.45   total_loss: 51.80\n",
      "=> STEP 5156/6250   lr: 0.000126   giou_loss: 1.59   conf_loss: 49.73   prob_loss: 0.45   total_loss: 51.77\n",
      "=> STEP 5157/6250   lr: 0.000126   giou_loss: 1.57   conf_loss: 49.73   prob_loss: 0.44   total_loss: 51.75\n",
      "=> STEP 5158/6250   lr: 0.000126   giou_loss: 1.58   conf_loss: 49.72   prob_loss: 0.44   total_loss: 51.75\n",
      "=> STEP 5159/6250   lr: 0.000125   giou_loss: 1.57   conf_loss: 49.71   prob_loss: 0.44   total_loss: 51.72\n",
      "=> STEP 5160/6250   lr: 0.000125   giou_loss: 1.64   conf_loss: 49.71   prob_loss: 0.45   total_loss: 51.79\n",
      "=> STEP 5161/6250   lr: 0.000125   giou_loss: 1.60   conf_loss: 49.70   prob_loss: 0.44   total_loss: 51.75\n",
      "=> STEP 5162/6250   lr: 0.000125   giou_loss: 1.57   conf_loss: 49.70   prob_loss: 0.44   total_loss: 51.71\n",
      "=> STEP 5163/6250   lr: 0.000125   giou_loss: 1.57   conf_loss: 49.69   prob_loss: 0.44   total_loss: 51.71\n",
      "=> STEP 5164/6250   lr: 0.000125   giou_loss: 1.57   conf_loss: 49.68   prob_loss: 0.44   total_loss: 51.69\n",
      "=> STEP 5165/6250   lr: 0.000125   giou_loss: 1.56   conf_loss: 49.67   prob_loss: 0.44   total_loss: 51.68\n",
      "=> STEP 5166/6250   lr: 0.000124   giou_loss: 1.56   conf_loss: 49.67   prob_loss: 0.44   total_loss: 51.68\n",
      "=> STEP 5167/6250   lr: 0.000124   giou_loss: 1.57   conf_loss: 49.66   prob_loss: 0.44   total_loss: 51.67\n",
      "=> STEP 5168/6250   lr: 0.000124   giou_loss: 1.57   conf_loss: 49.65   prob_loss: 0.45   total_loss: 51.66\n",
      "=> STEP 5169/6250   lr: 0.000124   giou_loss: 1.57   conf_loss: 49.64   prob_loss: 0.45   total_loss: 51.65\n",
      "=> STEP 5170/6250   lr: 0.000124   giou_loss: 1.56   conf_loss: 49.64   prob_loss: 0.45   total_loss: 51.65\n",
      "=> STEP 5171/6250   lr: 0.000124   giou_loss: 1.56   conf_loss: 49.63   prob_loss: 0.44   total_loss: 51.64\n",
      "=> STEP 5172/6250   lr: 0.000124   giou_loss: 1.57   conf_loss: 49.62   prob_loss: 0.44   total_loss: 51.64\n",
      "=> STEP 5173/6250   lr: 0.000124   giou_loss: 1.57   conf_loss: 49.62   prob_loss: 0.44   total_loss: 51.63\n",
      "=> STEP 5174/6250   lr: 0.000123   giou_loss: 1.57   conf_loss: 49.61   prob_loss: 0.44   total_loss: 51.62\n",
      "=> STEP 5175/6250   lr: 0.000123   giou_loss: 1.56   conf_loss: 49.60   prob_loss: 0.44   total_loss: 51.61\n",
      "=> STEP 5176/6250   lr: 0.000123   giou_loss: 1.56   conf_loss: 49.60   prob_loss: 0.44   total_loss: 51.60\n",
      "=> STEP 5177/6250   lr: 0.000123   giou_loss: 1.56   conf_loss: 49.59   prob_loss: 0.44   total_loss: 51.60\n",
      "=> STEP 5178/6250   lr: 0.000123   giou_loss: 1.57   conf_loss: 49.58   prob_loss: 0.44   total_loss: 51.60\n",
      "=> STEP 5179/6250   lr: 0.000123   giou_loss: 1.56   conf_loss: 49.57   prob_loss: 0.44   total_loss: 51.58\n",
      "=> STEP 5180/6250   lr: 0.000123   giou_loss: 1.63   conf_loss: 49.57   prob_loss: 0.45   total_loss: 51.64\n",
      "=> STEP 5181/6250   lr: 0.000122   giou_loss: 1.60   conf_loss: 49.56   prob_loss: 0.44   total_loss: 51.60\n",
      "=> STEP 5182/6250   lr: 0.000122   giou_loss: 1.57   conf_loss: 49.56   prob_loss: 0.44   total_loss: 51.57\n",
      "=> STEP 5183/6250   lr: 0.000122   giou_loss: 1.58   conf_loss: 49.55   prob_loss: 0.44   total_loss: 51.57\n",
      "=> STEP 5184/6250   lr: 0.000122   giou_loss: 1.56   conf_loss: 49.54   prob_loss: 0.44   total_loss: 51.55\n",
      "=> STEP 5185/6250   lr: 0.000122   giou_loss: 1.62   conf_loss: 49.54   prob_loss: 0.44   total_loss: 51.61\n",
      "=> STEP 5186/6250   lr: 0.000122   giou_loss: 1.60   conf_loss: 49.53   prob_loss: 0.44   total_loss: 51.58\n",
      "=> STEP 5187/6250   lr: 0.000122   giou_loss: 1.56   conf_loss: 49.53   prob_loss: 0.44   total_loss: 51.54\n",
      "=> STEP 5188/6250   lr: 0.000122   giou_loss: 1.65   conf_loss: 49.52   prob_loss: 0.44   total_loss: 51.61\n",
      "=> STEP 5189/6250   lr: 0.000121   giou_loss: 1.67   conf_loss: 49.51   prob_loss: 0.44   total_loss: 51.63\n",
      "=> STEP 5190/6250   lr: 0.000121   giou_loss: 1.57   conf_loss: 49.51   prob_loss: 0.44   total_loss: 51.52\n",
      "=> STEP 5191/6250   lr: 0.000121   giou_loss: 1.57   conf_loss: 49.51   prob_loss: 0.44   total_loss: 51.52\n",
      "=> STEP 5192/6250   lr: 0.000121   giou_loss: 1.63   conf_loss: 49.50   prob_loss: 0.44   total_loss: 51.57\n",
      "=> STEP 5193/6250   lr: 0.000121   giou_loss: 1.64   conf_loss: 49.50   prob_loss: 0.44   total_loss: 51.58\n",
      "=> STEP 5194/6250   lr: 0.000121   giou_loss: 1.57   conf_loss: 49.49   prob_loss: 0.44   total_loss: 51.50\n",
      "=> STEP 5195/6250   lr: 0.000121   giou_loss: 1.57   conf_loss: 49.49   prob_loss: 0.44   total_loss: 51.50\n",
      "=> STEP 5196/6250   lr: 0.000121   giou_loss: 1.62   conf_loss: 49.48   prob_loss: 0.44   total_loss: 51.54\n",
      "=> STEP 5197/6250   lr: 0.000120   giou_loss: 1.58   conf_loss: 49.47   prob_loss: 0.44   total_loss: 51.50\n",
      "=> STEP 5198/6250   lr: 0.000120   giou_loss: 1.60   conf_loss: 49.46   prob_loss: 0.44   total_loss: 51.51\n",
      "=> STEP 5199/6250   lr: 0.000120   giou_loss: 1.63   conf_loss: 49.46   prob_loss: 0.44   total_loss: 51.53\n",
      "=> STEP 5200/6250   lr: 0.000120   giou_loss: 1.64   conf_loss: 49.45   prob_loss: 0.44   total_loss: 51.53\n",
      "=> STEP 5201/6250   lr: 0.000120   giou_loss: 1.58   conf_loss: 49.44   prob_loss: 0.44   total_loss: 51.46\n",
      "=> STEP 5202/6250   lr: 0.000120   giou_loss: 1.57   conf_loss: 49.44   prob_loss: 0.44   total_loss: 51.45\n",
      "=> STEP 5203/6250   lr: 0.000120   giou_loss: 1.68   conf_loss: 49.43   prob_loss: 0.44   total_loss: 51.55\n",
      "=> STEP 5204/6250   lr: 0.000119   giou_loss: 1.71   conf_loss: 49.42   prob_loss: 0.44   total_loss: 51.58\n",
      "=> STEP 5205/6250   lr: 0.000119   giou_loss: 1.72   conf_loss: 49.42   prob_loss: 0.44   total_loss: 51.58\n",
      "=> STEP 5206/6250   lr: 0.000119   giou_loss: 1.57   conf_loss: 49.41   prob_loss: 0.44   total_loss: 51.43\n",
      "=> STEP 5207/6250   lr: 0.000119   giou_loss: 1.68   conf_loss: 49.41   prob_loss: 0.44   total_loss: 51.53\n",
      "=> STEP 5208/6250   lr: 0.000119   giou_loss: 1.78   conf_loss: 49.40   prob_loss: 0.44   total_loss: 51.63\n",
      "=> STEP 5209/6250   lr: 0.000119   giou_loss: 1.73   conf_loss: 49.39   prob_loss: 0.44   total_loss: 51.57\n",
      "=> STEP 5210/6250   lr: 0.000119   giou_loss: 1.64   conf_loss: 49.39   prob_loss: 0.45   total_loss: 51.48\n",
      "=> STEP 5211/6250   lr: 0.000119   giou_loss: 1.64   conf_loss: 49.38   prob_loss: 0.45   total_loss: 51.47\n",
      "=> STEP 5212/6250   lr: 0.000118   giou_loss: 1.66   conf_loss: 49.37   prob_loss: 0.44   total_loss: 51.47\n",
      "=> STEP 5213/6250   lr: 0.000118   giou_loss: 1.66   conf_loss: 49.37   prob_loss: 0.44   total_loss: 51.47\n",
      "=> STEP 5214/6250   lr: 0.000118   giou_loss: 1.65   conf_loss: 49.36   prob_loss: 0.44   total_loss: 51.45\n",
      "=> STEP 5215/6250   lr: 0.000118   giou_loss: 1.58   conf_loss: 49.37   prob_loss: 0.44   total_loss: 51.39\n",
      "=> STEP 5216/6250   lr: 0.000118   giou_loss: 1.64   conf_loss: 49.36   prob_loss: 0.44   total_loss: 51.43\n",
      "=> STEP 5217/6250   lr: 0.000118   giou_loss: 1.68   conf_loss: 49.34   prob_loss: 0.44   total_loss: 51.46\n",
      "=> STEP 5218/6250   lr: 0.000118   giou_loss: 1.58   conf_loss: 49.34   prob_loss: 0.44   total_loss: 51.36\n",
      "=> STEP 5219/6250   lr: 0.000118   giou_loss: 1.60   conf_loss: 49.34   prob_loss: 0.44   total_loss: 51.39\n",
      "=> STEP 5220/6250   lr: 0.000117   giou_loss: 1.62   conf_loss: 49.33   prob_loss: 0.45   total_loss: 51.39\n",
      "=> STEP 5221/6250   lr: 0.000117   giou_loss: 1.56   conf_loss: 49.32   prob_loss: 0.45   total_loss: 51.33\n",
      "=> STEP 5222/6250   lr: 0.000117   giou_loss: 1.57   conf_loss: 49.31   prob_loss: 0.45   total_loss: 51.33\n",
      "=> STEP 5223/6250   lr: 0.000117   giou_loss: 1.59   conf_loss: 49.31   prob_loss: 0.45   total_loss: 51.34\n",
      "=> STEP 5224/6250   lr: 0.000117   giou_loss: 1.56   conf_loss: 49.30   prob_loss: 0.44   total_loss: 51.31\n",
      "=> STEP 5225/6250   lr: 0.000117   giou_loss: 1.57   conf_loss: 49.29   prob_loss: 0.44   total_loss: 51.31\n",
      "=> STEP 5226/6250   lr: 0.000117   giou_loss: 1.56   conf_loss: 49.29   prob_loss: 0.44   total_loss: 51.30\n",
      "=> STEP 5227/6250   lr: 0.000117   giou_loss: 1.58   conf_loss: 49.28   prob_loss: 0.44   total_loss: 51.30\n",
      "=> STEP 5228/6250   lr: 0.000116   giou_loss: 1.56   conf_loss: 49.28   prob_loss: 0.44   total_loss: 51.28\n",
      "=> STEP 5229/6250   lr: 0.000116   giou_loss: 1.56   conf_loss: 49.27   prob_loss: 0.44   total_loss: 51.27\n",
      "=> STEP 5230/6250   lr: 0.000116   giou_loss: 1.59   conf_loss: 49.27   prob_loss: 0.44   total_loss: 51.30\n",
      "=> STEP 5231/6250   lr: 0.000116   giou_loss: 1.58   conf_loss: 49.26   prob_loss: 0.44   total_loss: 51.28\n",
      "=> STEP 5232/6250   lr: 0.000116   giou_loss: 1.56   conf_loss: 49.25   prob_loss: 0.44   total_loss: 51.25\n",
      "=> STEP 5233/6250   lr: 0.000116   giou_loss: 1.57   conf_loss: 49.24   prob_loss: 0.44   total_loss: 51.26\n",
      "=> STEP 5234/6250   lr: 0.000116   giou_loss: 1.58   conf_loss: 49.23   prob_loss: 0.44   total_loss: 51.25\n",
      "=> STEP 5235/6250   lr: 0.000116   giou_loss: 1.58   conf_loss: 49.23   prob_loss: 0.44   total_loss: 51.25\n",
      "=> STEP 5236/6250   lr: 0.000115   giou_loss: 1.56   conf_loss: 49.22   prob_loss: 0.44   total_loss: 51.23\n",
      "=> STEP 5237/6250   lr: 0.000115   giou_loss: 1.56   conf_loss: 49.21   prob_loss: 0.44   total_loss: 51.22\n",
      "=> STEP 5238/6250   lr: 0.000115   giou_loss: 1.59   conf_loss: 49.21   prob_loss: 0.44   total_loss: 51.24\n",
      "=> STEP 5239/6250   lr: 0.000115   giou_loss: 1.56   conf_loss: 49.20   prob_loss: 0.44   total_loss: 51.20\n",
      "=> STEP 5240/6250   lr: 0.000115   giou_loss: 1.57   conf_loss: 49.19   prob_loss: 0.44   total_loss: 51.20\n",
      "=> STEP 5241/6250   lr: 0.000115   giou_loss: 1.57   conf_loss: 49.18   prob_loss: 0.44   total_loss: 51.19\n",
      "=> STEP 5242/6250   lr: 0.000115   giou_loss: 1.57   conf_loss: 49.17   prob_loss: 0.44   total_loss: 51.18\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "logdir = \"/home/jupyter/moles-detection/log\"\n",
    "\n",
    "if os.path.exists(logdir): shutil.rmtree(logdir)\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "freeze_layers = load_freeze_layer(model='yolov4', tiny=False)\n",
    "isfreeze = False\n",
    "\n",
    "def train_step(image_data, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_result = model(image_data, training=True)\n",
    "        giou_loss = conf_loss = prob_loss = 0\n",
    "\n",
    "        for i in range(len(freeze_layers)):\n",
    "            conv, pred = pred_result[i * 2], pred_result[i * 2 + 1]\n",
    "            loss_items = compute_loss(pred, conv, target[i][0], target[i][1], \n",
    "                                      STRIDES=STRIDES, NUM_CLASS=NUM_CLASS, IOU_LOSS_THRESH=IOU_LOSS_THRESH, i=i)\n",
    "            giou_loss += loss_items[0]\n",
    "            conf_loss += loss_items[1]\n",
    "            prob_loss += loss_items[2]\n",
    "\n",
    "        total_loss = giou_loss + conf_loss + prob_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        tf.print(\"=> STEP %4d/%4d   lr: %.6f   giou_loss: %4.2f   conf_loss: %4.2f   \"\n",
    "                 \"prob_loss: %4.2f   total_loss: %4.2f\" % (global_steps, total_steps, optimizer.lr.numpy(),\n",
    "                                                           giou_loss, conf_loss,\n",
    "                                                           prob_loss, total_loss))\n",
    "        # update learning rate\n",
    "        global_steps.assign_add(1)\n",
    "        if global_steps < warmup_steps:\n",
    "            lr = global_steps / warmup_steps * LR_INIT\n",
    "        else:\n",
    "            lr = LR_END + 0.5 * (LR_INIT - LR_END) * (\n",
    "                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))\n",
    "            )\n",
    "        optimizer.lr.assign(lr.numpy())\n",
    "\n",
    "        # writing summary data\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"lr\", optimizer.lr, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/total_loss\", total_loss, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/giou_loss\", giou_loss, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/conf_loss\", conf_loss, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/prob_loss\", prob_loss, step=global_steps)\n",
    "        writer.flush()\n",
    "        \n",
    "# def test_step(image_data, target):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         pred_result = model(image_data, training=True)\n",
    "#         giou_loss = conf_loss = prob_loss = 0\n",
    "\n",
    "#         # optimizing process\n",
    "#         for i in range(len(freeze_layers)):\n",
    "#             conv, pred = pred_result[i * 2], pred_result[i * 2 + 1]\n",
    "#             loss_items = compute_loss(pred, conv, target[i][0], target[i][1], STRIDES=STRIDES, NUM_CLASS=NUM_CLASS, IOU_LOSS_THRESH=IOU_LOSS_THRESH, i=i)\n",
    "#             giou_loss += loss_items[0]\n",
    "#             conf_loss += loss_items[1]\n",
    "#             prob_loss += loss_items[2]\n",
    "\n",
    "#         total_loss = giou_loss + conf_loss + prob_loss\n",
    "\n",
    "#         tf.print(\"=> TEST STEP %4d   giou_loss: %4.2f   conf_loss: %4.2f   \"\n",
    "#                  \"prob_loss: %4.2f   total_loss: %4.2f\" % (global_steps, giou_loss, conf_loss,\n",
    "#                                                            prob_loss, total_loss))\n",
    "\n",
    "for epoch in range(first_stage_epochs + second_stage_epochs):\n",
    "    if epoch < first_stage_epochs:\n",
    "        if not isfreeze:\n",
    "            isfreeze = True\n",
    "            for name in freeze_layers:\n",
    "                freeze = model.get_layer(name)\n",
    "                freeze_all(freeze)\n",
    "    elif epoch >= first_stage_epochs:\n",
    "        if isfreeze:\n",
    "            isfreeze = False\n",
    "            for name in freeze_layers:\n",
    "                freeze = model.get_layer(name)\n",
    "                unfreeze_all(freeze)\n",
    "    for image_data, target in trainset:\n",
    "        train_step(image_data, target)\n",
    "#     for image_data, target in testset:\n",
    "#         test_step(image_data, target)\n",
    "    model.save_weights(\"/home/jupyter/moles-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
